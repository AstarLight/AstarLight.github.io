---
title: shell日志数据分析实战
date: 2020-12-06 11:35:26
tags: [linux,Shell]	
categories: 工具
---

shell 脚本在日志扫档中用的比较多，脚本使用的熟练程度能极大地影响你的工作效率，比如某一天我的领导叫我帮忙扫一下日志，把一些关键的数据扫出来，当时因为自己对shell不是很熟练再加上日志格式比较复杂，自己花了一个下午才把数据给整出来，当时也特别烦躁，因为明知这是一个很简单的任务，但因为自己对工具（脚本）的不熟练，导致这个领导看起来无关痛痒的小需求（确实也是很简单）却占用了自己很多的工作时间，把我原先的工作给耽误了，搞到我当天加班到很晚才把当天的工作完成，当时就认识到服务器开发中shell脚本的熟练掌握非常重要。

日常数据收集和分析中shell脚本用的最多的指令就是这几个：**grep, awk, sed, sort, uniq, xargs,wc,head,tail**。个人认为这9个指令可以解决我们平时90%的数据分析任务，实在有些复杂的功能，那就需要写一写shell脚本，所以还得熟悉一下shell语法的for,if else,读写文件就足够了。 以下以2个实际的数据归档的案例，串联起上面提到的所有shell 指令，把他们的重要的使用场景和方法实践。

## 任务1：2020年11月25日8点以来login次数前20的玩家
日志格式如下：
```
[2020-12-05 19:21:37][6772100] logout
[2020-12-05 19:22:18][6797297] login
[2020-12-05 19:22:50][6770758] login
[2020-12-05 19:24:46][6796204] relogin
[2020-12-05 19:24:48][6770256] login
[2020-12-05 19:25:34][6796204] relogin
```

实现思路：
1. 筛选登录: grep
2. 筛选时间：awk 的 if
3. 排序前20：sort

实现步骤：
1. 我比较喜欢用Sed先整理格式，即把分割符统一替换成空格： `sed -e 's/\[/ /g' -e 's/\]/ /g' login.dat`

输出：
```
 2020-12-05 19:39:47  6771096  login
 2020-12-05 19:42:33  6769079  logout
 2020-12-05 19:42:33  6769162  logout
 2020-12-05 19:43:13  6798402  relogin

```

2. 筛选登录用grep： ` sed -e 's/\[/ /g' -e 's/\]/ /g' login.dat  |grep -w login`

输出
```
 2020-12-05 19:14:53  6772163  login
 2020-12-05 19:18:24  6796958  login
 2020-12-05 19:18:27  6798402  login
 2020-12-05 19:39:47  6771096  login
 2020-12-05 19:46:19  6769423  login

```

3. 筛选时间用awk: `sed -e 's/\[/ /g' -e 's/\]/ /g' login.dat  |grep -w login | awk '{if($1" "$2> "2020-11-25 08:00:00") print $3}'`

注意，$变量之间用""就可以把他们转化为字符串连接在一起，如果要转整形需使用"+"

输出
```
6768524
6797124
6797435
6795472
6768435
6769433
6797123
6770253
6772163
6796958
6798402
6797297
6770758
6770256
6798284

```

4. 排序+计数用sort+uniq: ` sed -e 's/\[/ /g' -e 's/\]/ /g' login.dat  |grep -w login | awk '{if($1" "$2> "2020-11-25 08:00:00") print $3}'  | sort | uniq -c |sort`

注意sort后再接uniq 统计重复行才能统计出来，最后再接sort排序
```
  11 6796765
  11 6797821
  12 6769451
  12 6769464
  12 6796365
  12 6796761
  12 6796762
  13 6798238
  14 6796763
  15 6766560
  22 6798433
  23 6768911
  37 6806225

```

5. 取前20用tail：`sed -e 's/\[/ /g' -e 's/\]/ /g' login.dat  |grep -w login | awk '{if($1" "$2> "2020-11-25 08:00:00") print $3}'  | sort | uniq -c |sort |tail -n 20 |awk '{print $2}'`得到最后结果

输出
```
6798615
6767256
6769367
6769684
6796775
6797820
6769387
6796765
6797821
6769451
6769464
6796365
6796761
6796762
6798238
6796763
6766560
6798433
6768911
6806225

```

## 任务2：输出2020年11月25日8点以来login次数前20的玩家具体的参与活动的具体情况，格式按usernum 总分数输出

这里涉及到两个文件的数据合并，文件1就是上一个任务的最终结果，文件2的格式如下：
```
[2020-12-05 18:55:25] [party]desc=finish_game,uid=6772009,game=1,score=1168, count=64
[2020-12-05 18:56:02] [party]desc=finish_game,uid=6772009,game=1,score=1680, count=128
[2020-12-05 18:56:40] [party]desc=finish_game,uid=6772009,game=2,score=120, count=12
[2020-12-05 18:57:34] [party]desc=finish_game,uid=6772009,game=2,score=240, count=24
[2020-12-05 18:58:53] [party]desc=finish_game,uid=6772009,game=3,score=120, count=12
[2020-12-05 21:38:23] [party]desc=finish_game,uid=6795956,game=5,score=780, count=50
[2020-12-05 21:38:54] [party]desc=finish_game,uid=6795956,game=4,score=60, count=60
[2020-12-05 21:40:27] [party]desc=finish_game,uid=6795956,game=2,score=300, count=30
[2020-12-05 21:42:10] [party]desc=finish_game,uid=6766739,game=5,score=830, count=68
[2020-12-05 21:43:11] [party]desc=finish_game,uid=6766739,game=1,score=1888, count=128
[2020-12-05 21:43:54] [party]desc=finish_game,uid=6766739,game=2,score=170, count=17
[2020-12-05 21:45:27] [party]desc=finish_game,uid=6766739,game=3,score=150, count=15
[2020-12-05 21:46:20] [party]desc=finish_game,uid=6766739,game=2,score=120, count=12

```

思路：
1. grep出关键信息，做好格式化
2. 累计计算玩家的所有的分数
3. 找出文件1的uid在文件2中的分数

实现步骤：

1. grep出关键信息，通过awk做好格式化：
`grep finish_game znq20.dat  | awk -F ',' '{print $2,$4}'`

输出：
```
uid=6772005 score=40
uid=6772005 score=120
uid=6772005 score=110
uid=6772005 score=500
uid=6772007 score=1608
uid=6772007 score=230
uid=6772007 score=110
uid=6772007 score=550
uid=6772007 score=280
uid=6772008 score=1560
uid=6772008 score=240
uid=6772008 score=210
uid=6772008 score=140
uid=6772008 score=430

```

2. 继续使用awk继续分割，继续格式化，输出【uid 分数】的格式：
`grep finish_game znq20.dat  | awk -F ',' '{print $2,$4}' | awk -F '[= ]'  '{print $2,$4}'`

输出：
```
6772008 140
6772008 430
6772009 1168
6772009 1680
6772009 120
6772009 240
6772009 120
6795956 780
6795956 60
6795956 300
6766739 830
6766739 1888
6766739 170
```
3. 统计每个用户的游戏总分数，使用awk中的字典实现，用END流程控制，最后输出结果：
```
grep finish_game znq20.dat  | awk -F ',' '{print $2,$4}' | awk -F '[= ]'  '{print $2,$4}' |  awk '{map[$1]+=$2} END{for(k in map) print k,map[k]}'
```
输出：
```
6797415 310
6795471 3108
6798595 1735
6797867 50856
6767900 5269
6767695 200
6772097 6246
6797801 570
6768636 1897
6769367 121206
6795865 2580

```
5. 上一步的结果已经输出到文件，现在cat practice*.txt就会输出AB文件的内容，因为A文件每行只有一列，B文件每行2列,因此用awk 的NF区分这是A文件的数据还是B文件数据。再判断A文件的UID是否在B文件，有则输出B文件对应的行：
```
 cat practice*.txt | awk '{if(NF==1) map1[$1]=1; else map2[$1]=$2}  END{for(k in map1) if(map2[k]>0) print k,map2[k]}'
```
输出：
```
6769451 1135
6798433 1230
6769367 121206
6806225 9901

```


## 总结
日志数据过滤分析中重度依赖awk和grep,而sed,wc,sort,uniq等也有比较多的使用场景，因此着重掌握awk和grep尤为重要。这是这些指令的使用场景：
- awk：格式化输出，按列分割，按列打印，使用if,for,begin,end做流程控制，用关联内置的关联数组做数据的记录和计算，还有一些常用的内置额字符串相关的函数需要记牢：match(s,r)，index(s,t)，length(s) ，substr(s,p,n)
- grep：用于数据过滤，掌握一些参数即可，比如-v，-w, -c, -i, -r, -n,-A,-B,-C
- sed: 用于替换，重度使用这个：`sed 's/替换前的值/替换后的值/g'`
- sort: 排序，掌握-f,-r,-n,-k(按自己需要选择哪一列的key进行排序，很有用,如-k2就是使用第2列进行排序)
- uniq: 去重，一般都是结合sort使用，先sort再uniq -c
- wc: 统计行数, wc -l即可